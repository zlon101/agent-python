"""
åˆ†é¡µè¡¨æ ¼æ•°æ®æå–å·¥å…·
æ”¯æŒå¤šç§åˆ†é¡µæ–¹å¼å’Œè¡¨æ ¼æ ¼å¼
"""

import json
import csv
from typing import List, Dict, Optional, Any
from dataclasses import dataclass, asdict
from playwright.async_api import Page, Locator
from bs4 import BeautifulSoup
import asyncio


@dataclass
class TableData:
    """è¡¨æ ¼æ•°æ®ç»“æ„"""
    headers: List[str]
    rows: List[List[str]]
    page_number: int
    total_rows: int


@dataclass
class PaginationConfig:
    """åˆ†é¡µé…ç½®"""
    # åˆ†é¡µç±»å‹: "button" | "number" | "infinite_scroll" | "url_param"
    type: str
    
    # ä¸‹ä¸€é¡µæŒ‰é’®é€‰æ‹©å™¨ï¼ˆtype="button"ï¼‰
    next_button_selector: Optional[str] = None
    
    # é¡µç é€‰æ‹©å™¨ï¼ˆtype="number"ï¼‰
    page_number_selector: Optional[str] = None
    
    # URL å‚æ•°åï¼ˆtype="url_param"ï¼‰
    url_param_name: Optional[str] = None
    
    # æœ€å¤§é¡µæ•°ï¼ˆ0 è¡¨ç¤ºæ— é™åˆ¶ï¼‰
    max_pages: int = 0
    
    # ç­‰å¾…æ—¶é—´ï¼ˆç§’ï¼‰
    wait_time: float = 2.0


class TableScraper:
    """è¡¨æ ¼æ•°æ®æå–å™¨"""
    
    def __init__(self, page: Page):
        """
        åˆå§‹åŒ–è¡¨æ ¼æå–å™¨
        
        Args:
            page: Playwright é¡µé¢å¯¹è±¡
        """
        self.page = page
        self.all_data: List[TableData] = []
    
    async def extract_table(
        self,
        table_selector: str = "table",
        headers_selector: str = "thead th",
        rows_selector: str = "tbody tr",
        cells_selector: str = "td"
    ) -> TableData:
        """
        æå–å½“å‰é¡µè¡¨æ ¼æ•°æ®
        
        Args:
            table_selector: è¡¨æ ¼é€‰æ‹©å™¨
            headers_selector: è¡¨å¤´é€‰æ‹©å™¨
            rows_selector: è¡Œé€‰æ‹©å™¨
            cells_selector: å•å…ƒæ ¼é€‰æ‹©å™¨
            
        Returns:
            TableData: è¡¨æ ¼æ•°æ®å¯¹è±¡
        """
        # ç­‰å¾…è¡¨æ ¼åŠ è½½
        await self.page.wait_for_selector(table_selector, timeout=10000)
        
        # æå–è¡¨å¤´
        headers = await self.page.locator(headers_selector).all_text_contents()
        headers = [h.strip() for h in headers if h.strip()]
        
        # æå–è¡Œæ•°æ®
        rows = []
        row_elements = await self.page.locator(rows_selector).all()
        
        for row_element in row_elements:
            cells = await row_element.locator(cells_selector).all_text_contents()
            cells = [c.strip() for c in cells]
            if cells:  # è·³è¿‡ç©ºè¡Œ
                rows.append(cells)
        
        return TableData(
            headers=headers,
            rows=rows,
            page_number=len(self.all_data) + 1,
            total_rows=len(rows)
        )
    
    async def scrape_with_button_pagination(
        self,
        table_selector: str,
        next_button_selector: str,
        max_pages: int = 0,
        wait_time: float = 2.0
    ) -> List[TableData]:
        """
        ä½¿ç”¨"ä¸‹ä¸€é¡µ"æŒ‰é’®åˆ†é¡µæŠ“å–
        
        Args:
            table_selector: è¡¨æ ¼é€‰æ‹©å™¨
            next_button_selector: ä¸‹ä¸€é¡µæŒ‰é’®é€‰æ‹©å™¨
            max_pages: æœ€å¤§é¡µæ•°ï¼ˆ0 è¡¨ç¤ºæ— é™åˆ¶ï¼‰
            wait_time: æ¯é¡µç­‰å¾…æ—¶é—´
            
        Returns:
            List[TableData]: æ‰€æœ‰é¡µé¢çš„æ•°æ®
        """
        page_count = 0
        
        while True:
            # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°æœ€å¤§é¡µæ•°
            if max_pages > 0 and page_count >= max_pages:
                print(f"âœ… è¾¾åˆ°æœ€å¤§é¡µæ•°é™åˆ¶: {max_pages}")
                break
            
            # æå–å½“å‰é¡µæ•°æ®
            print(f"ğŸ“„ æå–ç¬¬ {page_count + 1} é¡µ...")
            data = await self.extract_table(table_selector)
            self.all_data.append(data)
            page_count += 1
            
            print(f"   âœ“ æå– {data.total_rows} è¡Œæ•°æ®")
            
            # æ£€æŸ¥æ˜¯å¦æœ‰ä¸‹ä¸€é¡µæŒ‰é’®
            next_button = self.page.locator(next_button_selector)
            
            try:
                # æ£€æŸ¥æŒ‰é’®æ˜¯å¦å­˜åœ¨ä¸”å¯ç‚¹å‡»
                is_visible = await next_button.is_visible()
                is_enabled = await next_button.is_enabled()
                
                if not is_visible or not is_enabled:
                    print("âœ… å·²åˆ°è¾¾æœ€åä¸€é¡µï¼ˆæŒ‰é’®ä¸å¯ç”¨ï¼‰")
                    break
                
                # ç‚¹å‡»ä¸‹ä¸€é¡µ
                await next_button.click()
                
                # ç­‰å¾…é¡µé¢åŠ è½½
                await asyncio.sleep(wait_time)
                
                # ç­‰å¾…è¡¨æ ¼æ›´æ–°ï¼ˆå¯é€‰ï¼šæ£€æŸ¥è¡¨æ ¼å˜åŒ–ï¼‰
                await self.page.wait_for_load_state("networkidle")
                
            except Exception as e:
                print(f"âœ… å·²åˆ°è¾¾æœ€åä¸€é¡µ: {str(e)}")
                break
        
        return self.all_data
    
    async def scrape_with_page_numbers(
        self,
        table_selector: str,
        page_number_selector: str,
        max_pages: int = 0,
        wait_time: float = 2.0
    ) -> List[TableData]:
        """
        ä½¿ç”¨é¡µç åˆ†é¡µæŠ“å–ï¼ˆ1, 2, 3, ...ï¼‰
        
        Args:
            table_selector: è¡¨æ ¼é€‰æ‹©å™¨
            page_number_selector: é¡µç é“¾æ¥é€‰æ‹©å™¨æ¨¡æ¿ï¼ˆä¾‹å¦‚ï¼š'a.page-{page}'ï¼‰
            max_pages: æœ€å¤§é¡µæ•°
            wait_time: æ¯é¡µç­‰å¾…æ—¶é—´
            
        Returns:
            List[TableData]: æ‰€æœ‰é¡µé¢çš„æ•°æ®
        """
        page_count = 1
        
        # æå–ç¬¬ä¸€é¡µ
        print(f"ğŸ“„ æå–ç¬¬ {page_count} é¡µ...")
        data = await self.extract_table(table_selector)
        self.all_data.append(data)
        print(f"   âœ“ æå– {data.total_rows} è¡Œæ•°æ®")
        
        # å¾ªç¯æå–åç»­é¡µé¢
        while True:
            page_count += 1
            
            if max_pages > 0 and page_count > max_pages:
                print(f"âœ… è¾¾åˆ°æœ€å¤§é¡µæ•°é™åˆ¶: {max_pages}")
                break
            
            # æ„é€ é¡µç é€‰æ‹©å™¨
            selector = page_number_selector.replace("{page}", str(page_count))
            page_link = self.page.locator(selector)
            
            try:
                is_visible = await page_link.is_visible()
                if not is_visible:
                    print(f"âœ… å·²åˆ°è¾¾æœ€åä¸€é¡µï¼ˆé¡µç  {page_count} ä¸å­˜åœ¨ï¼‰")
                    break
                
                print(f"ğŸ“„ æå–ç¬¬ {page_count} é¡µ...")
                
                # ç‚¹å‡»é¡µç 
                await page_link.click()
                await asyncio.sleep(wait_time)
                await self.page.wait_for_load_state("networkidle")
                
                # æå–æ•°æ®
                data = await self.extract_table(table_selector)
                self.all_data.append(data)
                print(f"   âœ“ æå– {data.total_rows} è¡Œæ•°æ®")
                
            except Exception as e:
                print(f"âœ… å·²åˆ°è¾¾æœ€åä¸€é¡µ: {str(e)}")
                break
        
        return self.all_data
    
    async def scrape_with_url_params(
        self,
        base_url: str,
        table_selector: str,
        page_param: str = "page",
        start_page: int = 1,
        max_pages: int = 0,
        wait_time: float = 2.0
    ) -> List[TableData]:
        """
        ä½¿ç”¨ URL å‚æ•°åˆ†é¡µæŠ“å–ï¼ˆä¾‹å¦‚ï¼š?page=1, ?page=2ï¼‰
        
        Args:
            base_url: åŸºç¡€ URL
            table_selector: è¡¨æ ¼é€‰æ‹©å™¨
            page_param: é¡µç å‚æ•°å
            start_page: èµ·å§‹é¡µç 
            max_pages: æœ€å¤§é¡µæ•°
            wait_time: æ¯é¡µç­‰å¾…æ—¶é—´
            
        Returns:
            List[TableData]: æ‰€æœ‰é¡µé¢çš„æ•°æ®
        """
        page_count = start_page
        
        while True:
            if max_pages > 0 and (page_count - start_page + 1) > max_pages:
                print(f"âœ… è¾¾åˆ°æœ€å¤§é¡µæ•°é™åˆ¶: {max_pages}")
                break
            
            # æ„é€  URL
            separator = "&" if "?" in base_url else "?"
            url = f"{base_url}{separator}{page_param}={page_count}"
            
            print(f"ğŸ“„ æå–ç¬¬ {page_count} é¡µ...")
            print(f"   URL: {url}")
            
            try:
                # å¯¼èˆªåˆ°é¡µé¢
                await self.page.goto(url)
                await asyncio.sleep(wait_time)
                
                # æ£€æŸ¥è¡¨æ ¼æ˜¯å¦å­˜åœ¨
                table = self.page.locator(table_selector)
                is_visible = await table.is_visible()
                
                if not is_visible:
                    print(f"âœ… å·²åˆ°è¾¾æœ€åä¸€é¡µï¼ˆè¡¨æ ¼ä¸å­˜åœ¨ï¼‰")
                    break
                
                # æå–æ•°æ®
                data = await self.extract_table(table_selector)
                
                # æ£€æŸ¥æ˜¯å¦æœ‰æ•°æ®
                if data.total_rows == 0:
                    print(f"âœ… å·²åˆ°è¾¾æœ€åä¸€é¡µï¼ˆæ— æ•°æ®ï¼‰")
                    break
                
                self.all_data.append(data)
                print(f"   âœ“ æå– {data.total_rows} è¡Œæ•°æ®")
                
                page_count += 1
                
            except Exception as e:
                print(f"âœ… å·²åˆ°è¾¾æœ€åä¸€é¡µ: {str(e)}")
                break
        
        return self.all_data
    
    def merge_all_data(self) -> Dict[str, Any]:
        """
        åˆå¹¶æ‰€æœ‰é¡µé¢çš„æ•°æ®
        
        Returns:
            dict: åˆå¹¶åçš„æ•°æ®
        """
        if not self.all_data:
            return {"headers": [], "rows": [], "total_pages": 0, "total_rows": 0}
        
        # ä½¿ç”¨ç¬¬ä¸€é¡µçš„è¡¨å¤´
        headers = self.all_data[0].headers
        
        # åˆå¹¶æ‰€æœ‰è¡Œ
        all_rows = []
        for page_data in self.all_data:
            all_rows.extend(page_data.rows)
        
        return {
            "headers": headers,
            "rows": all_rows,
            "total_pages": len(self.all_data),
            "total_rows": len(all_rows)
        }
    
    def save_to_csv(self, filename: str = "table_data.csv"):
        """ä¿å­˜ä¸º CSV æ–‡ä»¶"""
        merged = self.merge_all_data()
        
        with open(filename, 'w', newline='', encoding='utf-8') as f:
            writer = csv.writer(f)
            writer.writerow(merged["headers"])
            writer.writerows(merged["rows"])
        
        print(f"ğŸ’¾ æ•°æ®å·²ä¿å­˜åˆ°: {filename}")
        print(f"   æ€»é¡µæ•°: {merged['total_pages']}")
        print(f"   æ€»è¡Œæ•°: {merged['total_rows']}")
    
    def save_to_json(self, filename: str = "table_data.json"):
        """ä¿å­˜ä¸º JSON æ–‡ä»¶"""
        merged = self.merge_all_data()
        
        # è½¬æ¢ä¸ºå­—å…¸åˆ—è¡¨
        data_list = []
        headers = merged["headers"]
        for row in merged["rows"]:
            row_dict = {headers[i]: row[i] for i in range(len(headers))}
            data_list.append(row_dict)
        
        output = {
            "metadata": {
                "total_pages": merged["total_pages"],
                "total_rows": merged["total_rows"],
                "headers": headers
            },
            "data": data_list
        }
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(output, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ’¾ æ•°æ®å·²ä¿å­˜åˆ°: {filename}")
        print(f"   æ€»é¡µæ•°: {merged['total_pages']}")
        print(f"   æ€»è¡Œæ•°: {merged['total_rows']}")