"""
åˆ†é¡µè¡¨æ ¼æŠ“å–ç¤ºä¾‹
æ¼”ç¤ºå¦‚ä½•ä½¿ç”¨ TableScraper æ”¶é›†ä¸åŒç±»å‹çš„åˆ†é¡µè¡¨æ ¼
"""

import asyncio
from dotenv import load_dotenv
from lib.browser import BrowserManager
from .table_scraper import TableScraper

load_dotenv()


# ==========================================
# ç¤ºä¾‹ 1: ä½¿ç”¨"ä¸‹ä¸€é¡µ"æŒ‰é’®çš„åˆ†é¡µè¡¨æ ¼
# ==========================================

async def example_button_pagination():
    """
    ç¤ºä¾‹ï¼šæŠ“å–ä½¿ç”¨"ä¸‹ä¸€é¡µ"æŒ‰é’®çš„è¡¨æ ¼
    é€‚ç”¨äºï¼šç”µå•†ç½‘ç«™ã€æ–°é—»åˆ—è¡¨ç­‰
    """
    print("\n" + "="*60)
    print("ğŸ“Œ ç¤ºä¾‹ 1: æŒ‰é’®åˆ†é¡µ")
    print("="*60 + "\n")
    
    async with BrowserManager(mode="connect") as bm:
        page = await bm.get_or_create_page()
        scraper = TableScraper(page)
        
        # 1. å¯¼èˆªåˆ°ç›®æ ‡é¡µé¢
        url = "https://example.com/products"  # æ›¿æ¢ä¸ºå®é™… URL
        print(f"ğŸŒ è®¿é—®: {url}")
        await page.goto(url)
        
        # 2. æŠ“å–æ‰€æœ‰é¡µé¢
        await scraper.scrape_with_button_pagination(
            table_selector="table.product-list",  # è¡¨æ ¼é€‰æ‹©å™¨
            next_button_selector="button.next-page",  # ä¸‹ä¸€é¡µæŒ‰é’®
            max_pages=5,  # æœ€å¤šæŠ“å– 5 é¡µ
            wait_time=2.0  # æ¯é¡µç­‰å¾… 2 ç§’
        )
        
        # 3. ä¿å­˜æ•°æ®
        scraper.save_to_csv("products.csv")
        scraper.save_to_json("products.json")


# ==========================================
# ç¤ºä¾‹ 2: ä½¿ç”¨é¡µç çš„åˆ†é¡µè¡¨æ ¼
# ==========================================

async def example_number_pagination():
    """
    ç¤ºä¾‹ï¼šæŠ“å–ä½¿ç”¨é¡µç ï¼ˆ1, 2, 3...ï¼‰çš„è¡¨æ ¼
    é€‚ç”¨äºï¼šè®ºå›ã€åšå®¢ç­‰
    """
    print("\n" + "="*60)
    print("ğŸ“Œ ç¤ºä¾‹ 2: é¡µç åˆ†é¡µ")
    print("="*60 + "\n")
    
    async with BrowserManager(mode="connect") as bm:
        page = await bm.get_or_create_page()
        scraper = TableScraper(page)
        
        # 1. å¯¼èˆªåˆ°ç¬¬ä¸€é¡µ
        url = "https://example.com/articles"
        print(f"ğŸŒ è®¿é—®: {url}")
        await page.goto(url)
        
        # 2. æŠ“å–æ‰€æœ‰é¡µé¢
        await scraper.scrape_with_page_numbers(
            table_selector="table#articles",
            page_number_selector="a[data-page='{page}']",  # {page} ä¼šè¢«æ›¿æ¢
            max_pages=10,
            wait_time=1.5
        )
        
        # 3. ä¿å­˜æ•°æ®
        scraper.save_to_csv("articles.csv")


# ==========================================
# ç¤ºä¾‹ 3: ä½¿ç”¨ URL å‚æ•°çš„åˆ†é¡µè¡¨æ ¼
# ==========================================

async def example_url_pagination():
    """
    ç¤ºä¾‹ï¼šæŠ“å–ä½¿ç”¨ URL å‚æ•°çš„è¡¨æ ¼ï¼ˆ?page=1ï¼‰
    é€‚ç”¨äºï¼šAPI ç»“æœã€æœç´¢ç»“æœç­‰
    """
    print("\n" + "="*60)
    print("ğŸ“Œ ç¤ºä¾‹ 3: URL å‚æ•°åˆ†é¡µ")
    print("="*60 + "\n")
    
    async with BrowserManager(mode="connect") as bm:
        page = await bm.get_or_create_page()
        scraper = TableScraper(page)
        
        # æŠ“å–æ‰€æœ‰é¡µé¢
        await scraper.scrape_with_url_params(
            base_url="https://example.com/search?q=python",
            table_selector="table.results",
            page_param="page",
            start_page=1,
            max_pages=20,
            wait_time=1.0
        )
        
        # ä¿å­˜æ•°æ®
        scraper.save_to_json("search_results.json")


# ==========================================
# ç¤ºä¾‹ 4: è‡ªå®šä¹‰è¡¨æ ¼é€‰æ‹©å™¨
# ==========================================

async def example_custom_selectors():
    """
    ç¤ºä¾‹ï¼šä½¿ç”¨è‡ªå®šä¹‰é€‰æ‹©å™¨æå–ç‰¹å®šæ ¼å¼çš„è¡¨æ ¼
    """
    print("\n" + "="*60)
    print("ğŸ“Œ ç¤ºä¾‹ 4: è‡ªå®šä¹‰é€‰æ‹©å™¨")
    print("="*60 + "\n")
    
    async with BrowserManager(mode="connect") as bm:
        page = await bm.get_or_create_page()
        scraper = TableScraper(page)
        
        # å¯¼èˆªåˆ°é¡µé¢
        await page.goto("https://example.com/data")
        
        # æå–å•é¡µæ•°æ®ï¼ˆè‡ªå®šä¹‰é€‰æ‹©å™¨ï¼‰
        data = await scraper.extract_table(
            table_selector="div.data-table",  # ä¸æ˜¯æ ‡å‡† <table>
            headers_selector="div.header span",  # è‡ªå®šä¹‰è¡¨å¤´
            rows_selector="div.row",  # è‡ªå®šä¹‰è¡Œ
            cells_selector="div.cell"  # è‡ªå®šä¹‰å•å…ƒæ ¼
        )
        
        print(f"âœ… æå–åˆ° {data.total_rows} è¡Œæ•°æ®")
        print(f"è¡¨å¤´: {data.headers}")


# ==========================================
# ç¤ºä¾‹ 5: å®æˆ˜ - æŠ“å– GitHub Trending
# ==========================================

async def example_github_trending():
    """
    å®æˆ˜ç¤ºä¾‹ï¼šæŠ“å– GitHub Trending è¡¨æ ¼
    """
    print("\n" + "="*60)
    print("ğŸ“Œ ç¤ºä¾‹ 5: GitHub Trending å®æˆ˜")
    print("="*60 + "\n")
    
    async with BrowserManager(mode="connect") as bm:
        page = await bm.get_or_create_page()
        scraper = TableScraper(page)
        
        # 1. è®¿é—® GitHub Trending
        url = "https://github.com/trending"
        print(f"ğŸŒ è®¿é—®: {url}")
        await page.goto(url)
        
        # 2. ç­‰å¾…è¡¨æ ¼åŠ è½½
        await page.wait_for_selector("article.Box-row", timeout=10000)
        
        # 3. è‡ªå®šä¹‰æå–é€»è¾‘ï¼ˆGitHub ä¸æ˜¯æ ‡å‡†è¡¨æ ¼ï¼‰
        print("ğŸ“Š æå–é¡¹ç›®åˆ—è¡¨...")
        
        articles = await page.locator("article.Box-row").all()
        
        headers = ["Rank", "Repository", "Description", "Language", "Stars Today"]
        rows = []
        
        for i, article in enumerate(articles[:25], 1):  # å‰ 25 ä¸ª
            try:
                # æå–é¡¹ç›®å
                repo_name = await article.locator("h2 a").text_content()
                repo_name = repo_name.strip().replace("\n", "").replace("  ", "")
                
                # æå–æè¿°
                desc_elem = article.locator("p")
                description = await desc_elem.text_content() if await desc_elem.count() > 0 else "N/A"
                description = description.strip() if description else "N/A"
                
                # æå–è¯­è¨€
                lang_elem = article.locator("span[itemprop='programmingLanguage']")
                language = await lang_elem.text_content() if await lang_elem.count() > 0 else "N/A"
                
                # æå–ä»Šæ—¥æ˜Ÿæ•°
                stars_elem = article.locator("span.float-sm-right")
                stars = await stars_elem.text_content() if await stars_elem.count() > 0 else "N/A"
                stars = stars.strip()
                
                rows.append([str(i), repo_name, description, language, stars])
                
            except Exception as e:
                print(f"âš ï¸  è·³è¿‡é¡¹ç›® {i}: {e}")
                continue
        
        # 4. æ‰‹åŠ¨åˆ›å»º TableData
        from table_scraper import TableData
        data = TableData(
            headers=headers,
            rows=rows,
            page_number=1,
            total_rows=len(rows)
        )
        scraper.all_data.append(data)
        
        # 5. ä¿å­˜æ•°æ®
        scraper.save_to_csv("github_trending.csv")
        scraper.save_to_json("github_trending.json")
        
        print(f"\nâœ… æˆåŠŸæå– {len(rows)} ä¸ªé¡¹ç›®")


# ==========================================
# ç¤ºä¾‹ 6: å¤„ç†åŠ¨æ€åŠ è½½çš„è¡¨æ ¼
# ==========================================

async def example_dynamic_table():
    """
    ç¤ºä¾‹ï¼šå¤„ç†é€šè¿‡ JavaScript åŠ¨æ€åŠ è½½çš„è¡¨æ ¼
    """
    print("\n" + "="*60)
    print("ğŸ“Œ ç¤ºä¾‹ 6: åŠ¨æ€åŠ è½½è¡¨æ ¼")
    print("="*60 + "\n")
    
    async with BrowserManager(mode="connect") as bm:
        page = await bm.get_or_create_page()
        scraper = TableScraper(page)
        
        # è®¿é—®é¡µé¢
        await page.goto("https://example.com/dynamic-table")
        
        # ç­‰å¾… JavaScript åŠ è½½å®Œæˆ
        await page.wait_for_load_state("networkidle")
        
        # ç­‰å¾…ç‰¹å®šå…ƒç´ å‡ºç°
        await page.wait_for_selector("table tbody tr", timeout=15000)
        
        # æå–æ•°æ®
        data = await scraper.extract_table()
        
        print(f"âœ… æå– {data.total_rows} è¡Œæ•°æ®")
        scraper.save_to_csv("dynamic_data.csv")


# ==========================================
# ä¸»èœå•
# ==========================================

async def main():
    """ä¸»èœå•"""
    examples = {
        "1": ("æŒ‰é’®åˆ†é¡µ", example_button_pagination),
        "2": ("é¡µç åˆ†é¡µ", example_number_pagination),
        "3": ("URL å‚æ•°åˆ†é¡µ", example_url_pagination),
        "4": ("è‡ªå®šä¹‰é€‰æ‹©å™¨", example_custom_selectors),
        "5": ("GitHub Trending å®æˆ˜", example_github_trending),
        "6": ("åŠ¨æ€åŠ è½½è¡¨æ ¼", example_dynamic_table)
    }
    
    print("\n" + "="*60)
    print("ğŸ“ åˆ†é¡µè¡¨æ ¼æŠ“å–ç¤ºä¾‹")
    print("="*60)
    print("\nå¯ç”¨ç¤ºä¾‹:")
    for key, (name, _) in examples.items():
        print(f"   {key}. {name}")
    
    choice = input("\né€‰æ‹©ç¤ºä¾‹ (1-6): ").strip()
    
    if choice in examples:
        name, func = examples[choice]
        print(f"\nğŸš€ è¿è¡Œç¤ºä¾‹: {name}")
        await func()
    else:
        print("âŒ æ— æ•ˆé€‰æ‹©")


if __name__ == "__main__":
    asyncio.run(main())