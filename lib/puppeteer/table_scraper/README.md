# ğŸ“Š åˆ†é¡µè¡¨æ ¼æŠ“å–å®Œæ•´æŒ‡å—

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. å®‰è£…ä¾èµ–

```bash
pip install beautifulsoup4
```

### 2. å°†æ–°æ–‡ä»¶æ·»åŠ åˆ°é¡¹ç›®

```
agent-python/
â”œâ”€â”€ lib/
â”‚   â”œâ”€â”€ table_scraper.py        # è¡¨æ ¼æå–æ ¸å¿ƒ
â”‚   â”œâ”€â”€ table_tools.py          # Agent å·¥å…·
â”‚   â”œâ”€â”€ scrape_table_example.py # ä½¿ç”¨ç¤ºä¾‹
â”‚   â””â”€â”€ agent_scrape_table.py   # Agent è‡ªåŠ¨æŠ“å–
```

---

## ğŸ¯ ä¸‰ç§ä½¿ç”¨æ–¹å¼

### æ–¹å¼ 1: ç›´æ¥ä½¿ç”¨ TableScraperï¼ˆæ‰‹åŠ¨ï¼‰

é€‚åˆï¼š**å·²çŸ¥è¡¨æ ¼ç»“æ„å’Œåˆ†é¡µæ–¹å¼**

```python
from table_scraper import TableScraper
from browser import BrowserManager

async with BrowserManager(mode="connect") as bm:
    page = await bm.get_or_create_page()
    scraper = TableScraper(page)
    
    # è®¿é—®é¡µé¢
    await page.goto("https://example.com/data")
    
    # æŠ“å–åˆ†é¡µè¡¨æ ¼
    await scraper.scrape_with_button_pagination(
        table_selector="table.data",
        next_button_selector="button.next",
        max_pages=10
    )
    
    # ä¿å­˜æ•°æ®
    scraper.save_to_csv("output.csv")
```

### æ–¹å¼ 2: è¿è¡Œç¤ºä¾‹è„šæœ¬ï¼ˆåŠè‡ªåŠ¨ï¼‰

é€‚åˆï¼š**å­¦ä¹ å’Œæµ‹è¯•ä¸åŒåœºæ™¯**

```bash
python lib/scrape_table_example.py
```

é€‰æ‹©é¢„å®šä¹‰çš„ç¤ºä¾‹ï¼š
1. æŒ‰é’®åˆ†é¡µ
2. é¡µç åˆ†é¡µ
3. URL å‚æ•°åˆ†é¡µ
4. GitHub Trending å®æˆ˜
5. ç­‰ç­‰...

### æ–¹å¼ 3: Agent è‡ªåŠ¨æŠ“å–ï¼ˆå…¨è‡ªåŠ¨ï¼‰

é€‚åˆï¼š**è®© AI è‡ªä¸»è¯†åˆ«å’ŒæŠ“å–**

```bash
python lib/agent_scrape_table.py
```

åªéœ€æè¿°ä»»åŠ¡ï¼ŒAgent ä¼šï¼š
- âœ… è‡ªåŠ¨å¯¼èˆªåˆ°é¡µé¢
- âœ… åˆ†æè¡¨æ ¼ç»“æ„
- âœ… é€‰æ‹©åˆé€‚çš„æŠ“å–æ–¹æ³•
- âœ… ä¿å­˜æ•°æ®

---

## ğŸ“ å¸¸è§åœºæ™¯ç¤ºä¾‹

### åœºæ™¯ 1: ç”µå•†äº§å“åˆ—è¡¨ï¼ˆæŒ‰é’®åˆ†é¡µï¼‰

**ç‰¹å¾ï¼š**
- æ ‡å‡† HTML è¡¨æ ¼
- "ä¸‹ä¸€é¡µ"æŒ‰é’®
- æ¯é¡µæ˜¾ç¤ºå›ºå®šæ•°é‡

**ä»£ç ï¼š**

```python
await scraper.scrape_with_button_pagination(
    table_selector="table#products",
    next_button_selector="button[aria-label='Next']",
    max_pages=5,
    wait_time=2.0
)
```

**å®é™…ç½‘ç«™ç¤ºä¾‹ï¼š**
- æ·˜å®å•†å“åˆ—è¡¨
- äº¬ä¸œæœç´¢ç»“æœ
- äºšé©¬é€Šäº§å“é¡µ

---

### åœºæ™¯ 2: è®ºå›å¸–å­åˆ—è¡¨ï¼ˆé¡µç åˆ†é¡µï¼‰

**ç‰¹å¾ï¼š**
- åº•éƒ¨æœ‰é¡µç  1, 2, 3, ...
- å¯ä»¥ç›´æ¥è·³è½¬åˆ°æŒ‡å®šé¡µ
- é€šå¸¸æœ‰"é¦–é¡µ"ã€"å°¾é¡µ"æŒ‰é’®

**ä»£ç ï¼š**

```python
await scraper.scrape_with_page_numbers(
    table_selector="table.threads",
    page_number_selector="a.page-{page}",  # {page} è‡ªåŠ¨æ›¿æ¢
    max_pages=20
)
```

**å®é™…ç½‘ç«™ç¤ºä¾‹ï¼š**
- V2EX ä¸»é¢˜åˆ—è¡¨
- Stack Overflow é—®é¢˜åˆ—è¡¨
- Reddit è®ºå›

---

### åœºæ™¯ 3: API ç»“æœå±•ç¤ºï¼ˆURL å‚æ•°åˆ†é¡µï¼‰

**ç‰¹å¾ï¼š**
- URL åŒ…å«é¡µç å‚æ•°ï¼š`?page=1`
- æ¯ä¸ªé¡µé¢ç‹¬ç«‹è®¿é—®
- é€‚åˆçˆ¬å–æœç´¢ç»“æœ

**ä»£ç ï¼š**

```python
await scraper.scrape_with_url_params(
    base_url="https://api.example.com/search?q=python",
    table_selector="table.results",
    page_param="page",
    start_page=1,
    max_pages=10
)
```

**å®é™…ç½‘ç«™ç¤ºä¾‹ï¼š**
- GitHub æœç´¢ç»“æœ
- Google Scholar
- æ‹›è˜ç½‘ç«™èŒä½åˆ—è¡¨

---

### åœºæ™¯ 4: éæ ‡å‡†è¡¨æ ¼ï¼ˆè‡ªå®šä¹‰é€‰æ‹©å™¨ï¼‰

**ç‰¹å¾ï¼š**
- ä¸æ˜¯ `<table>` æ ‡ç­¾
- ä½¿ç”¨ `<div>` æˆ–å…¶ä»–å…ƒç´ æ¨¡æ‹Ÿè¡¨æ ¼
- éœ€è¦è‡ªå®šä¹‰é€‰æ‹©å™¨

**ä»£ç ï¼š**

```python
data = await scraper.extract_table(
    table_selector="div.data-grid",
    headers_selector="div.header > span",
    rows_selector="div.row",
    cells_selector="div.cell"
)
```

**å®é™…ç½‘ç«™ç¤ºä¾‹ï¼š**
- ç°ä»£å•é¡µåº”ç”¨ (SPA)
- React/Vue æ„å»ºçš„è¡¨æ ¼
- å“åº”å¼è®¾è®¡çš„è¡¨æ ¼

---

## ğŸ” å¦‚ä½•è¯†åˆ«åˆ†é¡µç±»å‹ï¼Ÿ

### æ­¥éª¤ 1: æ‰“å¼€æµè§ˆå™¨å¼€å‘è€…å·¥å…·

1. æŒ‰ `F12` æ‰“å¼€å¼€å‘è€…å·¥å…·
2. ç‚¹å‡»"å…ƒç´ "é€‰é¡¹å¡
3. æ‰¾åˆ°åˆ†é¡µæ§ä»¶

### æ­¥éª¤ 2: è¯†åˆ«åˆ†é¡µæœºåˆ¶

#### âœ… æŒ‰é’®åˆ†é¡µ

**ç‰¹å¾ï¼š**
```html
<button class="next-page">ä¸‹ä¸€é¡µ</button>
<button id="btnNext">Next â†’</button>
```

**é€‰æ‹©å™¨ï¼š**
```python
next_button_selector="button.next-page"
# æˆ–
next_button_selector="button#btnNext"
```

---

#### âœ… é¡µç åˆ†é¡µ

**ç‰¹å¾ï¼š**
```html
<a class="page-link" data-page="1">1</a>
<a class="page-link" data-page="2">2</a>
<a class="page-link" data-page="3">3</a>
```

**é€‰æ‹©å™¨ï¼š**
```python
page_number_selector="a.page-link[data-page='{page}']"
```

---

#### âœ… URL å‚æ•°åˆ†é¡µ

**ç‰¹å¾ï¼š**
```
https://example.com/list?page=1
https://example.com/list?page=2
```

**æ£€æŸ¥æ–¹æ³•ï¼š**
- ç‚¹å‡»é¡µç ï¼Œè§‚å¯Ÿ URL æ˜¯å¦å˜åŒ–
- å¦‚æœå˜åŒ–ï¼Œè®°å½•å‚æ•°åï¼ˆå¦‚ `page`ã€`p`ã€`pageNum` ç­‰ï¼‰

**ä½¿ç”¨ï¼š**
```python
await scraper.scrape_with_url_params(
    base_url="https://example.com/list",
    page_param="page"  # å‚æ•°å
)
```

---

## ğŸ› ï¸ å¸¸è§é—®é¢˜è§£å†³

### Q1: è¡¨æ ¼åŠ è½½å¾ˆæ…¢æ€ä¹ˆåŠï¼Ÿ

**A:** å¢åŠ ç­‰å¾…æ—¶é—´

```python
await scraper.scrape_with_button_pagination(
    ...
    wait_time=5.0  # å¢åŠ åˆ° 5 ç§’
)
```

æˆ–ç­‰å¾…ç‰¹å®šå…ƒç´ ï¼š

```python
await page.wait_for_selector("table tbody tr", timeout=10000)
```

---

### Q2: è¡¨æ ¼æ˜¯åŠ¨æ€åŠ è½½çš„ï¼ˆAJAXï¼‰

**A:** ç­‰å¾…ç½‘ç»œè¯·æ±‚å®Œæˆ

```python
# ç­‰å¾…ç½‘ç»œç©ºé—²
await page.wait_for_load_state("networkidle")

# æˆ–ç­‰å¾…ç‰¹å®šæ•°æ®å‡ºç°
await page.wait_for_selector("table tr[data-loaded='true']")
```

---

### Q3: åˆ†é¡µæŒ‰é’®è¢«ç¦ç”¨æ€ä¹ˆæ£€æµ‹ï¼Ÿ

**A:** æ£€æŸ¥æŒ‰é’®çŠ¶æ€

```python
next_button = page.locator("button.next")

# æ£€æŸ¥æ˜¯å¦ç¦ç”¨
is_disabled = await next_button.is_disabled()
if is_disabled:
    print("å·²åˆ°æœ€åä¸€é¡µ")
    break

# æˆ–æ£€æŸ¥ class
has_disabled_class = await next_button.evaluate(
    "el => el.classList.contains('disabled')"
)
```

---

### Q4: å¦‚ä½•å¤„ç†éªŒè¯ç æˆ–ç™»å½•ï¼Ÿ

**A:** ä½¿ç”¨ `connect` æ¨¡å¼ï¼Œåœ¨æŠ“å–å‰æ‰‹åŠ¨ç™»å½•

```bash
# 1. å¯åŠ¨ Chrome
chrome.exe --remote-debugging-port=9222

# 2. æ‰‹åŠ¨ç™»å½•ç½‘ç«™

# 3. è¿è¡Œè„šæœ¬
python agent_scrape_table.py
```

---

### Q5: è¡¨æ ¼ç»“æ„å¤æ‚ï¼Œæœ‰åˆå¹¶å•å…ƒæ ¼

**A:** ä½¿ç”¨ BeautifulSoup è‡ªå®šä¹‰è§£æ

```python
from bs4 import BeautifulSoup

# è·å– HTML
html = await page.content()
soup = BeautifulSoup(html, 'html.parser')

# è‡ªå®šä¹‰è§£æé€»è¾‘
table = soup.find('table', class_='complex')
for row in table.find_all('tr'):
    cells = row.find_all(['td', 'th'])
    # å¤„ç† rowspanã€colspan
    for cell in cells:
        rowspan = int(cell.get('rowspan', 1))
        colspan = int(cell.get('colspan', 1))
        # ä½ çš„é€»è¾‘...
```

---

## ğŸ“Š æ•°æ®æ ¼å¼å¯¹æ¯”

### CSV æ ¼å¼ï¼ˆæ¨èç”¨äºè¡¨æ ¼æ•°æ®ï¼‰

**ä¼˜ç‚¹ï¼š**
- âœ… Excel å¯ç›´æ¥æ‰“å¼€
- âœ… æ–‡ä»¶å°
- âœ… æ˜“äºå¯¼å…¥æ•°æ®åº“

**ç¼ºç‚¹ï¼š**
- âŒ ä¸æ”¯æŒå¤æ‚åµŒå¥—
- âŒ ç‰¹æ®Šå­—ç¬¦å¯èƒ½æœ‰é—®é¢˜

**ç¤ºä¾‹ï¼š**
```csv
Name,Age,City
Alice,25,Beijing
Bob,30,Shanghai
```

---

### JSON æ ¼å¼ï¼ˆæ¨èç”¨äº API äº¤äº’ï¼‰

**ä¼˜ç‚¹ï¼š**
- âœ… æ”¯æŒåµŒå¥—ç»“æ„
- âœ… æ˜“äºç¨‹åºå¤„ç†
- âœ… ä¿ç•™æ•°æ®ç±»å‹

**ç¼ºç‚¹ï¼š**
- âŒ æ–‡ä»¶è¾ƒå¤§
- âŒ ä¸èƒ½ç›´æ¥ç”¨ Excel æ‰“å¼€

**ç¤ºä¾‹ï¼š**
```json
{
  "metadata": {
    "total_pages": 3,
    "total_rows": 150
  },
  "data": [
    {"Name": "Alice", "Age": 25, "City": "Beijing"},
    {"Name": "Bob", "Age": 30, "City": "Shanghai"}
  ]
}
```

---

## ğŸ¯ å®æˆ˜æ£€æŸ¥æ¸…å•

æŠ“å–å‰æ£€æŸ¥ï¼š

- [ ] ç¡®è®¤ç›®æ ‡ç½‘ç«™å…è®¸çˆ¬å–ï¼ˆæŸ¥çœ‹ robots.txtï¼‰
- [ ] è¯†åˆ«è¡¨æ ¼é€‰æ‹©å™¨
- [ ] ç¡®å®šåˆ†é¡µç±»å‹
- [ ] æµ‹è¯•å•é¡µæå–
- [ ] ä¼°ç®—æ€»é¡µæ•°
- [ ] è®¾ç½®åˆç†çš„ç­‰å¾…æ—¶é—´
- [ ] å‡†å¤‡å¥½ä¿å­˜è·¯å¾„

æŠ“å–ä¸­æ³¨æ„ï¼š

- [ ] ç›‘æ§æ§åˆ¶å°è¾“å‡º
- [ ] æ£€æŸ¥æ•°æ®å®Œæ•´æ€§
- [ ] å¤„ç†å¼‚å¸¸ï¼ˆç½‘ç»œé”™è¯¯ã€è¶…æ—¶ï¼‰
- [ ] é¿å…è¿‡äºé¢‘ç¹çš„è¯·æ±‚

æŠ“å–åéªŒè¯ï¼š

- [ ] æ‰“å¼€ CSV/JSON æ£€æŸ¥æ•°æ®
- [ ] éªŒè¯è¡Œæ•°æ˜¯å¦æ­£ç¡®
- [ ] æ£€æŸ¥æ˜¯å¦æœ‰é‡å¤æ•°æ®
- [ ] ç¡®è®¤ç‰¹æ®Šå­—ç¬¦æ­£å¸¸æ˜¾ç¤º

---

## ğŸ’¡ é«˜çº§æŠ€å·§

### æŠ€å·§ 1: å¹¶å‘æŠ“å–ï¼ˆè°¨æ…ä½¿ç”¨ï¼‰

```python
import asyncio

# åŒæ—¶æŠ“å–å¤šä¸ªé¡µé¢
tasks = [
    scraper.scrape_with_url_params(f"https://example.com?page={i}")
    for i in range(1, 11)
]
results = await asyncio.gather(*tasks)
```

âš ï¸ **æ³¨æ„ï¼š** å¯èƒ½è¢«ç½‘ç«™é™æµæˆ–å°ç¦

---

### æŠ€å·§ 2: å¢é‡æ›´æ–°

```python
# è¯»å–å·²æœ‰æ•°æ®
existing_data = pd.read_csv("data.csv")
last_id = existing_data['id'].max()

# åªæŠ“å–æ–°æ•°æ®
new_data = scraper.scrape_with_condition(
    lambda row: int(row['id']) > last_id
)
```

---

### æŠ€å·§ 3: æ•°æ®æ¸…æ´—

```python
# æ¸…ç†æå–çš„æ•°æ®
for row in data.rows:
    # å»é™¤ç©ºæ ¼
    row = [cell.strip() for cell in row]
    
    # è½¬æ¢æ•°æ®ç±»å‹
    row[1] = int(row[1])  # å¹´é¾„è½¬æ•´æ•°
    
    # å¤„ç†ç©ºå€¼
    row = [cell if cell else 'N/A' for cell in row]
```

---

## ğŸ“š å‚è€ƒèµ„æº

- [Playwright æ–‡æ¡£](https://playwright.dev/python/)
- [BeautifulSoup æ–‡æ¡£](https://www.crummy.com/software/BeautifulSoup/)
- [CSS é€‰æ‹©å™¨å‚è€ƒ](https://www.w3schools.com/cssref/css_selectors.asp)
- [ç½‘é¡µçˆ¬è™«ç¤¼ä»ª](https://www.robotstxt.org/)

---

## âš–ï¸ æ³•å¾‹ä¸é“å¾·

**è¯·éµå®ˆï¼š**
- âœ… æŸ¥çœ‹å¹¶éµå®ˆ robots.txt
- âœ… å°Šé‡ç½‘ç«™æœåŠ¡æ¡æ¬¾
- âœ… é€‚å½“é™åˆ¶è¯·æ±‚é¢‘ç‡
- âœ… æ ‡è¯†çˆ¬è™«èº«ä»½ï¼ˆUser-Agentï¼‰
- âœ… ä»…ç”¨äºä¸ªäººå­¦ä¹ å’Œç ”ç©¶

**ç¦æ­¢ï¼š**
- âŒ çˆ¬å–å—ç‰ˆæƒä¿æŠ¤çš„å†…å®¹
- âŒ å¯¹ç½‘ç«™é€ æˆæ€§èƒ½å½±å“
- âŒ å•†ä¸šç”¨é€”ï¼ˆæœªç»è®¸å¯ï¼‰
- âŒ è§„é¿åçˆ¬è™«æœºåˆ¶
- âŒ çˆ¬å–ä¸ªäººéšç§æ•°æ®

---

## ğŸ†˜ è·å–å¸®åŠ©

é‡åˆ°é—®é¢˜ï¼Ÿ

1. æŸ¥çœ‹é”™è¯¯ä¿¡æ¯
2. æ£€æŸ¥é€‰æ‹©å™¨æ˜¯å¦æ­£ç¡®
3. ä½¿ç”¨ `analyze_table` å·¥å…·
4. æŸ¥çœ‹æµè§ˆå™¨å¼€å‘è€…å·¥å…·
5. æäº¤ Issue

---

ç¥ä½ æŠ“å–é¡ºåˆ©ï¼ğŸ‰