"""
åˆå¹¶æŠ“å–å™¨ä½¿ç”¨ç¤ºä¾‹
Example Usage of Merged Scraper
"""

import asyncio
import sys
from pathlib import Path

# æ·»åŠ libåˆ°è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent.parent))

from browser import BrowserManager
from puppeteer.universal_scraper import create_scraper_config
from puppeteer.merged_scraper import (
    MergedScraper,
    MergedScraperConfig,
    create_merged_scraper_config
)


async def example_1_basic_usage():
    """
    ç¤ºä¾‹1ï¼šåŸºç¡€ä½¿ç”¨ - æŠ“å–æ–‡ç« åˆ—è¡¨å’Œè¯¦æƒ…é¡µ
    """
    print("\n" + "="*60)
    print("ç¤ºä¾‹1ï¼šåŸºç¡€ä½¿ç”¨ - æ–‡ç« åˆ—è¡¨ + è¯¦æƒ…é¡µ")
    print("="*60)
    
    async with BrowserManager(mode="launch", headless=False) as bm:
        page = await bm.get_or_create_page()
        
        # é…ç½®åˆ—è¡¨é¡µæŠ“å–
        list_config = create_scraper_config(
            url="https://segmentfault.com/",
            fields={
                "æ ‡é¢˜": "h3 a.text-body",
                "æ‘˜è¦": ".excerpt",
                "æŠ•ç¥¨": ".num-card .font-size-16",
                "è¯¦æƒ…é“¾æ¥": "h3 a.text-body"  # è¿™ä¸ªå­—æ®µä¼šè¢«ç”¨æ¥æå–href
            },
            container_selector=".list-group-item",
            next_button_selector="a.page-link[rel='next']",
            delay=3.0,
            max_pages=2  # åªæŠ“2é¡µä½œä¸ºç¤ºä¾‹
        )
        
        # é…ç½®è¯¦æƒ…é¡µæŠ“å–
        merged_config = create_merged_scraper_config(
            list_config=list_config,
            detail_fields={
                "æ–‡ç« å†…å®¹": ".article-content",
                "ä½œè€…": ".user-info .name",
                "å‘å¸ƒæ—¶é—´": ".article-meta time",
                "æµè§ˆé‡": ".article-meta .views"
            },
            detail_container_selector=".article-content",  # ç”¨äºç­‰å¾…è¯¦æƒ…é¡µåŠ è½½
            detail_url_field="è¯¦æƒ…é“¾æ¥",  # å¯¹åº”åˆ—è¡¨é…ç½®ä¸­çš„å­—æ®µå
            detail_url_attribute="href",
            navigation_mode="go_back",
            back_wait_time=2.0,
            detail_page_wait_time=2.0,
            max_detail_retries=2,
            continue_on_error=True
        )
        
        # åˆ›å»ºæŠ“å–å™¨å¹¶æ‰§è¡Œ
        scraper = MergedScraper(page, merged_config)
        data = await scraper.scrape()
        
        # ä¿å­˜æ•°æ®
        scraper.save_to_json("example_1_merged_data.json")
        
        # æŸ¥çœ‹éƒ¨åˆ†æ•°æ®
        if data:
            print(f"\né¢„è§ˆç¬¬ä¸€æ¡æ•°æ®ï¼š")
            print(f"åˆ—è¡¨æ•°æ®: {data[0]['list_data']}")
            print(f"è¯¦æƒ…æ•°æ®: {data[0]['detail_data']}")
            print(f"å…ƒæ•°æ®: {data[0]['metadata']}")


async def example_2_opened_page():
    """
    ç¤ºä¾‹2ï¼šåœ¨å·²æ‰“å¼€çš„é¡µé¢ä¸ŠæŠ“å–
    """
    print("\n" + "="*60)
    print("ç¤ºä¾‹2ï¼šåœ¨å·²æ‰“å¼€çš„é¡µé¢ä¸ŠæŠ“å–")
    print("="*60)
    
    async with BrowserManager(mode="connect") as bm:
        # åˆ—å‡ºæ‰€æœ‰æ‰“å¼€çš„é¡µé¢
        pages_info = await bm.list_all_pages()
        print(f"\nå½“å‰æ‰“å¼€çš„æ ‡ç­¾é¡µ ({len(pages_info)} ä¸ª):")
        for i, info in enumerate(pages_info, 1):
            print(f"{i}. {info['title'][:50]}")
            print(f"   {info['url']}\n")
        
        if not pages_info:
            print("âŒ æ²¡æœ‰æ‰¾åˆ°æ‰“å¼€çš„æ ‡ç­¾é¡µ")
            return
        
        # è¿æ¥åˆ°ç›®æ ‡é¡µé¢
        page = await bm.get_or_create_page(target_url="segmentfault.com")
        
        if not page:
            print("âŒ æœªæ‰¾åˆ°ç›®æ ‡é¡µé¢")
            return
        
        print(f"âœ… è¿æ¥åˆ°é¡µé¢: {page.url}")
        
        # é…ç½®åˆ—è¡¨é¡µ
        list_config = create_scraper_config(
            url=page.url,  # ä½¿ç”¨å½“å‰é¡µé¢URL
            fields={
                "æ ‡é¢˜": "h3 a.text-body",
                "è¯¦æƒ…é“¾æ¥": "h3 a.text-body"
            },
            container_selector=".list-group-item",
            next_button_selector="a.page-link[rel='next']",
            delay=2.0,
            max_pages=1  # åªæŠ“1é¡µ
        )
        
        # é…ç½®è¯¦æƒ…é¡µ
        merged_config = create_merged_scraper_config(
            list_config=list_config,
            detail_fields={
                "å†…å®¹": ".article-content",
                "ä½œè€…": ".user-info .name"
            },
            detail_container_selector=".article-content",
            detail_url_field="è¯¦æƒ…é“¾æ¥",
            detail_url_attribute="href",
            continue_on_error=True
        )
        
        # ä»å½“å‰é¡µé¢å¼€å§‹æŠ“å–
        scraper = MergedScraper(page, merged_config)
        data = await scraper.scrape_from_current_page()
        
        scraper.save_to_json("example_2_merged_data.json")


async def example_3_error_handling():
    """
    ç¤ºä¾‹3ï¼šé”™è¯¯å¤„ç†å’Œå®¹é”™
    """
    print("\n" + "="*60)
    print("ç¤ºä¾‹3ï¼šé”™è¯¯å¤„ç†å’Œå®¹é”™")
    print("="*60)
    
    async with BrowserManager(mode="launch", headless=False) as bm:
        page = await bm.get_or_create_page()
        
        # é…ç½®åˆ—è¡¨é¡µ
        list_config = create_scraper_config(
            url="https://segmentfault.com/",
            fields={
                "æ ‡é¢˜": "h3 a.text-body",
                "è¯¦æƒ…é“¾æ¥": "h3 a.text-body"
            },
            container_selector=".list-group-item",
            delay=2.0,
            max_pages=1
        )
        
        # é…ç½®è¯¦æƒ…é¡µï¼ˆæ•…æ„ä½¿ç”¨å¯èƒ½å¤±è´¥çš„é€‰æ‹©å™¨ï¼‰
        merged_config = create_merged_scraper_config(
            list_config=list_config,
            detail_fields={
                "å†…å®¹": ".article-content",
                "ä¸å­˜åœ¨çš„å­—æ®µ": ".non-existent-selector"  # æ•…æ„è®¾ç½®ä¸å­˜åœ¨çš„é€‰æ‹©å™¨
            },
            detail_container_selector=".article-content",
            detail_url_field="è¯¦æƒ…é“¾æ¥",
            detail_url_attribute="href",
            max_detail_retries=2,  # é‡è¯•2æ¬¡
            continue_on_error=True  # å¤±è´¥åç»§ç»­
        )
        
        scraper = MergedScraper(page, merged_config)
        data = await scraper.scrape()
        
        # æŸ¥çœ‹ç»Ÿè®¡ä¿¡æ¯
        stats = scraper.get_stats()
        print(f"\nç»Ÿè®¡ä¿¡æ¯ï¼š")
        print(f"æˆåŠŸ: {stats['successful_details']}")
        print(f"å¤±è´¥: {stats['failed_details']}")
        print(f"è·³è¿‡: {stats['skipped_details']}")
        
        scraper.save_to_json("example_3_merged_data.json")


async def example_4_custom_config():
    """
    ç¤ºä¾‹4ï¼šé«˜çº§é…ç½®
    """
    print("\n" + "="*60)
    print("ç¤ºä¾‹4ï¼šé«˜çº§é…ç½®")
    print("="*60)
    
    async with BrowserManager(mode="launch", headless=False) as bm:
        page = await bm.get_or_create_page()
        
        # ä½¿ç”¨MergedScraperConfigç±»è¿›è¡Œè¯¦ç»†é…ç½®
        from puppeteer.universal_scraper import FieldConfig, ScraperConfig
        
        list_config = ScraperConfig(
            url="https://segmentfault.com/",
            fields=[
                FieldConfig(name="æ ‡é¢˜", selector="h3 a.text-body"),
                FieldConfig(name="è¯¦æƒ…é“¾æ¥", selector="h3 a.text-body", attribute="href")
            ],
            container_selector=".list-group-item",
            next_button_selector="a.page-link[rel='next']",
            delay=2.0,
            max_pages=1
        )
        
        merged_config = MergedScraperConfig(
            list_config=list_config,
            detail_fields=[
                FieldConfig(name="å†…å®¹", selector=".article-content"),
                FieldConfig(name="æ ‡ç­¾", selector=".tag", multiple=True)  # æå–å¤šä¸ªæ ‡ç­¾
            ],
            detail_container_selector=".article-content",
            detail_url_field="è¯¦æƒ…é“¾æ¥",
            detail_url_attribute="href",
            back_wait_time=3.0,  # è¿”å›åˆ—è¡¨é¡µç­‰å¾…æ›´é•¿æ—¶é—´
            detail_page_wait_time=3.0,  # è¯¦æƒ…é¡µç­‰å¾…æ›´é•¿æ—¶é—´
            max_detail_retries=3,  # é‡è¯•3æ¬¡
            continue_on_error=True,
            skip_invalid_urls=True,  # è·³è¿‡æ— æ•ˆURL
            verify_list_page_state=True,  # éªŒè¯åˆ—è¡¨é¡µçŠ¶æ€
            save_partial_results=True  # ä¿å­˜éƒ¨åˆ†ç»“æœ
        )
        
        scraper = MergedScraper(page, merged_config)
        data = await scraper.scrape()
        
        scraper.save_to_json("example_4_merged_data.json")


async def main():
    """ä¸»å‡½æ•°"""
    print("\nğŸ¯ åˆå¹¶æŠ“å–å™¨ç¤ºä¾‹")
    print("é€‰æ‹©è¦è¿è¡Œçš„ç¤ºä¾‹ï¼š")
    print("1. åŸºç¡€ä½¿ç”¨")
    print("2. åœ¨å·²æ‰“å¼€çš„é¡µé¢ä¸ŠæŠ“å–ï¼ˆéœ€è¦å…ˆå¯åŠ¨Chromeï¼‰")
    print("3. é”™è¯¯å¤„ç†")
    print("4. é«˜çº§é…ç½®")
    
    choice = input("\nè¯·è¾“å…¥é€‰é¡¹ (1-4): ").strip()
    
    if choice == "1":
        await example_1_basic_usage()
    elif choice == "2":
        print("\nâš ï¸ è¯·ç¡®ä¿å·²å¯åŠ¨Chrome:")
        print("   chrome.exe --remote-debugging-port=9222")
        print("   å¹¶æ‰“å¼€ç›®æ ‡é¡µé¢ï¼ˆå¦‚ SegmentFaultï¼‰")
        input("\næŒ‰å›è½¦ç»§ç»­...")
        await example_2_opened_page()
    elif choice == "3":
        await example_3_error_handling()
    elif choice == "4":
        await example_4_custom_config()
    else:
        print("æ— æ•ˆçš„é€‰é¡¹")


if __name__ == "__main__":
    asyncio.run(main())
