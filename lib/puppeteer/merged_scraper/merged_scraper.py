"""
åˆ—è¡¨é¡µä¸è¯¦æƒ…é¡µåˆå¹¶æŠ“å–å™¨æ ¸å¿ƒå®ç°
Merged Scraper Core Implementation
"""

import json
import asyncio
from typing import Dict, List, Optional, Any
from datetime import datetime
from playwright.async_api import Page, TimeoutError as PlaywrightTimeout

import sys
from pathlib import Path
sys.path.insert(0, str(Path(__file__).parent.parent.parent))

from puppeteer.universal_scraper import UniversalScraper, FieldConfig
from .config import MergedScraperConfig, NavigationMode


class MergedScraper:
    """
    åˆ—è¡¨é¡µä¸è¯¦æƒ…é¡µåˆå¹¶æŠ“å–å™¨
    
    æ ¸å¿ƒç‰¹æ€§ï¼š
    1. ä¸¥æ ¼é¡ºåºæ‰§è¡Œï¼šList Item A -> Detail A -> List Item B -> Detail B
    2. æ•°æ®ä¸é”™é…ï¼šæ¯æ¡è®°å½•æºå¸¦å”¯ä¸€æ ‡è¯†ï¼ˆlist_page + item_indexï¼‰
    3. åŸå­åˆå¹¶ï¼šåˆ—è¡¨æ•°æ®å’Œè¯¦æƒ…æ•°æ®åœ¨åŒä¸€æ¬¡è¿­ä»£ä¸­åˆå¹¶
    4. é”™è¯¯éš”ç¦»ï¼šå•ä¸ªè¯¦æƒ…é¡µå¤±è´¥ä¸å½±å“å…¶ä»–é¡¹
    5. çŠ¶æ€è¿½è¸ªï¼šå®Œæ•´è®°å½•æ¯æ¡æ•°æ®çš„æŠ“å–çŠ¶æ€
    """
    
    def __init__(self, page: Page, config: MergedScraperConfig):
        """
        åˆå§‹åŒ–åˆå¹¶æŠ“å–å™¨
        
        Args:
            page: Playwrighté¡µé¢å¯¹è±¡
            config: åˆå¹¶æŠ“å–å™¨é…ç½®
        """
        self.page = page
        self.config = config
        self.merged_data: List[Dict[str, Any]] = []
        
        # åˆ›å»ºåˆ—è¡¨é¡µæŠ“å–å™¨
        self.list_scraper = UniversalScraper(page, config.list_config)
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            "total_list_items": 0,
            "successful_details": 0,
            "failed_details": 0,
            "skipped_details": 0,
            "start_time": None,
            "end_time": None
        }
    
    async def _extract_detail_url(self, list_item: Dict[str, Any]) -> Optional[str]:
        """
        ä»åˆ—è¡¨é¡¹ä¸­æå–è¯¦æƒ…é¡µURL
        
        Args:
            list_item: åˆ—è¡¨é¡¹æ•°æ®
            
        Returns:
            è¯¦æƒ…é¡µURLï¼Œå¦‚æœæå–å¤±è´¥è¿”å›None
        """
        url_value = list_item.get(self.config.detail_url_field)
        
        if not url_value:
            return None
        
        # å¦‚æœæ˜¯å­—ç¬¦ä¸²ï¼Œç›´æ¥è¿”å›
        if isinstance(url_value, str):
            # æ£€æŸ¥æ˜¯å¦æ˜¯æœ‰æ•ˆURL
            if url_value.startswith('http://') or url_value.startswith('https://'):
                return url_value
            # ç›¸å¯¹URLï¼Œéœ€è¦è¡¥å…¨
            elif url_value.startswith('/'):
                base_url = self.page.url
                from urllib.parse import urljoin
                return urljoin(base_url, url_value)
            else:
                return None
        
        return None
    
    async def _scrape_detail_page(self, detail_url: str) -> Optional[Dict[str, Any]]:
        """
        æŠ“å–å•ä¸ªè¯¦æƒ…é¡µæ•°æ®
        
        Args:
            detail_url: è¯¦æƒ…é¡µURL
            
        Returns:
            è¯¦æƒ…é¡µæ•°æ®å­—å…¸ï¼Œå¤±è´¥è¿”å›None
        """
        try:
            # å¯¼èˆªåˆ°è¯¦æƒ…é¡µ
            await self.page.goto(detail_url, wait_until="domcontentloaded", timeout=30000)
            
            # ç­‰å¾…è¯¦æƒ…é¡µå®¹å™¨åŠ è½½
            await self.page.wait_for_selector(
                self.config.detail_container_selector,
                timeout=15000
            )
            
            # é¢å¤–ç­‰å¾…æ—¶é—´
            await asyncio.sleep(self.config.detail_page_wait_time)
            
            # æå–è¯¦æƒ…é¡µå­—æ®µ
            detail_data = {}
            for field in self.config.detail_fields:
                try:
                    value = await self._extract_detail_field(field)
                    detail_data[field.name] = value
                except Exception as e:
                    print(f"      âš ï¸ æå–è¯¦æƒ…å­—æ®µå¤±è´¥ [{field.name}]: {e}")
                    detail_data[field.name] = None
            
            return detail_data
            
        except PlaywrightTimeout as e:
            print(f"      âŒ è¯¦æƒ…é¡µåŠ è½½è¶…æ—¶: {str(e)[:100]}")
            return None
        except Exception as e:
            print(f"      âŒ è¯¦æƒ…é¡µæŠ“å–å¤±è´¥: {str(e)[:100]}")
            return None
    
    async def _extract_detail_field(self, field: FieldConfig) -> Any:
        """
        æå–è¯¦æƒ…é¡µå•ä¸ªå­—æ®µçš„å€¼
        
        Args:
            field: å­—æ®µé…ç½®
            
        Returns:
            å­—æ®µå€¼
        """
        locator = self.page.locator(field.selector)
        
        # æ£€æŸ¥å…ƒç´ æ˜¯å¦å­˜åœ¨
        count = await locator.count()
        if count == 0:
            return None
        
        # æå–å¤šä¸ªå€¼
        if field.multiple:
            elements = await locator.all()
            values = []
            for elem in elements:
                if field.attribute:
                    val = await elem.get_attribute(field.attribute)
                else:
                    val = await elem.text_content()
                values.append(val.strip() if val else None)
            return values
        
        # æå–å•ä¸ªå€¼
        if field.attribute:
            value = await locator.first.get_attribute(field.attribute)
        else:
            value = await locator.first.text_content()
        
        return value.strip() if value else None
    
    async def _navigate_back_to_list(self):
        """è¿”å›åˆ—è¡¨é¡µ"""
        if self.config.navigation_mode == NavigationMode.GO_BACK:
            # ä½¿ç”¨æµè§ˆå™¨è¿”å›
            await self.page.go_back(wait_until="domcontentloaded", timeout=15000)
            
            # ç­‰å¾…åˆ—è¡¨é¡µç¨³å®š
            await asyncio.sleep(self.config.back_wait_time)
            
            # éªŒè¯åˆ—è¡¨é¡µçŠ¶æ€
            if self.config.verify_list_page_state:
                await self._verify_list_page_state()
        else:
            # ä½¿ç”¨æ–°æ ‡ç­¾é¡µæ¨¡å¼ï¼ˆå½“å‰æœªå®ç°ï¼Œå¯æ‰©å±•ï¼‰
            raise NotImplementedError("NEW_TAB æ¨¡å¼æš‚æœªå®ç°")
    
    async def _verify_list_page_state(self):
        """éªŒè¯è¿”å›åˆ—è¡¨é¡µåçš„çŠ¶æ€"""
        try:
            # ç­‰å¾…åˆ—è¡¨å®¹å™¨å‡ºç°
            await self.page.wait_for_selector(
                self.config.list_config.container_selector,
                timeout=10000
            )
            
            # ç­‰å¾…ç½‘ç»œç©ºé—²
            await self.page.wait_for_load_state("networkidle", timeout=10000)
            
        except Exception as e:
            print(f"      âš ï¸ åˆ—è¡¨é¡µçŠ¶æ€éªŒè¯å¤±è´¥: {e}")
    
    async def scrape_list_item_with_detail(
        self,
        list_item: Dict[str, Any],
        item_index: int,
        page_num: int
    ) -> Dict[str, Any]:
        """
        æŠ“å–å•ä¸ªåˆ—è¡¨é¡¹åŠå…¶è¯¦æƒ…é¡µæ•°æ®ï¼ˆæ ¸å¿ƒæ–¹æ³•ï¼‰
        
        è¿™æ˜¯é˜²æ­¢æ•°æ®é”™é…çš„å…³é”®ï¼š
        1. åœ¨å•ä¸ªå‡½æ•°è°ƒç”¨ä¸­å®Œæˆåˆ—è¡¨+è¯¦æƒ…çš„æ•°æ®è·å–
        2. ä½¿ç”¨å”¯ä¸€æ ‡è¯†ï¼ˆpage_num + item_indexï¼‰è¿½è¸ª
        3. åŸå­æ€§åˆå¹¶æ•°æ®
        
        Args:
            list_item: åˆ—è¡¨é¡¹æ•°æ®
            item_index: é¡¹åœ¨å½“å‰åˆ—è¡¨é¡µçš„ç´¢å¼•
            page_num: åˆ—è¡¨é¡µç 
            
        Returns:
            åˆå¹¶åçš„æ•°æ®è®°å½•
        """
        print(f"   ğŸ“ å¤„ç†ç¬¬ {item_index + 1} é¡¹...")
        
        # åˆå§‹åŒ–åˆå¹¶è®°å½•
        merged_item = {
            "list_data": list_item.copy(),  # åˆ—è¡¨é¡µæ•°æ®
            "detail_data": {},  # è¯¦æƒ…é¡µæ•°æ®ï¼ˆå¾…å¡«å……ï¼‰
            "metadata": {
                "list_page": page_num,
                "item_index": item_index,
                "detail_url": None,
                "scrape_status": "pending",
                "error_message": None,
                "scraped_at": datetime.now().isoformat()
            }
        }
        
        # æå–è¯¦æƒ…é¡µURL
        detail_url = await self._extract_detail_url(list_item)
        merged_item["metadata"]["detail_url"] = detail_url
        
        if not detail_url:
            print(f"      âš ï¸ æœªæ‰¾åˆ°è¯¦æƒ…é¡µURL")
            merged_item["metadata"]["scrape_status"] = "skipped"
            merged_item["metadata"]["error_message"] = "è¯¦æƒ…é¡µURLä¸ºç©º"
            self.stats["skipped_details"] += 1
            
            if self.config.skip_invalid_urls:
                return merged_item
        
        # æŠ“å–è¯¦æƒ…é¡µï¼ˆå¸¦é‡è¯•ï¼‰
        detail_data = None
        retry_count = 0
        
        while retry_count <= self.config.max_detail_retries and detail_data is None:
            if retry_count > 0:
                print(f"      ğŸ”„ é‡è¯•ç¬¬ {retry_count} æ¬¡...")
            
            try:
                detail_data = await self._scrape_detail_page(detail_url)
                
                if detail_data:
                    print(f"      âœ“ è¯¦æƒ…é¡µæŠ“å–æˆåŠŸ")
                    merged_item["detail_data"] = detail_data
                    merged_item["metadata"]["scrape_status"] = "success"
                    self.stats["successful_details"] += 1
                else:
                    retry_count += 1
                    if retry_count <= self.config.max_detail_retries:
                        await asyncio.sleep(1)  # é‡è¯•å‰ç­‰å¾…
                    
            except Exception as e:
                print(f"      âŒ è¯¦æƒ…é¡µæŠ“å–å¼‚å¸¸: {str(e)[:100]}")
                merged_item["metadata"]["error_message"] = str(e)
                retry_count += 1
                if retry_count <= self.config.max_detail_retries:
                    await asyncio.sleep(1)
        
        # å¦‚æœæœ€ç»ˆå¤±è´¥
        if not detail_data:
            merged_item["metadata"]["scrape_status"] = "failed"
            self.stats["failed_details"] += 1
            
            if not self.config.continue_on_error:
                raise Exception(f"è¯¦æƒ…é¡µæŠ“å–å¤±è´¥ä¸” continue_on_error=False")
        
        # è¿”å›åˆ—è¡¨é¡µ
        try:
            await self._navigate_back_to_list()
        except Exception as e:
            print(f"      âš ï¸ è¿”å›åˆ—è¡¨é¡µå¤±è´¥: {e}")
            merged_item["metadata"]["navigation_error"] = str(e)
        
        return merged_item
    
    async def scrape_current_list_page_with_details(
        self,
        page_num: int
    ) -> List[Dict[str, Any]]:
        """
        æŠ“å–å½“å‰åˆ—è¡¨é¡µçš„æ‰€æœ‰é¡¹åŠå…¶è¯¦æƒ…
        
        Args:
            page_num: å½“å‰é¡µç 
            
        Returns:
            å½“å‰é¡µæ‰€æœ‰åˆå¹¶åçš„æ•°æ®
        """
        print(f"\nğŸ“„ æŠ“å–åˆ—è¡¨é¡µç¬¬ {page_num} é¡µ...")
        
        # æŠ“å–åˆ—è¡¨é¡µæ•°æ®
        list_items = await self.list_scraper.scrape_current_page()
        
        if not list_items:
            print(f"   âš ï¸ åˆ—è¡¨é¡µæ— æ•°æ®")
            return []
        
        print(f"   æ‰¾åˆ° {len(list_items)} ä¸ªåˆ—è¡¨é¡¹")
        self.stats["total_list_items"] += len(list_items)
        
        page_merged_data = []
        
        # é¡ºåºå¤„ç†æ¯ä¸ªåˆ—è¡¨é¡¹
        for index, list_item in enumerate(list_items):
            merged_item = await self.scrape_list_item_with_detail(
                list_item=list_item,
                item_index=index,
                page_num=page_num
            )
            page_merged_data.append(merged_item)
            
            # éƒ¨åˆ†ä¿å­˜ï¼ˆå¯é€‰ï¼‰
            if self.config.save_partial_results and len(page_merged_data) % 5 == 0:
                self._save_partial_results()
        
        return page_merged_data
    
    async def scrape_with_pagination(self) -> List[Dict[str, Any]]:
        """
        æ‰§è¡Œåˆ†é¡µæŠ“å–ï¼ˆåˆ—è¡¨é¡µ+è¯¦æƒ…é¡µï¼‰
        
        æ ¸å¿ƒæµç¨‹ï¼š
        For each åˆ—è¡¨é¡µ:
            For each åˆ—è¡¨é¡¹:
                æŠ“å–è¯¦æƒ…é¡µ
                åˆå¹¶æ•°æ®
                è¿”å›åˆ—è¡¨é¡µ
            ç¿»åˆ°ä¸‹ä¸€é¡µ
        
        Returns:
            æ‰€æœ‰åˆå¹¶åçš„æ•°æ®
        """
        self.stats["start_time"] = datetime.now().isoformat()
        current_page = 1
        
        while True:
            # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°æœ€å¤§é¡µæ•°
            if self.config.list_config.max_pages > 0 and \
               current_page > self.config.list_config.max_pages:
                print(f"\nâœ… è¾¾åˆ°æœ€å¤§é¡µæ•°: {self.config.list_config.max_pages}")
                break
            
            # æŠ“å–å½“å‰åˆ—è¡¨é¡µåŠå…¶è¯¦æƒ…
            try:
                page_data = await self.scrape_current_list_page_with_details(current_page)
                self.merged_data.extend(page_data)
                
            except Exception as e:
                print(f"\nâŒ åˆ—è¡¨é¡µ {current_page} æŠ“å–å¤±è´¥: {e}")
                if not self.config.continue_on_error:
                    break
            
            # æ£€æŸ¥æ˜¯å¦æœ‰ä¸‹ä¸€é¡µ
            if not self.config.list_config.next_button_selector:
                print("\nâœ… æ— åˆ†é¡µé…ç½®ï¼ŒæŠ“å–å®Œæˆ")
                break
            
            # æŸ¥æ‰¾ä¸‹ä¸€é¡µæŒ‰é’®
            next_button = self.page.locator(self.config.list_config.next_button_selector)
            
            try:
                count = await next_button.count()
                if count == 0:
                    print("\nâœ… æœªæ‰¾åˆ°ä¸‹ä¸€é¡µæŒ‰é’®")
                    break
                
                is_visible = await next_button.first.is_visible()
                is_enabled = await next_button.first.is_enabled()
                
                if not is_visible or not is_enabled:
                    print("\nâœ… ä¸‹ä¸€é¡µæŒ‰é’®ä¸å¯ç”¨")
                    break
                
                # ç‚¹å‡»ä¸‹ä¸€é¡µ
                print(f"\nğŸ”„ ç¿»åˆ°ç¬¬ {current_page + 1} é¡µ...")
                await next_button.first.click()
                
                # ç­‰å¾…é¡µé¢åŠ è½½
                await asyncio.sleep(self.config.list_config.delay)
                await self.page.wait_for_load_state("networkidle", timeout=15000)
                
                current_page += 1
                
            except Exception as e:
                print(f"\nâœ… åˆ†é¡µç»“æŸ: {str(e)[:100]}")
                break
        
        self.stats["end_time"] = datetime.now().isoformat()
        return self.merged_data
    
    async def scrape_from_current_page(self) -> List[Dict[str, Any]]:
        """
        ä»å½“å‰é¡µé¢å¼€å§‹æŠ“å–ï¼ˆä¸å¯¼èˆªåˆ°list_config.urlï¼‰
        é€‚ç”¨äºå·²ç»æ‰“å¼€åˆ—è¡¨é¡µçš„åœºæ™¯
        
        Returns:
            æ‰€æœ‰åˆå¹¶åçš„æ•°æ®
        """
        print(f"\n{'='*60}")
        print(f"ğŸš€ å¼€å§‹åˆå¹¶æŠ“å–")
        print(f"{'='*60}")
        print(f"ğŸ“ å½“å‰é¡µé¢: {self.page.url}")
        print(f"ğŸ“‹ åˆ—è¡¨å®¹å™¨: {self.config.list_config.container_selector}")
        print(f"ğŸ“‹ è¯¦æƒ…å®¹å™¨: {self.config.detail_container_selector}")
        
        # ç­‰å¾…åˆ—è¡¨é¡µç¨³å®š
        await asyncio.sleep(self.config.list_config.delay)
        
        # æ‰§è¡Œåˆ†é¡µæŠ“å–
        data = await self.scrape_with_pagination()
        
        # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
        self._print_stats()
        
        return data
    
    async def scrape(self) -> List[Dict[str, Any]]:
        """
        æ‰§è¡ŒæŠ“å–ï¼ˆä»list_config.urlå¼€å§‹ï¼‰
        
        Returns:
            æ‰€æœ‰åˆå¹¶åçš„æ•°æ®
        """
        # å¯¼èˆªåˆ°åˆ—è¡¨é¡µ
        print(f"\nğŸŒ è®¿é—®åˆ—è¡¨é¡µ: {self.config.list_config.url}")
        await self.page.goto(self.config.list_config.url)
        await asyncio.sleep(self.config.list_config.delay)
        
        return await self.scrape_from_current_page()
    
    def _save_partial_results(self):
        """ä¿å­˜éƒ¨åˆ†ç»“æœï¼ˆå†…éƒ¨ä½¿ç”¨ï¼‰"""
        try:
            filename = "partial_merged_data.json"
            self._save_to_file(filename)
            print(f"      ğŸ’¾ éƒ¨åˆ†ç»“æœå·²ä¿å­˜")
        except Exception as e:
            print(f"      âš ï¸ éƒ¨åˆ†ç»“æœä¿å­˜å¤±è´¥: {e}")
    
    def _save_to_file(self, filename: str):
        """ä¿å­˜æ•°æ®åˆ°æ–‡ä»¶ï¼ˆå†…éƒ¨ä½¿ç”¨ï¼‰"""
        output = {
            "metadata": {
                "total_items": len(self.merged_data),
                "statistics": self.stats,
                "config": {
                    "list_url": self.config.list_config.url,
                    "list_fields": [f.name for f in self.config.list_config.fields],
                    "detail_fields": [f.name for f in self.config.detail_fields],
                    "detail_url_field": self.config.detail_url_field
                }
            },
            "data": self.merged_data
        }
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(output, f, ensure_ascii=False, indent=2)
    
    def save_to_json(self, filename: str = "merged_data.json") -> str:
        """
        ä¿å­˜åˆå¹¶åçš„æ•°æ®åˆ°JSONæ–‡ä»¶
        
        Args:
            filename: æ–‡ä»¶å
            
        Returns:
            ä¿å­˜ç»“æœä¿¡æ¯
        """
        if not self.merged_data:
            return "âŒ æ— æ•°æ®å¯ä¿å­˜"
        
        self._save_to_file(filename)
        
        print(f"\n{'='*60}")
        print(f"ğŸ’¾ æ•°æ®å·²ä¿å­˜åˆ°: {filename}")
        print(f"{'='*60}")
        print(f"ğŸ“Š æ€»æ¡ç›®: {len(self.merged_data)}")
        print(f"âœ… æˆåŠŸ: {self.stats['successful_details']}")
        print(f"âŒ å¤±è´¥: {self.stats['failed_details']}")
        print(f"â­ï¸  è·³è¿‡: {self.stats['skipped_details']}")
        
        return filename
    
    def _print_stats(self):
        """æ‰“å°ç»Ÿè®¡ä¿¡æ¯"""
        print(f"\n{'='*60}")
        print(f"ğŸ“Š æŠ“å–ç»Ÿè®¡")
        print(f"{'='*60}")
        print(f"ğŸ“‹ åˆ—è¡¨é¡¹æ€»æ•°: {self.stats['total_list_items']}")
        print(f"âœ… è¯¦æƒ…é¡µæˆåŠŸ: {self.stats['successful_details']}")
        print(f"âŒ è¯¦æƒ…é¡µå¤±è´¥: {self.stats['failed_details']}")
        print(f"â­ï¸  è¯¦æƒ…é¡µè·³è¿‡: {self.stats['skipped_details']}")
        
        if self.stats['total_list_items'] > 0:
            success_rate = (self.stats['successful_details'] / 
                          self.stats['total_list_items'] * 100)
            print(f"ğŸ“ˆ æˆåŠŸç‡: {success_rate:.1f}%")
        
        print(f"{'='*60}")
    
    def get_data(self) -> List[Dict[str, Any]]:
        """è·å–æŠ“å–çš„æ•°æ®"""
        return self.merged_data
    
    def get_stats(self) -> Dict[str, Any]:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        return self.stats.copy()
