# åˆ—è¡¨é¡µä¸è¯¦æƒ…é¡µåˆå¹¶æŠ“å–å™¨

## ğŸ“‹ åŠŸèƒ½æ¦‚è¿°

åˆå¹¶æŠ“å–å™¨ç”¨äºè§£å†³ä¸€ä¸ªå¸¸è§çš„çˆ¬è™«åœºæ™¯ï¼š**ä»åˆ—è¡¨é¡µè·å–æ¦‚è§ˆä¿¡æ¯ï¼Œç„¶åè®¿é—®æ¯ä¸ªè¯¦æƒ…é¡µè·å–å®Œæ•´æ•°æ®**ã€‚

### æ ¸å¿ƒç‰¹æ€§

âœ… **ä¸¥æ ¼é¡ºåºæ‰§è¡Œ**ï¼šList Item A â†’ Detail A â†’ List Item B â†’ Detail B  
âœ… **æ•°æ®ä¸é”™é…**ï¼šæ¯æ¡è®°å½•æºå¸¦å”¯ä¸€æ ‡è¯†ï¼ˆlist_page + item_indexï¼‰  
âœ… **åŸå­åˆå¹¶**ï¼šåˆ—è¡¨æ•°æ®å’Œè¯¦æƒ…æ•°æ®åœ¨åŒä¸€æ¬¡è¿­ä»£ä¸­åˆå¹¶  
âœ… **é”™è¯¯éš”ç¦»**ï¼šå•ä¸ªè¯¦æƒ…é¡µå¤±è´¥ä¸å½±å“å…¶ä»–é¡¹  
âœ… **çŠ¶æ€è¿½è¸ª**ï¼šå®Œæ•´è®°å½•æ¯æ¡æ•°æ®çš„æŠ“å–çŠ¶æ€  

## ğŸ¯ ä½¿ç”¨åœºæ™¯

```
åœºæ™¯ç¤ºä¾‹ï¼šæŠ“å–æ–‡ç« ç½‘ç«™

åˆ—è¡¨é¡µï¼š
â”œâ”€ æ–‡ç« æ ‡é¢˜
â”œâ”€ æ–‡ç« æ‘˜è¦
â”œâ”€ å‘å¸ƒæ—¶é—´
â””â”€ è¯¦æƒ…é“¾æ¥ â†’ ç‚¹å‡»è¿›å…¥è¯¦æƒ…é¡µ

è¯¦æƒ…é¡µï¼š
â”œâ”€ å®Œæ•´å†…å®¹
â”œâ”€ ä½œè€…ä¿¡æ¯
â”œâ”€ æ ‡ç­¾
â””â”€ è¯„è®ºæ•°
```

## ğŸš€ å¿«é€Ÿå¼€å§‹

### åŸºç¡€ç¤ºä¾‹

```python
from browser import BrowserManager
from puppeteer.universal_scraper import create_scraper_config
from puppeteer.merged_scraper import MergedScraper, create_merged_scraper_config

async def main():
    async with BrowserManager(mode="launch") as bm:
        page = await bm.get_or_create_page()
        
        # 1. é…ç½®åˆ—è¡¨é¡µæŠ“å–
        list_config = create_scraper_config(
            url="https://example.com/list",
            fields={
                "æ ‡é¢˜": "h3 a",
                "æ‘˜è¦": ".summary",
                "è¯¦æƒ…é“¾æ¥": "h3 a"  # ç”¨äºæå–è¯¦æƒ…é¡µURL
            },
            container_selector=".list-item",
            next_button_selector=".next-page",
            max_pages=2
        )
        
        # 2. é…ç½®è¯¦æƒ…é¡µæŠ“å–
        merged_config = create_merged_scraper_config(
            list_config=list_config,
            detail_fields={
                "å®Œæ•´å†…å®¹": ".article-content",
                "ä½œè€…": ".author",
                "å‘å¸ƒæ—¶é—´": ".publish-time"
            },
            detail_container_selector=".article-content",
            detail_url_field="è¯¦æƒ…é“¾æ¥",  # å¯¹åº”åˆ—è¡¨å­—æ®µå
            detail_url_attribute="href",
            continue_on_error=True
        )
        
        # 3. æ‰§è¡ŒæŠ“å–
        scraper = MergedScraper(page, merged_config)
        data = await scraper.scrape()
        
        # 4. ä¿å­˜æ•°æ®
        scraper.save_to_json("merged_data.json")
```

### è¾“å‡ºæ•°æ®æ ¼å¼

```json
{
  "metadata": {
    "total_items": 20,
    "statistics": {
      "total_list_items": 20,
      "successful_details": 18,
      "failed_details": 2,
      "skipped_details": 0
    }
  },
  "data": [
    {
      "list_data": {
        "æ ‡é¢˜": "æ–‡ç« æ ‡é¢˜",
        "æ‘˜è¦": "æ–‡ç« æ‘˜è¦",
        "è¯¦æƒ…é“¾æ¥": "https://example.com/article/123"
      },
      "detail_data": {
        "å®Œæ•´å†…å®¹": "æ–‡ç« å®Œæ•´å†…å®¹...",
        "ä½œè€…": "å¼ ä¸‰",
        "å‘å¸ƒæ—¶é—´": "2025-01-19"
      },
      "metadata": {
        "list_page": 1,
        "item_index": 0,
        "detail_url": "https://example.com/article/123",
        "scrape_status": "success",
        "error_message": null,
        "scraped_at": "2025-01-19T10:30:00"
      }
    }
  ]
}
```

## ğŸ”§ é…ç½®è¯´æ˜

### MergedScraperConfig å‚æ•°

| å‚æ•° | ç±»å‹ | è¯´æ˜ | é»˜è®¤å€¼ |
|------|------|------|--------|
| `list_config` | `ScraperConfig` | åˆ—è¡¨é¡µæŠ“å–é…ç½® | å¿…å¡« |
| `detail_fields` | `List[FieldConfig]` | è¯¦æƒ…é¡µå­—æ®µé…ç½® | å¿…å¡« |
| `detail_container_selector` | `str` | è¯¦æƒ…é¡µå®¹å™¨é€‰æ‹©å™¨ | å¿…å¡« |
| `detail_url_field` | `str` | åˆ—è¡¨ä¸­çš„URLå­—æ®µå | å¿…å¡« |
| `detail_url_attribute` | `str` | URLå±æ€§å | `"href"` |
| `navigation_mode` | `NavigationMode` | å¯¼èˆªæ¨¡å¼ | `GO_BACK` |
| `back_wait_time` | `float` | è¿”å›åˆ—è¡¨é¡µç­‰å¾…æ—¶é—´ | `2.0` |
| `detail_page_wait_time` | `float` | è¯¦æƒ…é¡µåŠ è½½ç­‰å¾…æ—¶é—´ | `2.0` |
| `max_detail_retries` | `int` | è¯¦æƒ…é¡µæœ€å¤§é‡è¯•æ¬¡æ•° | `2` |
| `continue_on_error` | `bool` | å¤±è´¥åæ˜¯å¦ç»§ç»­ | `True` |

## ğŸ¨ é«˜çº§ç”¨æ³•

### 1. åœ¨å·²æ‰“å¼€çš„é¡µé¢ä¸ŠæŠ“å–

```python
async with BrowserManager(mode="connect") as bm:
    # è¿æ¥åˆ°å·²æ‰“å¼€çš„é¡µé¢
    page = await bm.get_or_create_page(target_url="example.com")
    
    scraper = MergedScraper(page, merged_config)
    
    # ä»å½“å‰é¡µé¢å¼€å§‹æŠ“å–ï¼ˆä¸é‡æ–°å¯¼èˆªï¼‰
    data = await scraper.scrape_from_current_page()
```

### 2. å¤„ç†ç›¸å¯¹URL

```python
# åˆ—è¡¨é…ç½®ä¸­æå–è¯¦æƒ…é“¾æ¥æ—¶ï¼Œè‡ªåŠ¨å¤„ç†ç›¸å¯¹URL
list_config = create_scraper_config(
    url="https://example.com/list",
    fields={
        "è¯¦æƒ…é“¾æ¥": "h3 a"  # å³ä½¿æ˜¯ç›¸å¯¹è·¯å¾„ä¹Ÿä¼šè‡ªåŠ¨è¡¥å…¨
    },
    # ...
)
```

### 3. é”™è¯¯å¤„ç†å’Œé‡è¯•

```python
merged_config = create_merged_scraper_config(
    # ...
    max_detail_retries=3,  # è¯¦æƒ…é¡µå¤±è´¥åé‡è¯•3æ¬¡
    continue_on_error=True,  # æŸä¸ªè¯¦æƒ…é¡µå¤±è´¥åç»§ç»­æŠ“å–å…¶ä»–é¡¹
    skip_invalid_urls=True  # è·³è¿‡æ— æ•ˆçš„URL
)
```

### 4. æå–å¤šä¸ªå€¼

```python
from puppeteer.universal_scraper import FieldConfig

detail_fields = [
    FieldConfig(name="æ ‡ç­¾", selector=".tag", multiple=True),  # æå–æ‰€æœ‰æ ‡ç­¾
    FieldConfig(name="å›¾ç‰‡", selector="img", attribute="src", multiple=True)
]
```

## ğŸ“Š æ ¸å¿ƒåŸç†

### é˜²æ­¢æ•°æ®é”™é…çš„æœºåˆ¶

```
æ ¸å¿ƒæ€è·¯ï¼šé¡ºåºæ‰§è¡Œ + å”¯ä¸€æ ‡è¯† + åŸå­åˆå¹¶

For each åˆ—è¡¨é¡µ(page_num):
    For each åˆ—è¡¨é¡¹(index):
        1. æå–åˆ—è¡¨æ•°æ® â†’ list_data
        2. æå–è¯¦æƒ…URL â†’ detail_url
        3. è®¿é—®è¯¦æƒ…é¡µ â†’ detail_data
        4. åŸå­åˆå¹¶ â†’ merged_item = {
             "list_data": list_data,
             "detail_data": detail_data,
             "metadata": {
                 "list_page": page_num,
                 "item_index": index,
                 "detail_url": detail_url
             }
           }
        5. è¿”å›åˆ—è¡¨é¡µ
    ç¿»åˆ°ä¸‹ä¸€é¡µ
```

### å…³é”®è®¾è®¡

1. **å”¯ä¸€æ ‡è¯†è¿½è¸ª**
   - `list_page`: åˆ—è¡¨é¡µç 
   - `item_index`: é¡¹åœ¨å½“å‰é¡µçš„ç´¢å¼•
   - `detail_url`: è¯¦æƒ…é¡µURL

2. **ä¸¥æ ¼é¡ºåºæ‰§è¡Œ**
   - âŒ ä¸ä½¿ç”¨å¹¶å‘ï¼š`asyncio.gather()`ä¼šå¯¼è‡´é¡ºåºé”™ä¹±
   - âœ… ä½¿ç”¨é¡ºåºå¾ªç¯ï¼šç¡®ä¿æ•°æ®ä¸€ä¸€å¯¹åº”

3. **é”™è¯¯éš”ç¦»**
   - å•ä¸ªè¯¦æƒ…é¡µå¤±è´¥ä¸å½±å“å…¶ä»–é¡¹
   - è®°å½•è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯åˆ°metadata

## ğŸ§ª æµ‹è¯•

### è¿è¡Œæµ‹è¯•

```bash
# æµ‹è¯•åŸºç¡€åŠŸèƒ½
python lib/puppeteer/merged_scraper/example.py

# æµ‹è¯•å·²æ‰“å¼€çš„é¡µé¢
python examples/02.scrape_list_with_detail.py
```

### æµ‹è¯•å‡†å¤‡

```bash
# å¯åŠ¨Chromeï¼ˆç”¨äºè¿æ¥æ¨¡å¼ï¼‰
chrome.exe --remote-debugging-port=9222

# æ‰“å¼€ç›®æ ‡åˆ—è¡¨é¡µ
# ç„¶åè¿è¡Œæµ‹è¯•è„šæœ¬
```

## ğŸ“ å®Œæ•´ç¤ºä¾‹

æŸ¥çœ‹ `lib/puppeteer/merged_scraper/example.py` ä¸­çš„å®Œæ•´ç¤ºä¾‹ï¼š

- ç¤ºä¾‹1ï¼šåŸºç¡€ä½¿ç”¨
- ç¤ºä¾‹2ï¼šåœ¨å·²æ‰“å¼€çš„é¡µé¢ä¸ŠæŠ“å–
- ç¤ºä¾‹3ï¼šé”™è¯¯å¤„ç†
- ç¤ºä¾‹4ï¼šé«˜çº§é…ç½®

## âš ï¸ æ³¨æ„äº‹é¡¹

1. **é¡µé¢å¯¼èˆª**
   - ç›®å‰æ”¯æŒ `GO_BACK` æ¨¡å¼ï¼ˆä½¿ç”¨æµè§ˆå™¨è¿”å›ï¼‰
   - `NEW_TAB` æ¨¡å¼ï¼ˆæ–°æ ‡ç­¾é¡µï¼‰æš‚æœªå®ç°

2. **ç­‰å¾…æ—¶é—´**
   - æ ¹æ®ç½‘ç«™å“åº”é€Ÿåº¦è°ƒæ•´ `back_wait_time` å’Œ `detail_page_wait_time`
   - ç½‘é€Ÿæ…¢æ—¶å»ºè®®å¢åŠ ç­‰å¾…æ—¶é—´

3. **URLå¤„ç†**
   - ç›¸å¯¹URLä¼šè‡ªåŠ¨è¡¥å…¨ä¸ºå®Œæ•´URL
   - æ— æ•ˆURLä¼šè¢«è·³è¿‡ï¼ˆå¦‚æœ `skip_invalid_urls=True`ï¼‰

4. **å†…å­˜ä½¿ç”¨**
   - å¤§é‡æ•°æ®æ—¶å»ºè®®å¯ç”¨ `save_partial_results=True`
   - å®šæœŸä¿å­˜éƒ¨åˆ†ç»“æœé¿å…æ•°æ®ä¸¢å¤±

## ğŸ”— ç›¸å…³æ–‡æ¡£

- [UniversalScraper æ–‡æ¡£](../universal_scraper/README.md)
- [BrowserManager æ–‡æ¡£](../../browser/README.md)
- [é¡¹ç›®ä¸» README](../../../README.md)
