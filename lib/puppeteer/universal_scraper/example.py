"""
é€šç”¨æŠ“å–å™¨ä½¿ç”¨ç¤ºä¾‹
æ¼”ç¤ºå¦‚ä½•æŠ“å– SegmentFault å’Œå…¶ä»–ç½‘ç«™æ•°æ®
"""

import asyncio
import sys
import os
from pathlib import Path
from dotenv import load_dotenv

sys.path.insert(0, str(Path(__file__).parent.parent.parent.parent / "lib"))
from browser import BrowserManager
from puppeteer.universal_scraper import UniversalScraper, create_scraper_config

load_dotenv()


# ==========================================
# ç¤ºä¾‹ 1: SegmentFault åˆ—è¡¨æ•°æ®
# ==========================================

async def example_segmentfault():
    """
    ç¤ºä¾‹ï¼šæŠ“å– SegmentFault é¦–é¡µæ–‡ç« åˆ—è¡¨
    """
    print("\n" + "="*60)
    print("ğŸ“Œ ç¤ºä¾‹ 1: SegmentFault æ–‡ç« åˆ—è¡¨")
    print("="*60 + "\n")
    
    async with BrowserManager(mode="launch", headless=False) as bm:
        page = await bm.get_or_create_page()
        
        # é…ç½®æŠ“å–å‚æ•°
        config = create_scraper_config(
            url="https://segmentfault.com/",
            fields={
                "æ ‡é¢˜": "h3 a.text-body",
                "æŠ•ç¥¨æ•°é‡": ".num-card .font-size-16",
                "é˜…è¯»æ•°é‡": ".num-card.text-secondary .font-size-16"
            },
            container_selector=".list-group.list-group-flush > .list-group-item",
            next_button_selector="a.page-link[rel='next']",
            delay=5.0,  # é¡µé¢è‡³å°‘åœç•™5ç§’
            max_pages=2  # æŠ“å–2é¡µ
        )
        
        # æ‰§è¡ŒæŠ“å–
        scraper = UniversalScraper(page, config)
        data = await scraper.scrape()
        
        # ä¿å­˜æ•°æ®
        scraper.save_to_json("segmentfault_articles.json")
        
        # æ˜¾ç¤ºå‰3æ¡
        print("\nğŸ“Š æ•°æ®ç¤ºä¾‹ï¼ˆå‰3æ¡ï¼‰:")
        for i, item in enumerate(data[:3], 1):
            print(f"\n{i}. {item}")


# ==========================================
# ç¤ºä¾‹ 2: å•é¡µæŠ“å–ï¼ˆæ— åˆ†é¡µï¼‰
# ==========================================

async def example_single_page():
    """
    ç¤ºä¾‹ï¼šæŠ“å–å•é¡µæ•°æ®ï¼ˆæ— åˆ†é¡µï¼‰
    """
    print("\n" + "="*60)
    print("ğŸ“Œ ç¤ºä¾‹ 2: å•é¡µæŠ“å–")
    print("="*60 + "\n")
    
    async with BrowserManager(mode="launch", headless=False) as bm:
        page = await bm.get_or_create_page()
        
        config = create_scraper_config(
            url="https://news.ycombinator.com/",
            fields={
                "æ ‡é¢˜": ".titleline > a",
                "åˆ†æ•°": ".score",
                "ä½œè€…": ".hnuser"
            },
            container_selector=".athing",
            delay=3.0
        )
        
        scraper = UniversalScraper(page, config)
        data = await scraper.scrape()
        
        scraper.save_to_json("hackernews_top.json")
        
        print(f"\nâœ… æŠ“å–äº† {len(data)} æ¡æ•°æ®")


# ==========================================
# ç¤ºä¾‹ 3: é¡µç èŒƒå›´æŠ“å–
# ==========================================

async def example_page_range():
    """
    ç¤ºä¾‹ï¼šæŠ“å–æŒ‡å®šé¡µç èŒƒå›´
    """
    print("\n" + "="*60)
    print("ğŸ“Œ ç¤ºä¾‹ 3: é¡µç èŒƒå›´æŠ“å–")
    print("="*60 + "\n")
    
    async with BrowserManager(mode="launch", headless=False) as bm:
        page = await bm.get_or_create_page()
        
        config = create_scraper_config(
            url="https://segmentfault.com/",
            fields={
                "æ ‡é¢˜": "h3 a.text-body",
                "æŠ•ç¥¨æ•°": ".num-card .font-size-16"
            },
            container_selector=".list-group-item",
            next_button_selector="a.page-link[rel='next']",
            page_range=(1, 3),  # åªæŠ“å–ç¬¬1-3é¡µ
            delay=4.0
        )
        
        scraper = UniversalScraper(page, config)
        data = await scraper.scrape()
        
        scraper.save_to_json("segmentfault_pages_1_3.json")


# ==========================================
# ç¤ºä¾‹ 4: æå–å±æ€§å€¼
# ==========================================

async def example_extract_attributes():
    """
    ç¤ºä¾‹ï¼šæå–å…ƒç´ å±æ€§ï¼ˆå¦‚ href, srcï¼‰
    """
    print("\n" + "="*60)
    print("ğŸ“Œ ç¤ºä¾‹ 4: æå–å±æ€§å€¼")
    print("="*60 + "\n")
    
    from puppeteer.universal_scraper.scraper import FieldConfig, ScraperConfig
    
    async with BrowserManager(mode="launch", headless=False) as bm:
        page = await bm.get_or_create_page()
        
        # ä½¿ç”¨é«˜çº§å­—æ®µé…ç½®
        config = ScraperConfig(
            url="https://segmentfault.com/",
            fields=[
                FieldConfig(name="æ ‡é¢˜", selector="h3 a.text-body"),
                FieldConfig(name="é“¾æ¥", selector="h3 a.text-body", attribute="href"),
                FieldConfig(name="æŠ•ç¥¨æ•°", selector=".num-card .font-size-16")
            ],
            container_selector=".list-group-item",
            delay=3.0
        )
        
        scraper = UniversalScraper(page, config)
        data = await scraper.scrape()
        
        scraper.save_to_json("segmentfault_with_links.json")
        
        print("\nğŸ“Š ç¤ºä¾‹æ•°æ®ï¼ˆåŒ…å«é“¾æ¥ï¼‰:")
        if data:
            print(data[0])


# ==========================================
# ç¤ºä¾‹ 5: è‡ªå®šä¹‰å»¶è¿Ÿæ—¶é—´
# ==========================================

async def example_custom_delay():
    """
    ç¤ºä¾‹ï¼šè‡ªå®šä¹‰é¡µé¢ç­‰å¾…æ—¶é—´
    """
    print("\n" + "="*60)
    print("ğŸ“Œ ç¤ºä¾‹ 5: è‡ªå®šä¹‰å»¶è¿Ÿæ—¶é—´")
    print("="*60 + "\n")
    
    async with BrowserManager(mode="launch", headless=False) as bm:
        page = await bm.get_or_create_page()
        
        config = create_scraper_config(
            url="https://segmentfault.com/",
            fields={
                "æ ‡é¢˜": "h3 a.text-body"
            },
            container_selector=".list-group-item",
            next_button_selector="a.page-link[rel='next']",
            delay=8.0,  # æ¯é¡µåœç•™8ç§’
            max_pages=2
        )
        
        scraper = UniversalScraper(page, config)
        await scraper.scrape()
        
        scraper.save_to_json("slow_scrape.json")


# ==========================================
# ç¤ºä¾‹ 6: ç›´æ¥ä½¿ç”¨ï¼ˆä¸é€šè¿‡å·¥å…·ï¼‰
# ==========================================

async def example_direct_usage():
    """
    ç¤ºä¾‹ï¼šç›´æ¥ä½¿ç”¨æŠ“å–å™¨ï¼ˆç”¨äºè„šæœ¬ï¼‰
    å®Œæ•´æ¼”ç¤ºç”¨æˆ·éœ€æ±‚çš„åœºæ™¯
    """
    print("\n" + "="*60)
    print("ğŸ“Œ ç¤ºä¾‹ 6: å®Œæ•´ç”¨æˆ·åœºæ™¯")
    print("="*60 + "\n")
    
    print("ç”¨æˆ·è¾“å…¥:")
    print("---")
    user_input = '''
    æ‰“å¼€ https://segmentfault.com/ é¡µé¢ï¼Œ
    è·å– .list-group.list-group-flush å¯¹åº”çš„åˆ—è¡¨æ•°æ®ï¼Œ
    
    é‡‡é›†çš„ä¿¡æ¯å’Œå¯¹åº”çš„é€‰æ‹©å™¨å¦‚ä¸‹ï¼š
    æ ‡é¢˜ï¼šh3 a.text-body
    æŠ•ç¥¨æ•°é‡ï¼š.num-card .font-size-16
    é˜…è¯»æ•°é‡ï¼š.num-card.text-secondary .font-size-16
    
    ä¸‹ä¸€é¡µæŒ‰é’®é€‰æ‹©å™¨æ˜¯ a.page-link[rel='next']ï¼Œ
    é¡µé¢è‡³å°‘åœç•™5ç§’
    '''
    print(user_input)
    print("---\n")
    
    async with BrowserManager(mode="launch", headless=False) as bm:
        page = await bm.get_or_create_page()
        
        # æ ¹æ®ç”¨æˆ·è¾“å…¥åˆ›å»ºé…ç½®
        config = create_scraper_config(
            url="https://segmentfault.com/",
            fields={
                "æ ‡é¢˜": "h3 a.text-body",
                "æŠ•ç¥¨æ•°é‡": ".num-card .font-size-16",
                "é˜…è¯»æ•°é‡": ".num-card.text-secondary .font-size-16"
            },
            container_selector=".list-group.list-group-flush > .list-group-item",
            next_button_selector="a.page-link[rel='next']",
            delay=5.0,
            max_pages=2
        )
        
        # æ‰§è¡ŒæŠ“å–
        print("ğŸš€ å¼€å§‹æŠ“å–...")
        scraper = UniversalScraper(page, config)
        data = await scraper.scrape()
        
        # ä¿å­˜ä¸ºç”¨æˆ·æœŸæœ›çš„æ ¼å¼
        import json
        
        # ç®€åŒ–æ ¼å¼ï¼ˆåªä¿ç•™æ•°æ®æ•°ç»„ï¼‰
        simple_output = data
        
        with open("output.json", 'w', encoding='utf-8') as f:
            json.dump(simple_output, f, ensure_ascii=False, indent=2)
        
        print(f"\nâœ… è¾“å‡ºå·²ä¿å­˜åˆ° output.json")
        print(f"   æ€»æ¡ç›®: {len(data)}")
        
        print("\nğŸ“„ è¾“å‡ºç¤ºä¾‹:")
        print(json.dumps(data[:2], ensure_ascii=False, indent=2))


# ==========================================
# ä¸»èœå•
# ==========================================

async def main():
    """ä¸»èœå•"""
    examples = {
        "1": ("SegmentFault æ–‡ç« åˆ—è¡¨", example_segmentfault),
        "2": ("å•é¡µæŠ“å–", example_single_page),
        "3": ("é¡µç èŒƒå›´", example_page_range),
        "4": ("æå–å±æ€§", example_extract_attributes),
        "5": ("è‡ªå®šä¹‰å»¶è¿Ÿ", example_custom_delay),
        "6": ("å®Œæ•´ç”¨æˆ·åœºæ™¯", example_direct_usage)
    }
    
    print("\n" + "="*60)
    print("ğŸ“ é€šç”¨æŠ“å–å™¨ä½¿ç”¨ç¤ºä¾‹")
    print("="*60)
    print("\nå¯ç”¨ç¤ºä¾‹:")
    for key, (name, _) in examples.items():
        print(f"   {key}. {name}")
    
    choice = input("\né€‰æ‹©ç¤ºä¾‹ (1-6): ").strip()
    
    if choice in examples:
        name, func = examples[choice]
        print(f"\nğŸš€ è¿è¡Œç¤ºä¾‹: {name}")
        await func()
    else:
        print("âŒ æ— æ•ˆé€‰æ‹©")


if __name__ == "__main__":
    asyncio.run(main())
