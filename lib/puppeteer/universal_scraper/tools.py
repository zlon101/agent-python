"""
é€šç”¨æŠ“å–å™¨ - LangChain å·¥å…·é›†æˆ
è®© Agent èƒ½å¤Ÿä½¿ç”¨é€šç”¨æŠ“å–åŠŸèƒ½
"""

from langchain_core.tools import StructuredTool
from playwright.async_api import Browser
from typing import List, Dict, Optional
from .scraper import UniversalScraper, create_scraper_config


def get_universal_scraping_tools(browser: Browser) -> List[StructuredTool]:
    """
    åˆ›å»ºé€šç”¨æŠ“å–å·¥å…·é›†
    
    Args:
        browser: Playwright æµè§ˆå™¨å®ä¾‹
        
    Returns:
        å·¥å…·åˆ—è¡¨
    """
    
    async def get_current_page():
        """è·å–å½“å‰æ´»è·ƒé¡µé¢"""
        if not browser.contexts:
            raise RuntimeError("No browser context found")
        context = browser.contexts[0]
        if not context.pages:
            raise RuntimeError("No pages open")
        return context.pages[-1]
    
    # ==========================================
    # å·¥å…· 1: é€šç”¨æ•°æ®æŠ“å–ï¼ˆç®€åŒ–ç‰ˆï¼‰
    # ==========================================
    
    async def scrape_web_data(
        url: str,
        fields: str,
        container_selector: str,
        next_button_selector: str = "",
        delay: float = 3.0,
        max_pages: int = 1,
        filename: str = "scraped_data.json"
    ) -> str:
        """
        é€šç”¨ç½‘é¡µæ•°æ®æŠ“å–å·¥å…·ã€‚
        
        Args:
            url: ç›®æ ‡ç½‘å€
            fields: å­—æ®µé…ç½®ï¼ˆJSONå­—ç¬¦ä¸²ï¼‰ï¼Œæ ¼å¼: {"å­—æ®µå": "CSSé€‰æ‹©å™¨"}
            container_selector: æ•°æ®é¡¹å®¹å™¨çš„CSSé€‰æ‹©å™¨
            next_button_selector: ä¸‹ä¸€é¡µæŒ‰é’®CSSé€‰æ‹©å™¨ï¼ˆå¯é€‰ï¼‰
            delay: é¡µé¢ç­‰å¾…æ—¶é—´ï¼ˆç§’ï¼‰
            max_pages: æœ€å¤§æŠ“å–é¡µæ•°
            filename: ä¿å­˜çš„æ–‡ä»¶å
            
        Returns:
            æŠ“å–ç»“æœæ‘˜è¦
        
        Example:
            fields = '{"æ ‡é¢˜": "h3 a", "æŠ•ç¥¨æ•°": ".vote-count"}'
            container_selector = ".list-group-item"
        """
        try:
            import json
            
            # è§£æå­—æ®µé…ç½®
            try:
                fields_dict = json.loads(fields)
            except:
                return "âŒ å­—æ®µé…ç½®è§£æå¤±è´¥ï¼Œè¯·ç¡®ä¿æ˜¯æœ‰æ•ˆçš„JSONæ ¼å¼"
            
            page = await get_current_page()
            
            # åˆ›å»ºé…ç½®
            config = create_scraper_config(
                url=url,
                fields=fields_dict,
                container_selector=container_selector,
                next_button_selector=next_button_selector if next_button_selector else None,
                delay=delay,
                max_pages=max_pages
            )
            
            # æ‰§è¡ŒæŠ“å–
            scraper = UniversalScraper(page, config)
            data = await scraper.scrape()
            
            # ä¿å­˜æ•°æ®
            scraper.save_to_json(filename)
            
            return f"âœ… æˆåŠŸæŠ“å– {len(data)} æ¡æ•°æ®ï¼Œå·²ä¿å­˜åˆ° {filename}"
            
        except Exception as e:
            return f"âŒ æŠ“å–å¤±è´¥: {str(e)}"
    
    # ==========================================
    # å·¥å…· 2: é«˜çº§æŠ“å–ï¼ˆæ”¯æŒæ›´å¤šé…ç½®ï¼‰
    # ==========================================
    
    async def scrape_web_data_advanced(
        url: str,
        fields_json: str,
        container_selector: str,
        next_button_selector: str = "",
        page_range_start: int = 1,
        page_range_end: int = 1,
        delay: float = 3.0,
        filename: str = "scraped_data.json"
    ) -> str:
        """
        é«˜çº§é€šç”¨æŠ“å–å·¥å…·ï¼Œæ”¯æŒé¡µç èŒƒå›´ã€‚
        
        Args:
            url: ç›®æ ‡ç½‘å€
            fields_json: å­—æ®µé…ç½®JSONå­—ç¬¦ä¸²
            container_selector: å®¹å™¨é€‰æ‹©å™¨
            next_button_selector: ä¸‹ä¸€é¡µæŒ‰é’®é€‰æ‹©å™¨
            page_range_start: èµ·å§‹é¡µç 
            page_range_end: ç»“æŸé¡µç 
            delay: é¡µé¢å»¶è¿Ÿæ—¶é—´
            filename: ä¿å­˜æ–‡ä»¶å
            
        Returns:
            æŠ“å–ç»“æœ
        """
        try:
            import json
            
            # è§£æå­—æ®µ
            fields_dict = json.loads(fields_json)
            
            page = await get_current_page()
            
            # åˆ›å»ºé…ç½®
            page_range = (page_range_start, page_range_end) if page_range_end > page_range_start else None
            
            config = create_scraper_config(
                url=url,
                fields=fields_dict,
                container_selector=container_selector,
                next_button_selector=next_button_selector if next_button_selector else None,
                page_range=page_range,
                delay=delay,
                max_pages=page_range_end if page_range else 0
            )
            
            # æ‰§è¡ŒæŠ“å–
            scraper = UniversalScraper(page, config)
            data = await scraper.scrape()
            
            # ä¿å­˜
            scraper.save_to_json(filename)
            
            return f"âœ… æŠ“å–å®Œæˆï¼š{len(data)} æ¡æ•°æ® â†’ {filename}"
            
        except Exception as e:
            return f"âŒ é”™è¯¯: {str(e)}"
    
    # ==========================================
    # å·¥å…· 3: é¢„è§ˆæŠ“å–ï¼ˆä¸ä¿å­˜ï¼‰
    # ==========================================
    
    async def preview_scrape(
        url: str,
        fields: str,
        container_selector: str,
        limit: int = 3
    ) -> str:
        """
        é¢„è§ˆæŠ“å–ç»“æœï¼ˆåªæŠ“å–å‰å‡ æ¡ï¼Œä¸ä¿å­˜æ–‡ä»¶ï¼‰ã€‚
        ç”¨äºæµ‹è¯•é€‰æ‹©å™¨æ˜¯å¦æ­£ç¡®ã€‚
        
        Args:
            url: ç›®æ ‡ç½‘å€
            fields: å­—æ®µé…ç½®JSON
            container_selector: å®¹å™¨é€‰æ‹©å™¨
            limit: é¢„è§ˆæ¡æ•°
            
        Returns:
            é¢„è§ˆæ•°æ®
        """
        try:
            import json
            
            fields_dict = json.loads(fields)
            page = await get_current_page()
            
            # åˆ›å»ºé…ç½®ï¼ˆä¸åˆ†é¡µï¼‰
            config = create_scraper_config(
                url=url,
                fields=fields_dict,
                container_selector=container_selector,
                delay=2.0
            )
            
            # æŠ“å–
            scraper = UniversalScraper(page, config)
            await page.goto(url)
            await page.wait_for_selector(container_selector, timeout=10000)
            
            data = await scraper.scrape_current_page()
            
            # è¿”å›å‰å‡ æ¡
            preview_data = data[:limit]
            
            result = f"ğŸ“Š é¢„è§ˆæŠ“å–ç»“æœï¼ˆå‰ {len(preview_data)} æ¡ï¼‰:\n\n"
            result += json.dumps(preview_data, ensure_ascii=False, indent=2)
            
            return result
            
        except Exception as e:
            return f"âŒ é¢„è§ˆå¤±è´¥: {str(e)}"
    
    # ==========================================
    # åˆ›å»ºå·¥å…·åˆ—è¡¨
    # ==========================================
    
    tools = [
        StructuredTool.from_function(
            func=None,
            coroutine=scrape_web_data,
            name="scrape_web_data",
            description=(
                "é€šç”¨ç½‘é¡µæ•°æ®æŠ“å–å·¥å…·ã€‚æ”¯æŒè‡ªå®šä¹‰å­—æ®µã€åˆ†é¡µæŠ“å–ã€‚"
                "éœ€è¦æä¾›ï¼šURLã€å­—æ®µé…ç½®ï¼ˆJSONï¼‰ã€å®¹å™¨é€‰æ‹©å™¨ã€‚"
                "å¯é€‰ï¼šä¸‹ä¸€é¡µæŒ‰é’®ã€å»¶è¿Ÿæ—¶é—´ã€æœ€å¤§é¡µæ•°ã€‚"
            )
        ),
        StructuredTool.from_function(
            func=None,
            coroutine=scrape_web_data_advanced,
            name="scrape_web_data_advanced",
            description=(
                "é«˜çº§é€šç”¨æŠ“å–å·¥å…·ï¼Œæ”¯æŒé¡µç èŒƒå›´æ§åˆ¶ã€‚"
                "é€‚ç”¨äºéœ€è¦ç²¾ç¡®æ§åˆ¶æŠ“å–é¡µç èŒƒå›´çš„åœºæ™¯ã€‚"
            )
        ),
        StructuredTool.from_function(
            func=None,
            coroutine=preview_scrape,
            name="preview_scrape",
            description=(
                "é¢„è§ˆæŠ“å–ç»“æœï¼Œç”¨äºæµ‹è¯•é€‰æ‹©å™¨æ˜¯å¦æ­£ç¡®ã€‚"
                "åªæŠ“å–å‰å‡ æ¡æ•°æ®ï¼Œä¸ä¿å­˜æ–‡ä»¶ã€‚"
            )
        )
    ]
    
    return tools
