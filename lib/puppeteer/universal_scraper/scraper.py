"""
é€šç”¨ç½‘é¡µæ•°æ®æŠ“å–å™¨
æ”¯æŒè‡ªå®šä¹‰å­—æ®µã€åˆ†é¡µæŠ“å–ã€çµæ´»é…ç½®
"""

import json
import asyncio
from typing import Dict, List, Optional, Any
from dataclasses import dataclass, asdict
from playwright.async_api import Page


@dataclass
class FieldConfig:
    """å­—æ®µé…ç½®"""
    name: str  # å­—æ®µå
    selector: str  # CSSé€‰æ‹©å™¨
    attribute: Optional[str] = None  # æå–å±æ€§ï¼ˆé»˜è®¤æå–textï¼‰
    multiple: bool = False  # æ˜¯å¦æå–å¤šä¸ªå€¼


@dataclass
class ScraperConfig:
    """æŠ“å–å™¨é…ç½®"""
    url: str  # ç›®æ ‡ç½‘å€
    fields: List[FieldConfig]  # å­—æ®µé…ç½®åˆ—è¡¨
    container_selector: str  # å®¹å™¨é€‰æ‹©å™¨ï¼ˆæ¯ä¸ªæ•°æ®é¡¹çš„å®¹å™¨ï¼‰
    next_button_selector: Optional[str] = None  # ä¸‹ä¸€é¡µæŒ‰é’®é€‰æ‹©å™¨
    page_range: Optional[tuple] = None  # é¡µç èŒƒå›´ (start, end)
    delay: float = 3.0  # é¡µé¢å»¶è¿Ÿæ—¶é—´ï¼ˆç§’ï¼‰
    max_pages: int = 0  # æœ€å¤§é¡µæ•°ï¼ˆ0è¡¨ç¤ºæ— é™åˆ¶ï¼‰


class UniversalScraper:
    """é€šç”¨ç½‘é¡µæ•°æ®æŠ“å–å™¨"""
    
    def __init__(self, page: Page, config: ScraperConfig):
        """
        åˆå§‹åŒ–æŠ“å–å™¨
        
        Args:
            page: Playwrighté¡µé¢å¯¹è±¡
            config: æŠ“å–å™¨é…ç½®
        """
        self.page = page
        self.config = config
        self.all_data: List[Dict[str, Any]] = []
    
    async def scrape_current_page(self) -> List[Dict[str, Any]]:
        """
        æŠ“å–å½“å‰é¡µé¢æ•°æ®
        
        Returns:
            å½“å‰é¡µçš„æ•°æ®åˆ—è¡¨
        """
        page_data = []
        
        # ç­‰å¾…å®¹å™¨åŠ è½½
        try:
            await self.page.wait_for_selector(
                self.config.container_selector,
                timeout=10000
            )
        except Exception as e:
            print(f"âš ï¸ å®¹å™¨æœªæ‰¾åˆ°: {self.config.container_selector}")
            return page_data
        
        # è·å–æ‰€æœ‰å®¹å™¨
        containers = await self.page.locator(self.config.container_selector).all()
        print(f"   æ‰¾åˆ° {len(containers)} ä¸ªæ•°æ®é¡¹")
        
        # éå†æ¯ä¸ªå®¹å™¨
        for container in containers:
            item_data = {}
            
            # æå–æ¯ä¸ªå­—æ®µ
            for field in self.config.fields:
                try:
                    value = await self._extract_field(container, field)
                    item_data[field.name] = value
                except Exception as e:
                    print(f"   âš ï¸ æå–å­—æ®µå¤±è´¥ [{field.name}]: {e}")
                    item_data[field.name] = None
            
            page_data.append(item_data)
        
        return page_data
    
    async def _extract_field(self, container, field: FieldConfig) -> Any:
        """
        æå–å•ä¸ªå­—æ®µçš„å€¼
        
        Args:
            container: å®¹å™¨å…ƒç´ 
            field: å­—æ®µé…ç½®
            
        Returns:
            å­—æ®µå€¼
        """
        locator = container.locator(field.selector)
        
        # æ£€æŸ¥å…ƒç´ æ˜¯å¦å­˜åœ¨
        count = await locator.count()
        if count == 0:
            return None
        
        # æå–å¤šä¸ªå€¼
        if field.multiple:
            elements = await locator.all()
            values = []
            for elem in elements:
                if field.attribute:
                    val = await elem.get_attribute(field.attribute)
                else:
                    val = await elem.text_content()
                values.append(val.strip() if val else None)
            return values
        
        # æå–å•ä¸ªå€¼
        if field.attribute:
            value = await locator.first.get_attribute(field.attribute)
        else:
            value = await locator.first.text_content()
        
        return value.strip() if value else None
    
    async def scrape_with_pagination(self) -> List[Dict[str, Any]]:
        """
        æŠ“å–åˆ†é¡µæ•°æ®
        
        Returns:
            æ‰€æœ‰é¡µé¢çš„æ•°æ®
        """
        current_page = 1
        
        while True:
            # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°æœ€å¤§é¡µæ•°
            if self.config.max_pages > 0 and current_page > self.config.max_pages:
                print(f"âœ… è¾¾åˆ°æœ€å¤§é¡µæ•°: {self.config.max_pages}")
                break
            
            # æ£€æŸ¥é¡µç èŒƒå›´
            if self.config.page_range:
                start, end = self.config.page_range
                if current_page < start:
                    current_page += 1
                    continue
                if current_page > end:
                    print(f"âœ… è¾¾åˆ°é¡µç èŒƒå›´ä¸Šé™: {end}")
                    break
            
            # æŠ“å–å½“å‰é¡µ
            print(f"\nğŸ“„ æŠ“å–ç¬¬ {current_page} é¡µ...")
            page_data = await self.scrape_current_page()
            
            if page_data:
                self.all_data.extend(page_data)
                print(f"   âœ“ æˆåŠŸæå– {len(page_data)} æ¡æ•°æ®")
            else:
                print(f"   âš ï¸ å½“å‰é¡µæ— æ•°æ®")
            
            # æ£€æŸ¥æ˜¯å¦æœ‰ä¸‹ä¸€é¡µ
            if not self.config.next_button_selector:
                print("âœ… æ— åˆ†é¡µé…ç½®ï¼ŒæŠ“å–å®Œæˆ")
                break
            
            # æŸ¥æ‰¾ä¸‹ä¸€é¡µæŒ‰é’®
            next_button = self.page.locator(self.config.next_button_selector)
            
            try:
                # æ£€æŸ¥æŒ‰é’®æ˜¯å¦å­˜åœ¨
                count = await next_button.count()
                if count == 0:
                    print("âœ… æœªæ‰¾åˆ°ä¸‹ä¸€é¡µæŒ‰é’®")
                    break
                
                # æ£€æŸ¥æŒ‰é’®æ˜¯å¦å¯ç‚¹å‡»
                is_visible = await next_button.first.is_visible()
                is_enabled = await next_button.first.is_enabled()
                
                if not is_visible or not is_enabled:
                    print("âœ… ä¸‹ä¸€é¡µæŒ‰é’®ä¸å¯ç”¨")
                    break
                
                # ç‚¹å‡»ä¸‹ä¸€é¡µ
                print(f"   ğŸ”„ ç‚¹å‡»ä¸‹ä¸€é¡µï¼Œç­‰å¾… {self.config.delay} ç§’...")
                await next_button.first.click()
                
                # ç­‰å¾…é¡µé¢åŠ è½½
                await asyncio.sleep(self.config.delay)
                await self.page.wait_for_load_state("networkidle", timeout=15000)
                
                current_page += 1
                
            except Exception as e:
                print(f"âœ… åˆ†é¡µç»“æŸ: {str(e)}")
                break
        
        return self.all_data
    
    async def scrape(self) -> List[Dict[str, Any]]:
        """
        æ‰§è¡ŒæŠ“å–ï¼ˆè‡ªåŠ¨åˆ¤æ–­æ˜¯å¦åˆ†é¡µï¼‰
        
        Returns:
            æŠ“å–çš„æ‰€æœ‰æ•°æ®
        """
        # å¯¼èˆªåˆ°ç›®æ ‡é¡µé¢
        print(f"ğŸŒ è®¿é—®: {self.config.url}")
        await self.page.goto(self.config.url)
        await asyncio.sleep(self.config.delay)
        
        # åˆ¤æ–­æ˜¯å¦éœ€è¦åˆ†é¡µ
        if self.config.next_button_selector or self.config.page_range:
            return await self.scrape_with_pagination()
        else:
            data = await self.scrape_current_page()
            self.all_data = data
            return data
    
    def save_to_json(self, filename: str = "scraped_data.json") -> str:
        """
        ä¿å­˜æ•°æ®åˆ°JSONæ–‡ä»¶
        
        Args:
            filename: æ–‡ä»¶å
            
        Returns:
            ä¿å­˜ç»“æœä¿¡æ¯
        """
        if not self.all_data:
            return "âŒ æ— æ•°æ®å¯ä¿å­˜"
        
        output = {
            "metadata": {
                "total_items": len(self.all_data),
                "url": self.config.url,
                "fields": [field.name for field in self.config.fields]
            },
            "data": self.all_data
        }
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(output, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ’¾ æ•°æ®å·²ä¿å­˜åˆ°: {filename}")
        print(f"   æ€»æ¡ç›®: {len(self.all_data)}")
        
        return filename
    
    def get_data(self) -> List[Dict[str, Any]]:
        """è·å–æŠ“å–çš„æ•°æ®"""
        return self.all_data


def create_scraper_config(
    url: str,
    fields: Dict[str, str],
    container_selector: str,
    next_button_selector: Optional[str] = None,
    page_range: Optional[tuple] = None,
    delay: float = 3.0,
    max_pages: int = 0
) -> ScraperConfig:
    """
    åˆ›å»ºæŠ“å–å™¨é…ç½®ï¼ˆä¾¿æ·å‡½æ•°ï¼‰
    
    Args:
        url: ç›®æ ‡ç½‘å€
        fields: å­—æ®µé…ç½®å­—å…¸ {"å­—æ®µå": "CSSé€‰æ‹©å™¨"}
        container_selector: å®¹å™¨é€‰æ‹©å™¨
        next_button_selector: ä¸‹ä¸€é¡µæŒ‰é’®é€‰æ‹©å™¨
        page_range: é¡µç èŒƒå›´
        delay: å»¶è¿Ÿæ—¶é—´
        max_pages: æœ€å¤§é¡µæ•°
        
    Returns:
        ScraperConfigå¯¹è±¡
    """
    field_configs = [
        FieldConfig(name=name, selector=selector)
        for name, selector in fields.items()
    ]
    
    return ScraperConfig(
        url=url,
        fields=field_configs,
        container_selector=container_selector,
        next_button_selector=next_button_selector,
        page_range=page_range,
        delay=delay,
        max_pages=max_pages
    )
