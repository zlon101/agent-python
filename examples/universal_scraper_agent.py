"""
Agent é›†æˆç¤ºä¾‹ - é€šç”¨æ•°æ®æŠ“å–
è®© LLM Agent ä½¿ç”¨é€šç”¨æŠ“å–å·¥å…·
"""

import asyncio
import sys
import os
from pathlib import Path
from dotenv import load_dotenv
from langchain_core.messages import HumanMessage

sys.path.insert(0, str(Path(__file__).parent / "lib"))
from browser import BrowserManager
from custom_agent import create_custom_agent
from puppeteer import get_browser_tools, get_universal_scraping_tools

load_dotenv()


async def agent_universal_scrape(task: str):
    """
    ä½¿ç”¨ Agent æ‰§è¡Œé€šç”¨æŠ“å–ä»»åŠ¡
    
    Args:
        task: ç”¨æˆ·ä»»åŠ¡æè¿°
    """
    print("\n" + "="*60)
    print("ğŸ¤– Agent é€šç”¨æ•°æ®æŠ“å–")
    print("="*60 + "\n")
    
    async with BrowserManager(mode="launch", headless=False) as bm:
        browser = bm.get_browser()
        
        # è·å–æ‰€æœ‰å·¥å…·
        browser_tools = get_browser_tools(browser)
        scraping_tools = get_universal_scraping_tools(browser)
        all_tools = browser_tools + scraping_tools
        
        print(f"ğŸ”§ åŠ è½½äº† {len(all_tools)} ä¸ªå·¥å…·")
        print(f"   é€šç”¨æŠ“å–å·¥å…·: {len(scraping_tools)} ä¸ª")
        for tool in scraping_tools:
            print(f"   - {tool.name}")
        print()
        
        # åˆ›å»º Agentï¼ˆè‡ªå®šä¹‰ promptï¼‰
        system_prompt = """
You are an intelligent web scraping agent with universal data extraction capabilities.

CAPABILITIES:
1. Navigate to any web page
2. Extract data using custom CSS selectors
3. Handle pagination (button-based or URL-based)
4. Save data in JSON format
5. Preview scraping results before full extraction

AVAILABLE TOOLS:
- scrape_web_data: é€šç”¨æŠ“å–å·¥å…·ï¼ˆæ”¯æŒåˆ†é¡µï¼‰
- scrape_web_data_advanced: é«˜çº§æŠ“å–ï¼ˆæ”¯æŒé¡µç èŒƒå›´ï¼‰
- preview_scrape: é¢„è§ˆæŠ“å–ç»“æœï¼ˆç”¨äºæµ‹è¯•é€‰æ‹©å™¨ï¼‰

WORKFLOW:
1. ç†è§£ç”¨æˆ·éœ€æ±‚ï¼š
   - ç›®æ ‡URL
   - éœ€è¦æå–çš„å­—æ®µå’Œå¯¹åº”çš„CSSé€‰æ‹©å™¨
   - æ˜¯å¦éœ€è¦åˆ†é¡µ
   - å»¶è¿Ÿæ—¶é—´å’Œé¡µæ•°é™åˆ¶

2. æ„å»ºå­—æ®µé…ç½®ï¼š
   - å°†å­—æ®µé…ç½®è½¬æ¢ä¸ºJSONæ ¼å¼ï¼š{"å­—æ®µå": "CSSé€‰æ‹©å™¨"}
   - ç¡®ä¿é€‰æ‹©å™¨å‡†ç¡®

3. é€‰æ‹©åˆé€‚çš„å·¥å…·ï¼š
   - ç®€å•åœºæ™¯ï¼šä½¿ç”¨ scrape_web_data
   - éœ€è¦ç²¾ç¡®æ§åˆ¶é¡µç ï¼šä½¿ç”¨ scrape_web_data_advanced
   - æµ‹è¯•é€‰æ‹©å™¨ï¼šå…ˆä½¿ç”¨ preview_scrape

4. æ‰§è¡ŒæŠ“å–å¹¶æŠ¥å‘Šç»“æœ

IMPORTANT TIPS:
- å­—æ®µé…ç½®å¿…é¡»æ˜¯æœ‰æ•ˆçš„JSONæ ¼å¼
- å®¹å™¨é€‰æ‹©å™¨åº”è¯¥å‡†ç¡®å®šä½åˆ°æ¯ä¸ªæ•°æ®é¡¹
- åˆ†é¡µæŒ‰é’®é€‰æ‹©å™¨è¦ç²¾ç¡®ï¼Œé¿å…ç‚¹å‡»é”™è¯¯çš„å…ƒç´ 
- åˆç†è®¾ç½®å»¶è¿Ÿæ—¶é—´ï¼Œé¿å…è¯·æ±‚è¿‡å¿«
- å¦‚æœä¸ç¡®å®šé€‰æ‹©å™¨ï¼Œå¯ä»¥å…ˆç”¨ preview_scrape æµ‹è¯•

EXAMPLE:
User: "æŠ“å– SegmentFault é¦–é¡µæ–‡ç« ï¼ŒåŒ…æ‹¬æ ‡é¢˜å’ŒæŠ•ç¥¨æ•°"
Agentæ€è·¯:
1. URL: https://segmentfault.com/
2. å­—æ®µé…ç½®: {"æ ‡é¢˜": "h3 a.text-body", "æŠ•ç¥¨æ•°": ".num-card .font-size-16"}
3. å®¹å™¨é€‰æ‹©å™¨: .list-group-item
4. è°ƒç”¨ scrape_web_data å·¥å…·
"""
        
        agent = create_custom_agent(
            tools=all_tools,
            system_prompt=system_prompt,
            model=os.getenv("AGENT_MODEL", "qwen-plus")
        )
        
        # æ‰§è¡Œä»»åŠ¡
        print(f"ğŸ¯ ä»»åŠ¡: {task}\n")
        print("ğŸ¤– Agent æ­£åœ¨å·¥ä½œ...\n")
        
        result = await agent.ainvoke(
            {"messages": [HumanMessage(content=task)]}
        )
        
        # æ˜¾ç¤ºç»“æœ
        print("\n" + "="*60)
        print("âœ… Agent å®Œæˆ")
        print("="*60)
        print(result["messages"][-1].content)
        print("="*60 + "\n")


# ==========================================
# é¢„å®šä¹‰ä»»åŠ¡
# ==========================================

TASKS = {
    "1": {
        "name": "SegmentFault æ–‡ç« åˆ—è¡¨",
        "task": """
æŠ“å– SegmentFault é¦–é¡µæ–‡ç« åˆ—è¡¨æ•°æ®ï¼š
- URL: https://segmentfault.com/
- å®¹å™¨é€‰æ‹©å™¨: .list-group-item
- éœ€è¦æå–çš„å­—æ®µï¼š
  * æ ‡é¢˜: h3 a.text-body
  * æŠ•ç¥¨æ•°é‡: .num-card .font-size-16
  * é˜…è¯»æ•°é‡: .num-card.text-secondary .font-size-16
- ä¸‹ä¸€é¡µæŒ‰é’®: a.page-link[rel='next']
- æŠ“å–2é¡µï¼Œæ¯é¡µåœç•™5ç§’
- ä¿å­˜ä¸º segmentfault_result.json
"""
    },
    
    "2": {
        "name": "GitHub Trending",
        "task": """
æŠ“å– GitHub Trending é¡µé¢ï¼š
- URL: https://github.com/trending
- å®¹å™¨é€‰æ‹©å™¨: article.Box-row
- æå–å­—æ®µï¼š
  * é¡¹ç›®å: h2 a
  * æè¿°: p.col-9
  * è¯­è¨€: span[itemprop='programmingLanguage']
- å•é¡µæŠ“å–ï¼Œåœç•™3ç§’
- ä¿å­˜ä¸º github_trending_universal.json
"""
    },
    
    "3": {
        "name": "Hacker News",
        "task": """
æŠ“å– Hacker News é¦–é¡µï¼š
- URL: https://news.ycombinator.com/
- å®¹å™¨é€‰æ‹©å™¨: .athing
- å­—æ®µï¼š
  * æ ‡é¢˜: .titleline > a
  * åˆ†æ•°: .score
- å•é¡µæŠ“å–
- ä¿å­˜ä¸º hackernews.json
"""
    },
    
    "4": {
        "name": "é¢„è§ˆæµ‹è¯•",
        "task": """
é¢„è§ˆ SegmentFault é¦–é¡µçš„æŠ“å–ç»“æœï¼š
- URL: https://segmentfault.com/
- å®¹å™¨é€‰æ‹©å™¨: .list-group-item
- å­—æ®µ: {"æ ‡é¢˜": "h3 a.text-body"}
- ä½¿ç”¨ preview_scrape å·¥å…·ï¼Œåªçœ‹å‰3æ¡
"""
    },
    
    "5": {
        "name": "è‡ªå®šä¹‰ä»»åŠ¡",
        "task": None
    }
}


async def main():
    """ä¸»å‡½æ•°"""
    print("\n" + "="*60)
    print("ğŸ“ Agent é€šç”¨æŠ“å–ä»»åŠ¡")
    print("="*60)
    print("\nå¯ç”¨ä»»åŠ¡:")
    for key, info in TASKS.items():
        print(f"   {key}. {info['name']}")
    
    choice = input("\né€‰æ‹©ä»»åŠ¡ (1-5): ").strip()
    
    if choice not in TASKS:
        print("âŒ æ— æ•ˆé€‰æ‹©")
        return
    
    task_info = TASKS[choice]
    
    if choice == "5":
        print("\nè¯·è¾“å…¥è‡ªå®šä¹‰ä»»åŠ¡æè¿°:")
        print("ç¤ºä¾‹æ ¼å¼ï¼š")
        print("---")
        print("æŠ“å– [URL] é¡µé¢")
        print("å®¹å™¨é€‰æ‹©å™¨: [CSSé€‰æ‹©å™¨]")
        print("å­—æ®µ: {\"å­—æ®µå\": \"CSSé€‰æ‹©å™¨\"}")
        print("åˆ†é¡µ: [å¯é€‰]")
        print("---")
        task = input("\n> ").strip()
        if not task:
            print("âŒ ä»»åŠ¡ä¸èƒ½ä¸ºç©º")
            return
    else:
        task = task_info["task"]
        print(f"\nğŸ“‹ é€‰æ‹©ä»»åŠ¡: {task_info['name']}")
    
    # æ‰§è¡Œä»»åŠ¡
    await agent_universal_scrape(task)


if __name__ == "__main__":
    asyncio.run(main())
