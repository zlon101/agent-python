"""
Agent è‡ªåŠ¨æŠ“å–åˆ†é¡µè¡¨æ ¼ç¤ºä¾‹
è®© LLM Agent è‡ªä¸»è¯†åˆ«è¡¨æ ¼å¹¶æ”¶é›†æ•°æ®
"""

import asyncio
import sys
from dotenv import load_dotenv
from pathlib import Path
from langchain_core.messages import HumanMessage


from lib.browser import BrowserManager
from lib.custom_agent import create_custom_agent
from lib.puppeteer import get_browser_tools, get_table_scraping_tools

load_dotenv()


async def agent_scrape_table(task: str):
    """
    ä½¿ç”¨ Agent è‡ªåŠ¨æŠ“å–è¡¨æ ¼
    
    Args:
        task: ç”¨æˆ·ä»»åŠ¡æè¿°
    """
    print("\n" + "="*60)
    print("ðŸ¤– Agent è‡ªåŠ¨æŠ“å–è¡¨æ ¼")
    print("="*60 + "\n")
    
    async with BrowserManager(mode="connect") as bm:
        browser = bm.get_browser()
        
        # èŽ·å–æ‰€æœ‰å·¥å…·ï¼ˆæµè§ˆå™¨ + è¡¨æ ¼æŠ“å–ï¼‰
        browser_tools = get_browser_tools(browser)
        table_tools = get_table_scraping_tools(browser)
        all_tools = browser_tools + table_tools
        
        print(f"ðŸ”§ åŠ è½½äº† {len(all_tools)} ä¸ªå·¥å…·")
        print(f"   å…¶ä¸­ {len(table_tools)} ä¸ªè¡¨æ ¼å·¥å…·:")
        for tool in table_tools:
            print(f"   - {tool.name}")
        print()
        
        # åˆ›å»º Agentï¼ˆä½¿ç”¨è‡ªå®šä¹‰ promptï¼‰
        system_prompt = """
You are an intelligent web scraping agent specialized in extracting table data.

CAPABILITIES:
1. Navigate to web pages
2. Identify table structures
3. Extract data from single or paginated tables
4. Save data to CSV or JSON files

WORKFLOW:
1. Navigate to the target URL
2. Use 'analyze_table' to understand the table structure
3. Choose the appropriate scraping method:
   - Single page: use 'extract_table'
   - Button pagination: use 'scrape_paginated_table'
   - URL pagination: use 'scrape_table_url_pagination'
4. Save the results with a descriptive filename

IMPORTANT TIPS:
- Always analyze the table structure first
- Look for pagination elements (buttons, page numbers, URLs)
- Use appropriate CSS selectors for tables
- Handle errors gracefully
"""
        
        agent = create_custom_agent(
            tools=all_tools,
            system_prompt=system_prompt
        )
        
        # æ‰§è¡Œä»»åŠ¡
        print(f"ðŸŽ¯ ä»»åŠ¡: {task}\n")
        print("ðŸ¤– Agent æ­£åœ¨æ€è€ƒ...\n")
        
        result = await agent.ainvoke(
            {"messages": [HumanMessage(content=task)]}
        )
        
        # æ˜¾ç¤ºç»“æžœ
        print("\n" + "="*60)
        print("âœ… Agent å®Œæˆ")
        print("="*60)
        print(result["messages"][-1].content)
        print("="*60 + "\n")


# ==========================================
# é¢„å®šä¹‰ä»»åŠ¡ç¤ºä¾‹
# ==========================================

TASKS = {
    "1": {
        "name": "æŠ“å– Wikipedia è¡¨æ ¼",
        "task": """
Go to https://en.wikipedia.org/wiki/List_of_countries_by_population_(United_Nations).
Find the main population table and extract all data.
Save it as 'world_population.csv'.
"""
    },
    
    "2": {
        "name": "æŠ“å– GitHub Trending",
        "task": """
Go to https://github.com/trending.
Extract information about trending repositories including:
- Repository name
- Description
- Stars today
Save as 'github_trending.json'.
"""
    },
    
    "3": {
        "name": "æŠ“å–åˆ†é¡µäº§å“åˆ—è¡¨",
        "task": """
Go to the e-commerce website and find the product listing table.
The table has pagination with a "Next" button.
Scrape the first 3 pages of products.
Save as 'products.csv'.
(Note: You need to provide a real URL)
"""
    },
    
    "4": {
        "name": "åˆ†æžè¡¨æ ¼ç»“æž„",
        "task": """
Go to https://example.com/data-table (replace with your URL).
Analyze the table structure and tell me:
- How many columns
- What are the column names
- How many rows
- Is there pagination?
"""
    },
    
    "5": {
        "name": "è‡ªå®šä¹‰ä»»åŠ¡",
        "task": None  # ç”¨æˆ·è¾“å…¥
    }
}


async def main():
    """ä¸»å‡½æ•°"""
    print("\n" + "="*60)
    print("ðŸŽ“ Agent è‡ªåŠ¨æŠ“å–è¡¨æ ¼ - ä»»åŠ¡é€‰æ‹©")
    print("="*60)
    print("\nå¯ç”¨ä»»åŠ¡:")
    for key, info in TASKS.items():
        print(f"   {key}. {info['name']}")
    
    choice = input("\né€‰æ‹©ä»»åŠ¡ (1-5): ").strip()
    
    if choice not in TASKS:
        print("âŒ æ— æ•ˆé€‰æ‹©")
        return
    
    task_info = TASKS[choice]
    
    if choice == "5":
        print("\nè¯·è¾“å…¥è‡ªå®šä¹‰ä»»åŠ¡:")
        task = input("> ").strip()
        if not task:
            print("âŒ ä»»åŠ¡ä¸èƒ½ä¸ºç©º")
            return
    else:
        task = task_info["task"]
        print(f"\nðŸ“‹ é€‰æ‹©ä»»åŠ¡: {task_info['name']}")
    
    # æ‰§è¡Œä»»åŠ¡
    await agent_scrape_table(task)


# ==========================================
# å¿«é€Ÿæµ‹è¯•å‡½æ•°
# ==========================================

async def quick_test():
    """å¿«é€Ÿæµ‹è¯• - æŠ“å– Example.com æ¼”ç¤ºè¡¨æ ¼"""
    task = """
Navigate to a website with a data table.
Analyze the table structure.
Then extract the table data and save as 'test_table.csv'.
"""
    await agent_scrape_table(task)


if __name__ == "__main__":
    # é€‰æ‹©è¿è¡Œæ¨¡å¼
    import sys
    
    if len(sys.argv) > 1 and sys.argv[1] == "--quick":
        asyncio.run(quick_test())
    else:
        asyncio.run(main())