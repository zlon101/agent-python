"""
åœ¨å·²æ‰“å¼€çš„é¡µé¢ä¸Šæ‰§è¡Œåˆ—è¡¨+è¯¦æƒ…é¡µåˆå¹¶æŠ“å–
æµ‹è¯•æ–‡ä»¶
"""

import asyncio
import sys
from pathlib import Path

sys.path.insert(0, str(Path(__file__).parent.parent / "lib"))

from browser import BrowserManager
from puppeteer.universal_scraper import create_scraper_config
from puppeteer.merged_scraper import MergedScraper, create_merged_scraper_config


async def test_merged_scraper():
    """
    æµ‹è¯•åœºæ™¯ï¼šåœ¨å·²æ‰“å¼€çš„é¡µé¢ä¸Šè¿›è¡Œåˆ—è¡¨+è¯¦æƒ…é¡µåˆå¹¶æŠ“å–
    
    å‡†å¤‡å·¥ä½œï¼š
    1. å¯åŠ¨ Chrome: chrome.exe --remote-debugging-port=9222
    2. æ‰“å¼€è¦æŠ“å–çš„åˆ—è¡¨é¡µï¼ˆå¦‚ SegmentFault æœç´¢ç»“æœï¼‰
    """
    print("\n" + "="*60)
    print("ğŸ¯ æµ‹è¯•ï¼šåˆ—è¡¨é¡µ+è¯¦æƒ…é¡µåˆå¹¶æŠ“å–")
    print("="*60 + "\n")
    
    async with BrowserManager(mode="connect") as bm:
        # åˆ—å‡ºæ‰€æœ‰æ‰“å¼€çš„é¡µé¢
        pages_info = await bm.list_all_pages()
        
        print(f"ğŸ“‹ å½“å‰æ‰“å¼€çš„æ ‡ç­¾é¡µ ({len(pages_info)} ä¸ª):\n")
        for i, info in enumerate(pages_info, 1):
            print(f"{i}. {info['title'][:60]}")
            print(f"   {info['url']}\n")
        
        if not pages_info:
            print("âŒ æ²¡æœ‰æ‰¾åˆ°æ‰“å¼€çš„æ ‡ç­¾é¡µ")
            print("\nğŸ’¡ è¯·å…ˆå¯åŠ¨Chromeå¹¶æ‰“å¼€ç›®æ ‡é¡µé¢:")
            print("   1. chrome.exe --remote-debugging-port=9222")
            print("   2. æ‰“å¼€åˆ—è¡¨é¡µï¼ˆå¦‚ SegmentFaultï¼‰")
            return
        
        # è¿æ¥åˆ°ç›®æ ‡é¡µé¢
        print("ğŸ” æŸ¥æ‰¾ç›®æ ‡é¡µé¢...\n")
        page = await bm.get_or_create_page(target_url="segmentfault.com")
        
        if not page:
            print("âŒ æœªæ‰¾åˆ° SegmentFault é¡µé¢")
            print("\nğŸ’¡ è¯·åœ¨Chromeä¸­æ‰“å¼€ https://segmentfault.com/")
            return
        
        print(f"âœ… è¿æ¥åˆ°é¡µé¢: {page.url}")
        print(f"   æ ‡é¢˜: {await page.title()}\n")
        
        # ========== é…ç½®åˆ—è¡¨é¡µæŠ“å– ==========
        list_config = create_scraper_config(
            url=page.url,  # ä½¿ç”¨å½“å‰é¡µé¢çš„URL
            fields={
                "æ ‡é¢˜": "h3 a.text-body",
                "æ‘˜è¦": ".excerpt",
                "æŠ•ç¥¨æ•°": ".num-card .font-size-16",
                "è¯¦æƒ…é“¾æ¥": "h3 a.text-body"  # ç”¨äºæå–è¯¦æƒ…é¡µURL
            },
            container_selector=".list-group-item",
            next_button_selector="a.page-link[rel='next']",  # ä¸‹ä¸€é¡µæŒ‰é’®
            delay=3.0,
            max_pages=2  # æµ‹è¯•æŠ“å–2é¡µ
        )
        
        # ========== é…ç½®è¯¦æƒ…é¡µæŠ“å– ==========
        merged_config = create_merged_scraper_config(
            list_config=list_config,
            detail_fields={
                "æ–‡ç« å†…å®¹": ".article-content",
                "ä½œè€…": ".user-info .name a",
                "å‘å¸ƒæ—¶é—´": ".article-meta time",
                "æµè§ˆé‡": ".article-meta .views",
                "æ ‡ç­¾": ".taglist--inline .tag"
            },
            detail_container_selector=".article-content",  # ç­‰å¾…è¯¦æƒ…é¡µå®¹å™¨åŠ è½½
            detail_url_field="è¯¦æƒ…é“¾æ¥",  # å¯¹åº”åˆ—è¡¨é…ç½®ä¸­çš„å­—æ®µ
            detail_url_attribute="href",  # æå–hrefå±æ€§ä½œä¸ºURL
            navigation_mode="go_back",  # ä½¿ç”¨è¿”å›æŒ‰é’®
            back_wait_time=2.0,  # è¿”å›åˆ—è¡¨é¡µç­‰å¾…2ç§’
            detail_page_wait_time=2.0,  # è¯¦æƒ…é¡µåŠ è½½ç­‰å¾…2ç§’
            max_detail_retries=2,  # å¤±è´¥é‡è¯•2æ¬¡
            continue_on_error=True  # å•ä¸ªè¯¦æƒ…é¡µå¤±è´¥åç»§ç»­
        )
        
        # ========== æ‰§è¡ŒæŠ“å– ==========
        scraper = MergedScraper(page, merged_config)
        
        # ä»å½“å‰é¡µé¢å¼€å§‹æŠ“å–ï¼ˆä¸é‡æ–°å¯¼èˆªï¼‰
        data = await scraper.scrape_from_current_page()
        
        # ========== ä¿å­˜æ•°æ® ==========
        if data:
            scraper.save_to_json("test_merged_data.json")
            
            # æ˜¾ç¤ºéƒ¨åˆ†æ•°æ®é¢„è§ˆ
            print(f"\n{'='*60}")
            print(f"ğŸ“Š æ•°æ®é¢„è§ˆï¼ˆå‰2æ¡ï¼‰")
            print(f"{'='*60}")
            
            for i, item in enumerate(data[:2], 1):
                print(f"\nç¬¬ {i} æ¡:")
                print(f"  åˆ—è¡¨æ•°æ®:")
                for key, value in item['list_data'].items():
                    print(f"    {key}: {str(value)[:50]}")
                print(f"  è¯¦æƒ…æ•°æ®:")
                for key, value in item['detail_data'].items():
                    print(f"    {key}: {str(value)[:50]}")
                print(f"  çŠ¶æ€: {item['metadata']['scrape_status']}")
        else:
            print("\nâš ï¸ æœªæŠ“å–åˆ°æ•°æ®")


async def main():
    print("\nâš ï¸ å‡†å¤‡å·¥ä½œ:")
    print("   1. å¯åŠ¨ Chrome: chrome.exe --remote-debugging-port=9222")
    print("   2. æ‰“å¼€è¦æŠ“å–çš„ç½‘é¡µï¼ˆå¦‚ SegmentFaultï¼‰")
    print("   3. ç¡®ä¿é¡µé¢å·²å®Œå…¨åŠ è½½")
    
    input("\nå‡†å¤‡å¥½åæŒ‰å›è½¦å¼€å§‹æµ‹è¯•...")
    
    try:
        await test_merged_scraper()
    except ConnectionError as e:
        print(f"\nâŒ è¿æ¥å¤±è´¥: {e}")
        print("\nğŸ’¡ è§£å†³æ–¹æ³•:")
        print("   1. å¯åŠ¨ Chrome: chrome.exe --remote-debugging-port=9222")
        print("   2. é‡æ–°è¿è¡Œ")
    except Exception as e:
        print(f"\nâŒ é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    asyncio.run(main())
