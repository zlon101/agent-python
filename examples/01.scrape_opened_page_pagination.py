"""
åœ¨å·²æ‰“å¼€çš„é¡µé¢ä¸Šè¿›è¡Œåˆ†é¡µæŠ“å–
æ¼”ç¤ºå¦‚ä½•è¿æ¥åˆ°å·²æ‰“å¼€çš„é¡µé¢å¹¶è¿›è¡Œåˆ†é¡µæ•°æ®é‡‡é›†
"""

import asyncio
import sys
from pathlib import Path

sys.path.insert(0, str(Path(__file__).parent.parent / "lib"))
from browser import BrowserManager
from puppeteer import UniversalScraper, create_scraper_config


async def scrape_opened_page_with_pagination():
    """
    åœºæ™¯ï¼šåœ¨å·²æ‰“å¼€çš„é¡µé¢ä¸Šè¿›è¡Œåˆ†é¡µæŠ“å–
    
    æ­¥éª¤ï¼š
    1. è¿æ¥åˆ°å·²æ‰“å¼€çš„ Chrome
    2. æŸ¥æ‰¾ç›®æ ‡é¡µé¢
    3. åœ¨å½“å‰é¡µé¢ä¸Šè¿›è¡Œåˆ†é¡µæŠ“å–
    """
    print("\n" + "="*60)
    print("ğŸ¯ åœ¨å·²æ‰“å¼€çš„é¡µé¢ä¸Šè¿›è¡Œåˆ†é¡µæŠ“å–")
    print("="*60 + "\n")
    
    async with BrowserManager(mode="connect") as bm:
        # åˆ—å‡ºæ‰€æœ‰æ‰“å¼€çš„é¡µé¢
        pages_info = await bm.list_all_pages()
        if not pages_info:
            print("âŒ æ²¡æœ‰æ‰¾åˆ°æ‰“å¼€çš„æ ‡ç­¾é¡µ")
            return
        
        # è¿æ¥åˆ°å·²æ‰“å¼€çš„é¡µé¢
        page = await bm.get_or_create_page(target_url="devops.aliyun.com")
        
        if not page:
            print("âŒ æœªæ‰¾åˆ°æŒ‡å®šé¡µé¢")
            return
        
        print(f"âœ… è¿æ¥åˆ°é¡µé¢: {page.url}")
        
        parseBug = False
        # äº‘æ•ˆä»»åŠ¡é…ç½®
        fileName = "test_äº‘æ•ˆä»»åŠ¡ç»Ÿè®¡.json"
        config = create_scraper_config(
            url=page.url,
            container_selector=".next-table-body tr.next-table-row",
            next_button_selector=".next-btn.next-pagination-item.next-next",  # ä¸‹ä¸€é¡µæŒ‰é’®
            delay=3.0,  # æ¯é¡µç­‰å¾…3ç§’
            max_pages=2,  # æŠ“å–2é¡µ
            fields={
                "æ ‡é¢˜": ".yunxiao-projex-workitem-title",
                "äººå¤©": ".TextAndNumberModifier--statusName--yXxCXqU",
                "é¡¹ç›®": ".newTable--spaceItemsWrapper--gRll8b3 .newTable--itemButton--nbzOwGl",
                "è¿­ä»£": "td .workitemList--sprintTriger--ta4dk92",
                "ç‰ˆæœ¬": "td[data-next-table-col='10']",
            },
        )
        
        # äº‘æ•ˆç¼ºé™·é…ç½®
        if parseBug:
            fileName = "test_äº‘æ•ˆbugç»Ÿè®¡.json"
            config = create_scraper_config(
                url=page.url,
                container_selector=".next-table-body tr.next-table-row",
                next_button_selector=".next-btn.next-pagination-item.next-next",  # ä¸‹ä¸€é¡µæŒ‰é’®
                delay=4.0,  # æ¯é¡µç­‰å¾…3ç§’
                max_pages=2,  # æŠ“å–2é¡µ
                fields={
                    "æ ‡é¢˜": ".yunxiao-projex-workitem-title",
                    "é¡¹ç›®": ".newTable--spaceItemsWrapper--gRll8b3 .newTable--itemButton--nbzOwGl",
                    "bugäº§ç”ŸåŸå› ": "td[data-next-table-col='9'] em",
                },
            )
        
        # åˆ›å»ºæŠ“å–å™¨
        scraper = UniversalScraper(page, config)
        
        # â­ å…³é”®ï¼šä½¿ç”¨ scrape_from_current_page() è€Œä¸æ˜¯ scrape()
        # è¿™æ ·ä¸ä¼šé‡æ–°å¯¼èˆªï¼Œç›´æ¥åœ¨å½“å‰é¡µé¢ä¸ŠæŠ“å–
        print("ğŸš€ å¼€å§‹åˆ†é¡µæŠ“å–...\n")
        data = await scraper.scrape_from_current_page()
        
        # ä¿å­˜æ•°æ®
        if data:
            scraper.save_to_json(fileName)
            print(f"\nâœ… æˆåŠŸ!")
            print(f"   æ€»æ¡æ•°: {len(data)}")
        else:
            print("\nâš ï¸ æœªæŠ“å–åˆ°æ•°æ®")


async def main():
    print("\nâš ï¸ å‡†å¤‡å·¥ä½œ:")
    print("   1. å¯åŠ¨ Chrome: chrome.exe --remote-debugging-port=9222")
    print("   2. æ‰“å¼€è¦æŠ“å–çš„ç½‘é¡µï¼ˆå¦‚ SegmentFault æœç´¢ç»“æœé¡µï¼‰")
    
    try:
        await scrape_opened_page_with_pagination()
    except ConnectionError as e:
        print(f"\nâŒ è¿æ¥å¤±è´¥: {e}")
        print("\nğŸ’¡ è§£å†³æ–¹æ³•:")
        print("   1. å¯åŠ¨ Chrome: chrome.exe --remote-debugging-port=9222")
        print("   2. é‡æ–°è¿è¡Œ")
    except Exception as e:
        print(f"\nâŒ é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    asyncio.run(main())
