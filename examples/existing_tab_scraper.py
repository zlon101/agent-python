"""
å®æˆ˜ç¤ºä¾‹ï¼šä½¿ç”¨æ–°åŠŸèƒ½åœ¨å·²æ‰“å¼€çš„æ ‡ç­¾é¡µä¸Šç›´æ¥æŠ“å–æ•°æ®
åœºæ™¯ï¼šç”¨æˆ·å·²ç»åœ¨æµè§ˆå™¨ä¸­æ‰“å¼€äº† SegmentFaultï¼Œæƒ³ç›´æ¥æŠ“å–æ•°æ®
"""

import asyncio
import sys
from pathlib import Path

sys.path.insert(0, str(Path(__file__).parent / "lib"))
from browser import BrowserManager
from puppeteer import UniversalScraper, create_scraper_config


async def scrape_from_existing_tab():
    """
    ä»å·²æ‰“å¼€çš„æ ‡ç­¾é¡µæŠ“å–æ•°æ®
    
    ä¼˜åŠ¿ï¼š
    1. æ— éœ€é‡æ–°åŠ è½½é¡µé¢ï¼ŒèŠ‚çœæ—¶é—´
    2. ä¿ç•™ç”¨æˆ·çš„ç™»å½•çŠ¶æ€å’Œæ“ä½œå†å²
    3. å¯ä»¥ç»§ç»­ç”¨æˆ·å½“å‰çš„æµè§ˆä½ç½®
    """
    print("\n" + "="*60)
    print("ğŸ¯ å®æˆ˜ï¼šä»å·²æ‰“å¼€çš„æ ‡ç­¾é¡µæŠ“å–æ•°æ®")
    print("="*60 + "\n")
    
    try:
        async with BrowserManager(mode="connect") as bm:
            # æ­¥éª¤ 1: æ˜¾ç¤ºæ‰€æœ‰æ‰“å¼€çš„æ ‡ç­¾é¡µ
            print("ğŸ“‹ ç¬¬ä¸€æ­¥ï¼šæŸ¥çœ‹æ‰€æœ‰æ‰“å¼€çš„æ ‡ç­¾é¡µ\n")
            pages_info = await bm.list_all_pages()
            
            if not pages_info:
                print("âŒ æ²¡æœ‰æ‰¾åˆ°æ‰“å¼€çš„æ ‡ç­¾é¡µ")
                print("\nğŸ’¡ è§£å†³æ–¹æ³•:")
                print("   1. å¯åŠ¨ Chrome: chrome.exe --remote-debugging-port=9222")
                print("   2. æ‰“å¼€ä¸€äº›ç½‘é¡µï¼ˆå¦‚ SegmentFaultï¼‰")
                print("   3. é‡æ–°è¿è¡Œæ­¤è„šæœ¬")
                return
            
            print(f"æ‰¾åˆ° {len(pages_info)} ä¸ªæ ‡ç­¾é¡µ:\n")
            for i, info in enumerate(pages_info, 1):
                print(f"{i}. {info['title'][:60]}")
                print(f"   {info['url']}\n")
            
            # æ­¥éª¤ 2: æŸ¥æ‰¾ SegmentFault é¡µé¢
            print("ğŸ” ç¬¬äºŒæ­¥ï¼šæŸ¥æ‰¾ SegmentFault é¡µé¢\n")
            page = await bm.get_or_create_page(target_url="segmentfault.com")
            
            print(f"âœ… è¿æ¥åˆ°é¡µé¢: {page.url}")
            print(f"   æ ‡é¢˜: {await page.title()}\n")
            
            # æ­¥éª¤ 3: é…ç½®æŠ“å–å™¨
            print("âš™ï¸  ç¬¬ä¸‰æ­¥ï¼šé…ç½®æŠ“å–å™¨\n")
            config = create_scraper_config(
                url=page.url,  # ä½¿ç”¨å½“å‰é¡µé¢çš„ URL
                fields={
                    "æ ‡é¢˜": "h3 a.text-body",
                    "æŠ•ç¥¨æ•°é‡": ".num-card .font-size-16",
                    "é˜…è¯»æ•°é‡": ".num-card.text-secondary .font-size-16"
                },
                container_selector=".list-group-item",
                delay=2.0
            )
            
            print("é…ç½®å®Œæˆ:")
            print(f"   URL: {config.url}")
            print(f"   å®¹å™¨: {config.container_selector}")
            print(f"   å­—æ®µæ•°: {len(config.fields)}\n")
            
            # æ­¥éª¤ 4: æ‰§è¡ŒæŠ“å–
            print("ğŸš€ ç¬¬å››æ­¥ï¼šæŠ“å–å½“å‰é¡µé¢æ•°æ®\n")
            scraper = UniversalScraper(page, config)
            
            # ç›´æ¥æŠ“å–å½“å‰é¡µé¢ï¼ˆä¸éœ€è¦å¯¼èˆªï¼ŒèŠ‚çœæ—¶é—´ï¼‰
            data = await scraper.scrape_current_page()
            
            # æ­¥éª¤ 5: æ˜¾ç¤ºç»“æœ
            print("\n" + "="*60)
            print("âœ… æŠ“å–å®Œæˆ")
            print("="*60)
            print(f"   æ€»æ¡ç›®: {len(data)}")
            
            if data:
                print(f"\nğŸ“Š æ•°æ®ç¤ºä¾‹ï¼ˆå‰3æ¡ï¼‰:\n")
                for i, item in enumerate(data[:3], 1):
                    print(f"{i}. æ ‡é¢˜: {item.get('æ ‡é¢˜', 'N/A')}")
                    print(f"   æŠ•ç¥¨: {item.get('æŠ•ç¥¨æ•°é‡', 'N/A')}")
                    print(f"   é˜…è¯»: {item.get('é˜…è¯»æ•°é‡', 'N/A')}\n")
                
                # ä¿å­˜æ•°æ®
                scraper.save_to_json("existing_tab_data.json")
            else:
                print("\nâš ï¸ æœªæŠ“å–åˆ°æ•°æ®ï¼Œå¯èƒ½éœ€è¦è°ƒæ•´é€‰æ‹©å™¨")
            
    except ConnectionError as e:
        print(f"\nâŒ è¿æ¥å¤±è´¥: {e}")
        print("\nğŸ’¡ è§£å†³æ–¹æ³•:")
        print("   1. å¯åŠ¨ Chrome å¹¶å¼€å¯è¿œç¨‹è°ƒè¯•")
        print("      chrome.exe --remote-debugging-port=9222")
        print("   2. é‡æ–°è¿è¡Œæ­¤è„šæœ¬")
    except Exception as e:
        print(f"\nâŒ é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()


async def scrape_multiple_tabs():
    """
    æ‰¹é‡æŠ“å–å¤šä¸ªå·²æ‰“å¼€çš„æ ‡ç­¾é¡µ
    """
    print("\n" + "="*60)
    print("ğŸ¯ å®æˆ˜ï¼šæ‰¹é‡æŠ“å–å¤šä¸ªæ ‡ç­¾é¡µ")
    print("="*60 + "\n")
    
    async with BrowserManager(mode="connect") as bm:
        # å®šä¹‰è¦æŠ“å–çš„ç½‘ç«™é…ç½®
        site_configs = {
            "segmentfault.com": {
                "fields": {
                    "æ ‡é¢˜": "h3 a.text-body",
                    "æŠ•ç¥¨": ".num-card .font-size-16"
                },
                "container": ".list-group-item"
            },
            "github.com/trending": {
                "fields": {
                    "é¡¹ç›®å": "h2 a",
                    "æè¿°": "p.col-9"
                },
                "container": "article.Box-row"
            }
        }
        
        results = {}
        
        for url_pattern, config_data in site_configs.items():
            print(f"ğŸ” æŸ¥æ‰¾ {url_pattern} é¡µé¢...")
            
            page = await bm.find_page_by_url(url_pattern)
            
            if page:
                print(f"âœ… æ‰¾åˆ°: {page.url}")
                print(f"   æŠ“å–æ•°æ®...\n")
                
                # é…ç½®æŠ“å–å™¨
                config = create_scraper_config(
                    url=page.url,
                    fields=config_data["fields"],
                    container_selector=config_data["container"],
                    delay=2.0
                )
                
                scraper = UniversalScraper(page, config)
                data = await scraper.scrape_current_page()
                
                results[url_pattern] = data
                print(f"   âœ“ æŠ“å–äº† {len(data)} æ¡æ•°æ®\n")
            else:
                print(f"âŒ æœªæ‰¾åˆ°é¡µé¢\n")
        
        # æ˜¾ç¤ºæ±‡æ€»
        print("="*60)
        print("ğŸ“Š æŠ“å–æ±‡æ€»")
        print("="*60)
        for url, data in results.items():
            print(f"{url}: {len(data)} æ¡æ•°æ®")


async def smart_scraper():
    """
    æ™ºèƒ½æŠ“å–å™¨ï¼šè‡ªåŠ¨åˆ¤æ–­æ˜¯ä½¿ç”¨å·²æ‰“å¼€çš„é¡µé¢è¿˜æ˜¯æ–°æ‰“å¼€
    """
    print("\n" + "="*60)
    print("ğŸ¯ å®æˆ˜ï¼šæ™ºèƒ½æŠ“å–å™¨")
    print("="*60 + "\n")
    
    target_url = "https://segmentfault.com/"
    
    async with BrowserManager(mode="connect") as bm:
        print(f"ğŸ¯ ç›®æ ‡: {target_url}\n")
        
        # å°è¯•æŸ¥æ‰¾å·²æ‰“å¼€çš„é¡µé¢
        page = await bm.get_or_create_page(target_url="segmentfault.com")
        
        # æ£€æŸ¥ URL æ˜¯å¦åŒ¹é…
        current_url = page.url.lower()
        
        if "segmentfault.com" in current_url:
            print("âœ… ä½¿ç”¨å·²æ‰“å¼€çš„é¡µé¢")
            print(f"   å½“å‰ URL: {page.url}")
            print("   âš¡ èŠ‚çœäº†é¡µé¢åŠ è½½æ—¶é—´\n")
        else:
            print("âš ï¸ é¡µé¢ä¸åŒ¹é…ï¼Œéœ€è¦å¯¼èˆª")
            print(f"   å½“å‰ URL: {page.url}")
            print(f"   ç›®æ ‡ URL: {target_url}")
            print("   æ­£åœ¨å¯¼èˆª...\n")
            await page.goto(target_url)
            await asyncio.sleep(3)
            print("âœ… å¯¼èˆªå®Œæˆ\n")
        
        # ç»§ç»­æŠ“å–
        config = create_scraper_config(
            url=target_url,
            fields={
                "æ ‡é¢˜": "h3 a.text-body"
            },
            container_selector=".list-group-item",
            delay=2.0
        )
        
        scraper = UniversalScraper(page, config)
        data = await scraper.scrape_current_page()
        
        print(f"âœ… æŠ“å–å®Œæˆ: {len(data)} æ¡æ•°æ®")


async def interactive_tab_scraper():
    """
    äº¤äº’å¼ï¼šè®©ç”¨æˆ·é€‰æ‹©è¦æŠ“å–çš„æ ‡ç­¾é¡µ
    """
    print("\n" + "="*60)
    print("ğŸ¯ å®æˆ˜ï¼šäº¤äº’å¼æ ‡ç­¾é¡µæŠ“å–")
    print("="*60 + "\n")
    
    async with BrowserManager(mode="connect") as bm:
        # åˆ—å‡ºæ‰€æœ‰æ ‡ç­¾é¡µ
        pages_info = await bm.list_all_pages()
        
        if not pages_info:
            print("âŒ æ²¡æœ‰æ‰¾åˆ°æ‰“å¼€çš„æ ‡ç­¾é¡µ")
            return
        
        print("ğŸ“‹ å¯ç”¨çš„æ ‡ç­¾é¡µ:\n")
        for i, info in enumerate(pages_info, 1):
            print(f"{i}. {info['title'][:60]}")
            print(f"   {info['url']}\n")
        
        # ç”¨æˆ·é€‰æ‹©
        try:
            choice = int(input("é€‰æ‹©è¦æŠ“å–çš„æ ‡ç­¾é¡µ (è¾“å…¥ç¼–å·): ")) - 1
            
            if 0 <= choice < len(pages_info):
                target_url = pages_info[choice]['url']
                
                # è¿æ¥åˆ°é€‰ä¸­çš„é¡µé¢
                page = await bm.find_page_by_url(target_url, exact_match=True)
                
                if page:
                    print(f"\nâœ… å·²è¿æ¥åˆ°: {await page.title()}")
                    print(f"   URL: {page.url}\n")
                    
                    # ç”¨æˆ·é…ç½®æŠ“å–å‚æ•°
                    print("è¯·é…ç½®æŠ“å–å‚æ•°:")
                    container = input("å®¹å™¨é€‰æ‹©å™¨ (å¦‚ .list-group-item): ").strip()
                    field_selector = input("æ ‡é¢˜é€‰æ‹©å™¨ (å¦‚ h3 a): ").strip()
                    
                    if container and field_selector:
                        config = create_scraper_config(
                            url=page.url,
                            fields={"æ ‡é¢˜": field_selector},
                            container_selector=container,
                            delay=2.0
                        )
                        
                        scraper = UniversalScraper(page, config)
                        print("\nğŸš€ å¼€å§‹æŠ“å–...\n")
                        data = await scraper.scrape_current_page()
                        
                        print(f"âœ… æŠ“å–å®Œæˆ: {len(data)} æ¡æ•°æ®")
                        
                        if data:
                            print("\nå‰3æ¡æ•°æ®:")
                            for i, item in enumerate(data[:3], 1):
                                print(f"{i}. {item}")
                    else:
                        print("âŒ é…ç½®ä¸å®Œæ•´")
                else:
                    print("âŒ æ— æ³•è¿æ¥åˆ°é¡µé¢")
            else:
                print("âŒ æ— æ•ˆé€‰æ‹©")
        except ValueError:
            print("âŒ è¯·è¾“å…¥æœ‰æ•ˆçš„æ•°å­—")
        except Exception as e:
            print(f"âŒ é”™è¯¯: {e}")


async def main():
    """ä¸»èœå•"""
    examples = {
        "1": ("ä»å·²æ‰“å¼€æ ‡ç­¾é¡µæŠ“å–", scrape_from_existing_tab),
        "2": ("æ‰¹é‡æŠ“å–å¤šä¸ªæ ‡ç­¾é¡µ", scrape_multiple_tabs),
        "3": ("æ™ºèƒ½æŠ“å–å™¨", smart_scraper),
        "4": ("äº¤äº’å¼æ ‡ç­¾é¡µæŠ“å–", interactive_tab_scraper)
    }
    
    print("\n" + "="*60)
    print("ğŸ“ æ–°åŠŸèƒ½å®æˆ˜ç¤ºä¾‹")
    print("="*60)
    print("\nå¯ç”¨ç¤ºä¾‹:")
    for key, (name, _) in examples.items():
        print(f"   {key}. {name}")
    
    print("\nâš ï¸ å‡†å¤‡å·¥ä½œ:")
    print("   1. å¯åŠ¨ Chrome: chrome.exe --remote-debugging-port=9222")
    print("   2. æ‰“å¼€ä¸€äº›ç½‘é¡µï¼ˆSegmentFaultã€GitHub ç­‰ï¼‰")
    print("   3. é€‰æ‹©ä¸€ä¸ªç¤ºä¾‹è¿è¡Œ\n")
    
    choice = input("é€‰æ‹©ç¤ºä¾‹ (1-4): ").strip()
    
    if choice in examples:
        name, func = examples[choice]
        print(f"\nğŸš€ è¿è¡Œç¤ºä¾‹: {name}")
        await func()
    else:
        print("âŒ æ— æ•ˆé€‰æ‹©")


if __name__ == "__main__":
    asyncio.run(main())
