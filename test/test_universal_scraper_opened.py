"""
æµ‹è¯•æµè§ˆå™¨ç®¡ç†å™¨çš„æ–°åŠŸèƒ½ - æŸ¥æ‰¾å’Œè¿æ¥åˆ°æŒ‡å®š URL çš„æ ‡ç­¾é¡µ
"""

import asyncio
import sys
from pathlib import Path

sys.path.insert(0, str(Path(__file__).parent.parent / "lib"))
from browser import BrowserManager


async def test_find_page_by_url():
    """æµ‹è¯•æŸ¥æ‰¾æŒ‡å®š URL çš„é¡µé¢"""
    print("\n" + "="*60)
    print("ğŸ§ª æµ‹è¯•æŸ¥æ‰¾æŒ‡å®š URL çš„æ ‡ç­¾é¡µ")
    print("="*60 + "\n")
    
    async with BrowserManager(mode="connect") as bm:
        # åˆ—å‡ºæ‰€æœ‰æ‰“å¼€çš„é¡µé¢
        pages_info = await bm.list_all_pages()
        
        print(f"ğŸ“‹ å½“å‰æ‰“å¼€çš„æ ‡ç­¾é¡µ ({len(pages_info)} ä¸ª):\n")
        for i, info in enumerate(pages_info, 1):
            print(f"{i}. {info['title']}")
            print(f"   URL: {info['url']}")
            print(f"   Context: {info['context_index']}, Page: {info['page_index']}\n")
        
        if not pages_info:
            print("âš ï¸ æ²¡æœ‰æ‰¾åˆ°æ‰“å¼€çš„æ ‡ç­¾é¡µ")
            print("ğŸ’¡ è¯·å…ˆåœ¨ Chrome ä¸­æ‰“å¼€ä¸€äº›ç½‘é¡µ")
            return
        
        # æµ‹è¯•æŸ¥æ‰¾é¡µé¢
        print("\n" + "="*60)
        print("ğŸ” æµ‹è¯•æŸ¥æ‰¾åŠŸèƒ½")
        print("="*60 + "\n")
        
        # æµ‹è¯• 1: éƒ¨åˆ†åŒ¹é…
        test_urls = [
            "segmentfault.com",
            "github.com",
            "google.com"
        ]
        
        for url in test_urls:
            print(f"æŸ¥æ‰¾åŒ…å« '{url}' çš„é¡µé¢...")
            page = await bm.find_page_by_url(url)
            if page:
                print(f"âœ… æ‰¾åˆ°: {page.url}")
                print(f"   æ ‡é¢˜: {await page.title()}\n")
            else:
                print(f"âŒ æœªæ‰¾åˆ°\n")
        
        # æµ‹è¯• 2: ç²¾ç¡®åŒ¹é…
        if pages_info:
            exact_url = pages_info[0]['url']
            print(f"ç²¾ç¡®æŸ¥æ‰¾: {exact_url}...")
            page = await bm.find_page_by_url(exact_url, exact_match=True)
            if page:
                print(f"âœ… ç²¾ç¡®åŒ¹é…æˆåŠŸ\n")
            else:
                print(f"âŒ ç²¾ç¡®åŒ¹é…å¤±è´¥\n")


async def test_get_page_with_url():
    """æµ‹è¯• get_or_create_page çš„æ–°å‚æ•°"""
    print("\n" + "="*60)
    print("ğŸ§ª æµ‹è¯• get_or_create_page(target_url)")
    print("="*60 + "\n")
    
    async with BrowserManager(mode="connect") as bm:
        # åˆ—å‡ºå½“å‰é¡µé¢
        pages_info = await bm.list_all_pages()
        print(f"ğŸ“‹ å½“å‰æœ‰ {len(pages_info)} ä¸ªæ ‡ç­¾é¡µ\n")
        
        # åœºæ™¯ 1: æŸ¥æ‰¾å·²å­˜åœ¨çš„é¡µé¢
        print("åœºæ™¯ 1: æŸ¥æ‰¾å·²å­˜åœ¨çš„ SegmentFault é¡µé¢")
        print("-" * 60)
        
        page = await bm.get_or_create_page(target_url="segmentfault.com")
        print(f"è¿”å›çš„é¡µé¢: {page.url}")
        print(f"æ ‡é¢˜: {await page.title()}\n")
        
        # åœºæ™¯ 2: æŸ¥æ‰¾ä¸å­˜åœ¨çš„é¡µé¢ï¼ˆå›é€€åˆ°é»˜è®¤è¡Œä¸ºï¼‰
        print("åœºæ™¯ 2: æŸ¥æ‰¾ä¸å­˜åœ¨çš„é¡µé¢")
        print("-" * 60)
        
        page = await bm.get_or_create_page(target_url="éå¸¸ç½•è§çš„ç½‘å€xyz123")
        print(f"è¿”å›çš„é¡µé¢: {page.url}")
        print(f"æ ‡é¢˜: {await page.title()}\n")
        
        # åœºæ™¯ 3: ä¸æŒ‡å®š URLï¼ˆåŸå§‹è¡Œä¸ºï¼‰
        print("åœºæ™¯ 3: ä¸æŒ‡å®š URLï¼ˆä½¿ç”¨é»˜è®¤è¡Œä¸ºï¼‰")
        print("-" * 60)
        
        page = await bm.get_or_create_page()
        print(f"è¿”å›çš„é¡µé¢: {page.url}")
        print(f"æ ‡é¢˜: {await page.title()}\n")



# è¿æ¥åˆ°å·²ç»æ‰“å¼€çš„é¡µé¢ï¼Œè§£æé¡µé¢æ•°æ®
async def test_with_scraper():
    """æµ‹è¯•åœ¨æŠ“å–å™¨ä¸­ä½¿ç”¨æ–°åŠŸèƒ½"""
    print("\n" + "="*60)
    print("ğŸ§ª åœ¨æŠ“å–å™¨ä¸­ä½¿ç”¨ - ç›´æ¥åœ¨å·²æ‰“å¼€çš„æ ‡ç­¾é¡µæ“ä½œ")
    print("="*60 + "\n")
    
    from puppeteer import UniversalScraper, create_scraper_config
    
    async with BrowserManager(mode="connect") as bm:
        # è¿æ¥åˆ°å·²ç»æ‰“å¼€çš„ SegmentFault é¡µé¢
        page = await bm.get_or_create_page(target_url="segmentfault.com")
        
        print(f"âœ… è¿æ¥åˆ°é¡µé¢: {page.url}\n")
        
        # ç›´æ¥åœ¨è¿™ä¸ªé¡µé¢ä¸Šæ‰§è¡ŒæŠ“å–
        config = create_scraper_config(
            url=page.url,  # ä½¿ç”¨å½“å‰é¡µé¢çš„ URL
            fields={
                # "æ ‡é¢˜": "h3 a.text-body",
                # "æŠ•ç¥¨æ•°": ".num-card .font-size-16"
                "æ ‡é¢˜": "h5",
                "æ—¶é—´": ".mb-0.font-size-14"
            },
            container_selector=".row div.list-group li",
            delay=2.0
        )
        
        scraper = UniversalScraper(page, config)
        
        # ä¸éœ€è¦å¯¼èˆªï¼Œç›´æ¥æŠ“å–å½“å‰é¡µé¢
        print("ğŸ” æŠ“å–å½“å‰é¡µé¢æ•°æ®...")
        data = await scraper.scrape_from_current_page()
        scraper.save_to_json("test_è§£æå·²ç»æ‰“å¼€çš„é¡µé¢.json")
        
        
        print(f"\nâœ… æˆåŠŸæŠ“å– {len(data)} æ¡æ•°æ®")
        print(f"\nğŸ“Š å‰3æ¡æ•°æ®:")
        for i, item in enumerate(data[:3], 1):
            print(f"\n{i}. {item}")


"""äº¤äº’å¼é¡µé¢æŸ¥æ‰¾å™¨"""
async def interactive_page_finder():
    print("\n" + "="*60)
    print("ğŸ” äº¤äº’å¼é¡µé¢æŸ¥æ‰¾å™¨")
    print("="*60 + "\n")
    
    async with BrowserManager(mode="connect") as bm:
        while True:
            # åˆ—å‡ºæ‰€æœ‰é¡µé¢
            pages_info = await bm.list_all_pages()
            
            print(f"\nğŸ“‹ å½“å‰æ‰“å¼€çš„æ ‡ç­¾é¡µ ({len(pages_info)} ä¸ª):\n")
            for i, info in enumerate(pages_info, 1):
                status = "âŒ å·²å…³é—­" if info['is_closed'] else "âœ… æ´»è·ƒ"
                print(f"{i}. [{status}] {info['title'][:50]}")
                print(f"   {info['url']}\n")
            
            # ç”¨æˆ·è¾“å…¥
            print("\næ“ä½œé€‰é¡¹:")
            print("  1. è¾“å…¥ URL å…³é”®è¯æŸ¥æ‰¾é¡µé¢")
            print("  2. è¾“å…¥ 'refresh' åˆ·æ–°åˆ—è¡¨")
            print("  3. è¾“å…¥ 'quit' é€€å‡º")
            
            choice = input("\n> ").strip()
            
            if choice.lower() == 'quit':
                break
            elif choice.lower() == 'refresh':
                continue
            else:
                # æŸ¥æ‰¾é¡µé¢
                page = await bm.find_page_by_url(choice)
                if page:
                    print(f"\nâœ… æ‰¾åˆ°é¡µé¢!")
                    print(f"   URL: {page.url}")
                    print(f"   æ ‡é¢˜: {await page.title()}")
                    
                    # è¯¢é—®æ˜¯å¦åœ¨è¿™ä¸ªé¡µé¢ä¸Šæ“ä½œ
                    action = input("\næ˜¯å¦åˆ‡æ¢åˆ°æ­¤é¡µé¢? (y/n): ").strip()
                    if action.lower() == 'y':
                        await page.bring_to_front()
                        print("âœ… å·²åˆ‡æ¢åˆ°è¯¥é¡µé¢")
                else:
                    print(f"\nâŒ æœªæ‰¾åˆ°åŒ…å« '{choice}' çš„é¡µé¢")


async def main():
    """ä¸»èœå•"""
    tests = {
        "1": ("æŸ¥æ‰¾æŒ‡å®š URL çš„é¡µé¢", test_find_page_by_url),
        "2": ("æµ‹è¯• get_or_create_page æ–°åŠŸèƒ½", test_get_page_with_url),
        "3": ("åœ¨æŠ“å–å™¨ä¸­ä½¿ç”¨", test_with_scraper),
        "4": ("äº¤äº’å¼é¡µé¢æŸ¥æ‰¾å™¨", interactive_page_finder)
    }
    
    print("\n" + "="*60)
    print("ğŸ§ª æµè§ˆå™¨ç®¡ç†å™¨æ–°åŠŸèƒ½æµ‹è¯•")
    print("="*60)
    print("\nå¯ç”¨æµ‹è¯•:")
    for key, (name, _) in tests.items():
        print(f"   {key}. {name}")
    
    print("\nâš ï¸ æ³¨æ„: è¿™äº›æµ‹è¯•éœ€è¦åœ¨ connect æ¨¡å¼ä¸‹è¿è¡Œ")
    print("   è¯·å…ˆå¯åŠ¨ Chrome: chrome.exe --remote-debugging-port=9222")
    print("   å¹¶æ‰“å¼€ä¸€äº›ç½‘é¡µï¼ˆå¦‚ SegmentFaultã€GitHub ç­‰ï¼‰\n")
    
    choice = input("é€‰æ‹©æµ‹è¯• (1-4): ").strip()
    
    if choice in tests:
        name, func = tests[choice]
        print(f"\nğŸš€ è¿è¡Œæµ‹è¯•: {name}")
        try:
            await func()
        except ConnectionError as e:
            print(f"\nâŒ è¿æ¥å¤±è´¥: {e}")
            print("\nğŸ’¡ è§£å†³æ–¹æ³•:")
            print("   1. å¯åŠ¨ Chrome: chrome.exe --remote-debugging-port=9222")
            print("   2. é‡æ–°è¿è¡Œæµ‹è¯•")
        except Exception as e:
            print(f"\nâŒ é”™è¯¯: {e}")
            import traceback
            traceback.print_exc()
    else:
        print("âŒ æ— æ•ˆé€‰æ‹©")


if __name__ == "__main__":
    asyncio.run(main())
