"""
åœ¨å·²æ‰“å¼€çš„é¡µé¢ä¸Šè¿›è¡Œåˆ†é¡µæŠ“å–
æ¼”ç¤ºå¦‚ä½•è¿æ¥åˆ°å·²æ‰“å¼€çš„é¡µé¢å¹¶è¿›è¡Œåˆ†é¡µæ•°æ®é‡‡é›†
"""

import asyncio
import sys
from pathlib import Path

sys.path.insert(0, str(Path(__file__).parent.parent / "lib"))
from browser import BrowserManager
from puppeteer import UniversalScraper, create_scraper_config


async def scrape_opened_page_with_pagination():
    """
    åœºæ™¯ï¼šåœ¨å·²æ‰“å¼€çš„é¡µé¢ä¸Šè¿›è¡Œåˆ†é¡µæŠ“å–
    
    æ­¥éª¤ï¼š
    1. è¿æ¥åˆ°å·²æ‰“å¼€çš„ Chrome
    2. æŸ¥æ‰¾ç›®æ ‡é¡µé¢
    3. åœ¨å½“å‰é¡µé¢ä¸Šè¿›è¡Œåˆ†é¡µæŠ“å–
    """
    print("\n" + "="*60)
    print("ğŸ¯ åœ¨å·²æ‰“å¼€çš„é¡µé¢ä¸Šè¿›è¡Œåˆ†é¡µæŠ“å–")
    print("="*60 + "\n")
    
    async with BrowserManager(mode="connect") as bm:
        # åˆ—å‡ºæ‰€æœ‰æ‰“å¼€çš„é¡µé¢
        pages_info = await bm.list_all_pages()
        
        print(f"ğŸ“‹ å½“å‰æ‰“å¼€çš„æ ‡ç­¾é¡µ ({len(pages_info)} ä¸ª):\n")
        for i, info in enumerate(pages_info, 1):
            print(f"{i}. {info['title'][:60]}")
            print(f"   {info['url']}\n")
        
        if not pages_info:
            print("âŒ æ²¡æœ‰æ‰¾åˆ°æ‰“å¼€çš„æ ‡ç­¾é¡µ")
            return
        
        # è¿æ¥åˆ°å·²æ‰“å¼€çš„ SegmentFault é¡µé¢
        print("ğŸ” æŸ¥æ‰¾ SegmentFault é¡µé¢...\n")
        page = await bm.get_or_create_page(target_url="devops.aliyun.com")
        
        if not page:
            print("âŒ æœªæ‰¾åˆ° SegmentFault é¡µé¢")
            return
        
        print(f"âœ… è¿æ¥åˆ°é¡µé¢: {page.url}")
        print(f"   æ ‡é¢˜: {await page.title()}\n")
        
        # é…ç½®æŠ“å–å™¨ï¼ˆæ”¯æŒåˆ†é¡µï¼‰
        config = create_scraper_config(
            url=page.url,  # ä½¿ç”¨å½“å‰é¡µé¢çš„ URL
            fields={
                "æ ‡é¢˜": ".yunxiao-projex-workitem-title",
                "äººå¤©": ".TextAndNumberModifier--statusName--yXxCXqU"
            },
            container_selector=".next-table-body tr.next-table-row",
            next_button_selector=".next-btn.next-pagination-item.next-next",  # ä¸‹ä¸€é¡µæŒ‰é’®
            delay=4.0,  # æ¯é¡µç­‰å¾…3ç§’
            max_pages=2  # æŠ“å–2é¡µ
        )
        
        # åˆ›å»ºæŠ“å–å™¨
        scraper = UniversalScraper(page, config)
        
        # â­ å…³é”®ï¼šä½¿ç”¨ scrape_from_current_page() è€Œä¸æ˜¯ scrape()
        # è¿™æ ·ä¸ä¼šé‡æ–°å¯¼èˆªï¼Œç›´æ¥åœ¨å½“å‰é¡µé¢ä¸ŠæŠ“å–
        print("ğŸš€ å¼€å§‹åˆ†é¡µæŠ“å–...\n")
        data = await scraper.scrape_from_current_page()
        
        # ä¿å­˜æ•°æ®
        if data:
            scraper.save_to_json("test_äº‘æ•ˆä»»åŠ¡ç±»å‹äººå¤©ç»Ÿè®¡.json")
            print(f"\nâœ… æˆåŠŸ!")
            print(f"   æ€»æ¡æ•°: {len(data)}")
        else:
            print("\nâš ï¸ æœªæŠ“å–åˆ°æ•°æ®")


async def main():
    print("\nâš ï¸ å‡†å¤‡å·¥ä½œ:")
    print("   1. å¯åŠ¨ Chrome: chrome.exe --remote-debugging-port=9222")
    print("   2. æ‰“å¼€è¦æŠ“å–çš„ç½‘é¡µï¼ˆå¦‚ SegmentFault æœç´¢ç»“æœé¡µï¼‰")
    
    try:
        await scrape_opened_page_with_pagination()
    except ConnectionError as e:
        print(f"\nâŒ è¿æ¥å¤±è´¥: {e}")
        print("\nğŸ’¡ è§£å†³æ–¹æ³•:")
        print("   1. å¯åŠ¨ Chrome: chrome.exe --remote-debugging-port=9222")
        print("   2. é‡æ–°è¿è¡Œ")
    except Exception as e:
        print(f"\nâŒ é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    asyncio.run(main())
