#!/usr/bin/env python3
"""
ä¸€é”®æŠ“å–è¡¨æ ¼ - ç®€åŒ–å‘½ä»¤è¡Œå·¥å…·
å¿«é€ŸæŠ“å–åˆ†é¡µè¡¨æ ¼æ•°æ®ï¼Œæ— éœ€ç¼–å†™ä»£ç 
"""

import asyncio
import argparse
import sys
from pathlib import Path

sys.path.insert(0, str(Path(__file__).parent.parent / "lib"))
from browser import BrowserManager
from puppeteer import TableScraper


async def quick_scrape(args):
    """å¿«é€ŸæŠ“å–è¡¨æ ¼"""
    
    print("\n" + "="*60)
    print("ğŸš€ ä¸€é”®æŠ“å–è¡¨æ ¼")
    print("="*60 + "\n")
    
    print(f"ğŸ“‹ é…ç½®:")
    print(f"   URL: {args.url}")
    print(f"   è¡¨æ ¼é€‰æ‹©å™¨: {args.table}")
    print(f"   åˆ†é¡µç±»å‹: {args.pagination_type}")
    print(f"   æœ€å¤§é¡µæ•°: {args.max_pages}")
    print(f"   è¾“å‡ºæ–‡ä»¶: {args.output}\n")
    
    try:
        async with BrowserManager(mode=args.mode) as bm:
            page = await bm.get_or_create_page()
            scraper = TableScraper(page)
            
            # 1. å¯¼èˆªåˆ°é¡µé¢
            print(f"ğŸŒ è®¿é—®: {args.url}")
            await page.goto(args.url)
            await page.wait_for_load_state("networkidle")
            print("   âœ“ é¡µé¢åŠ è½½å®Œæˆ\n")
            
            # 2. æ ¹æ®åˆ†é¡µç±»å‹æŠ“å–
            if args.pagination_type == "button":
                print(f"ğŸ“„ ä½¿ç”¨æŒ‰é’®åˆ†é¡µæŠ“å–...")
                if not args.next_button:
                    print("âŒ é”™è¯¯: æŒ‰é’®åˆ†é¡µéœ€è¦ --next-button å‚æ•°")
                    return
                
                await scraper.scrape_with_button_pagination(
                    table_selector=args.table,
                    next_button_selector=args.next_button,
                    max_pages=args.max_pages,
                    wait_time=args.wait
                )
            
            elif args.pagination_type == "url":
                print(f"ğŸ“„ ä½¿ç”¨ URL å‚æ•°åˆ†é¡µæŠ“å–...")
                await scraper.scrape_with_url_params(
                    base_url=args.url,
                    table_selector=args.table,
                    page_param=args.page_param,
                    start_page=1,
                    max_pages=args.max_pages,
                    wait_time=args.wait
                )
            
            elif args.pagination_type == "none":
                print(f"ğŸ“„ æå–å•é¡µè¡¨æ ¼...")
                data = await scraper.extract_table(table_selector=args.table)
                scraper.all_data.append(data)
            
            else:
                print(f"âŒ ä¸æ”¯æŒçš„åˆ†é¡µç±»å‹: {args.pagination_type}")
                return
            
            # 3. ä¿å­˜æ•°æ®
            print()
            if args.output.endswith('.json'):
                scraper.save_to_json(args.output)
            else:
                scraper.save_to_csv(args.output)
            
            # 4. æ˜¾ç¤ºæ‘˜è¦
            merged = scraper.merge_all_data()
            print(f"\nâœ… æŠ“å–å®Œæˆ!")
            print(f"   æ€»é¡µæ•°: {merged['total_pages']}")
            print(f"   æ€»è¡Œæ•°: {merged['total_rows']}")
            print(f"   åˆ—æ•°: {len(merged['headers'])}")
            print(f"   æ–‡ä»¶: {args.output}")
    
    except Exception as e:
        print(f"\nâŒ é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description="ä¸€é”®æŠ“å–åˆ†é¡µè¡¨æ ¼æ•°æ®",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:

  # æŠ“å–å•é¡µè¡¨æ ¼
  python scrape_table.py https://example.com/data --table "table.data" -o output.csv

  # æŠ“å–æŒ‰é’®åˆ†é¡µè¡¨æ ¼ï¼ˆå‰5é¡µï¼‰
  python scrape_table.py https://example.com/products \\
      --table "table#products" \\
      --pagination button \\
      --next-button "button.next" \\
      --max-pages 5 \\
      -o products.csv

  # æŠ“å– URL å‚æ•°åˆ†é¡µè¡¨æ ¼
  python scrape_table.py "https://example.com/search?q=python" \\
      --table "table.results" \\
      --pagination url \\
      --page-param page \\
      --max-pages 10 \\
      -o results.json

  # ä½¿ç”¨å·²æ‰“å¼€çš„ Chrome
  python scrape_table.py https://example.com/data \\
      --mode connect \\
      --table "table" \\
      -o data.csv
        """
    )
    
    # å¿…éœ€å‚æ•°
    parser.add_argument(
        "url",
        help="ç›®æ ‡é¡µé¢ URL"
    )
    
    # è¡¨æ ¼é…ç½®
    parser.add_argument(
        "--table", "-t",
        default="table",
        help="è¡¨æ ¼ CSS é€‰æ‹©å™¨ (é»˜è®¤: 'table')"
    )
    
    parser.add_argument(
        "--output", "-o",
        default="output.csv",
        help="è¾“å‡ºæ–‡ä»¶å (æ”¯æŒ .csv å’Œ .json)"
    )
    
    # åˆ†é¡µé…ç½®
    parser.add_argument(
        "--pagination", "-p",
        choices=["none", "button", "url"],
        default="none",
        help="åˆ†é¡µç±»å‹ (none=å•é¡µ, button=æŒ‰é’®, url=URLå‚æ•°)"
    )
    
    parser.add_argument(
        "--next-button",
        help="ä¸‹ä¸€é¡µæŒ‰é’®é€‰æ‹©å™¨ (pagination=button æ—¶éœ€è¦)"
    )
    
    parser.add_argument(
        "--page-param",
        default="page",
        help="URL é¡µç å‚æ•°å (pagination=url æ—¶ä½¿ç”¨)"
    )
    
    parser.add_argument(
        "--max-pages",
        type=int,
        default=10,
        help="æœ€å¤§æŠ“å–é¡µæ•° (é»˜è®¤: 10)"
    )
    
    parser.add_argument(
        "--wait",
        type=float,
        default=2.0,
        help="æ¯é¡µç­‰å¾…æ—¶é—´(ç§’) (é»˜è®¤: 2.0)"
    )
    
    # æµè§ˆå™¨é…ç½®
    parser.add_argument(
        "--mode",
        choices=["launch", "connect"],
        default="launch",
        help="æµè§ˆå™¨æ¨¡å¼ (é»˜è®¤: launch)"
    )
    
    parser.add_argument(
        "--headless",
        action="store_true",
        help="æ— å¤´æ¨¡å¼è¿è¡Œ"
    )
    
    # è¾…åŠ©åŠŸèƒ½
    parser.add_argument(
        "--version", "-v",
        action="version",
        version="Quick Scrape v1.0.0"
    )
    
    args = parser.parse_args()
    
    # ä¿å­˜ä¸ºå®ä¾‹å˜é‡
    args.pagination_type = args.pagination
    
    # è¿è¡Œ
    try:
        asyncio.run(quick_scrape(args))
    except KeyboardInterrupt:
        print("\n\nğŸ‘‹ ç”¨æˆ·ä¸­æ–­")


if __name__ == "__main__":
    main()