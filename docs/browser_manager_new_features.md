# æµè§ˆå™¨ç®¡ç†å™¨æ–°åŠŸèƒ½ - è¿æ¥åˆ°æŒ‡å®š URL æ ‡ç­¾é¡µ

## ğŸ¯ åŠŸèƒ½æ¦‚è¿°

æ›´æ–°åçš„ `BrowserManager` æ”¯æŒï¼š
- âœ… æŸ¥æ‰¾å¹¶è¿æ¥åˆ°å·²æ‰“å¼€çš„æŒ‡å®š URL æ ‡ç­¾é¡µ
- âœ… æ”¯æŒéƒ¨åˆ†åŒ¹é…å’Œç²¾ç¡®åŒ¹é…
- âœ… åˆ—å‡ºæ‰€æœ‰æ‰“å¼€çš„æ ‡ç­¾é¡µä¿¡æ¯
- âœ… åœ¨å·²æ‰“å¼€çš„æ ‡ç­¾é¡µä¸Šç›´æ¥æ“ä½œï¼ˆæ— éœ€é‡æ–°å¯¼èˆªï¼‰

---

## ğŸ†• æ–°å¢æ–¹æ³•

### 1. `get_or_create_page(target_url=None)`

è·å–æˆ–åˆ›å»ºé¡µé¢ï¼Œæ”¯æŒæŸ¥æ‰¾æŒ‡å®š URL çš„æ ‡ç­¾é¡µã€‚

**å‚æ•°**ï¼š
- `target_url` (å¯é€‰): ç›®æ ‡ URL
  - å¦‚æœæä¾›ï¼Œä¼šæŸ¥æ‰¾åŒ¹é…æ­¤ URL çš„å·²æ‰“å¼€æ ‡ç­¾é¡µ
  - æ”¯æŒéƒ¨åˆ†åŒ¹é…ï¼ˆURL åŒ…å«å…³ç³»ï¼‰
  - å¦‚æœæ‰¾ä¸åˆ°ï¼ŒæŒ‰åŸé€»è¾‘è¿”å›æˆ–åˆ›å»ºé¡µé¢

**ç¤ºä¾‹**ï¼š

```python
from browser import BrowserManager

async with BrowserManager(mode="connect") as bm:
    # åŸå§‹ç”¨æ³•ï¼šè·å–æœ€åä¸€ä¸ªæ´»è·ƒé¡µé¢
    page = await bm.get_or_create_page()
    
    # æ–°ç”¨æ³•ï¼šæŸ¥æ‰¾å¹¶è¿æ¥åˆ° SegmentFault é¡µé¢
    page = await bm.get_or_create_page(target_url="segmentfault.com")
    
    # ç›´æ¥åœ¨è¿™ä¸ªé¡µé¢ä¸Šæ“ä½œï¼Œæ— éœ€é‡æ–°å¯¼èˆª
    print(await page.title())
```

---

### 2. `find_page_by_url(target_url, exact_match=False)`

åœ¨æ‰€æœ‰æ‰“å¼€çš„æ ‡ç­¾é¡µä¸­æŸ¥æ‰¾åŒ¹é…æŒ‡å®š URL çš„é¡µé¢ã€‚

**å‚æ•°**ï¼š
- `target_url`: ç›®æ ‡ URL
- `exact_match`: æ˜¯å¦ç²¾ç¡®åŒ¹é…
  - `False`ï¼ˆé»˜è®¤ï¼‰: éƒ¨åˆ†åŒ¹é…ï¼ˆé¡µé¢ URL åŒ…å« target_urlï¼‰
  - `True`: ç²¾ç¡®åŒ¹é…ï¼ˆé¡µé¢ URL å®Œå…¨ç­‰äº target_urlï¼‰

**è¿”å›**ï¼š
- æ‰¾åˆ°çš„ `Page` å¯¹è±¡ï¼Œå¦‚æœæ²¡æ‰¾åˆ°è¿”å› `None`

**ç¤ºä¾‹**ï¼š

```python
# éƒ¨åˆ†åŒ¹é…
page = await bm.find_page_by_url("github.com")
if page:
    print(f"æ‰¾åˆ°: {page.url}")

# ç²¾ç¡®åŒ¹é…
page = await bm.find_page_by_url(
    "https://github.com/trending",
    exact_match=True
)
```

---

### 3. `list_all_pages()`

åˆ—å‡ºæ‰€æœ‰æ‰“å¼€çš„é¡µé¢ä¿¡æ¯ã€‚

**è¿”å›**ï¼š
- åŒ…å«æ‰€æœ‰é¡µé¢ä¿¡æ¯çš„åˆ—è¡¨

**ç¤ºä¾‹**ï¼š

```python
pages_info = await bm.list_all_pages()

for info in pages_info:
    print(f"æ ‡é¢˜: {info['title']}")
    print(f"URL: {info['url']}")
    print(f"Context: {info['context_index']}")
    print(f"å·²å…³é—­: {info['is_closed']}")
```

---

## ğŸš€ ä½¿ç”¨åœºæ™¯

### åœºæ™¯ 1: åœ¨å·²æ‰“å¼€çš„é¡µé¢ä¸Šç›´æ¥æŠ“å–

**éœ€æ±‚**ï¼šç”¨æˆ·å·²ç»åœ¨æµè§ˆå™¨ä¸­æ‰“å¼€äº† SegmentFaultï¼Œæƒ³ç›´æ¥åœ¨è¿™ä¸ªé¡µé¢ä¸ŠæŠ“å–æ•°æ®ï¼Œè€Œä¸æ˜¯é‡æ–°å¯¼èˆªã€‚

**è§£å†³æ–¹æ¡ˆ**ï¼š

```python
from browser import BrowserManager
from puppeteer import UniversalScraper, create_scraper_config

async def scrape_existing_tab():
    async with BrowserManager(mode="connect") as bm:
        # è¿æ¥åˆ°å·²æ‰“å¼€çš„ SegmentFault é¡µé¢
        page = await bm.get_or_create_page(target_url="segmentfault.com")
        
        # é…ç½®æŠ“å–å™¨ï¼ˆä½¿ç”¨å½“å‰é¡µé¢çš„ URLï¼‰
        config = create_scraper_config(
            url=page.url,  # ä½¿ç”¨å½“å‰ URL
            fields={
                "æ ‡é¢˜": "h3 a.text-body",
                "æŠ•ç¥¨æ•°": ".num-card .font-size-16"
            },
            container_selector=".list-group-item",
            delay=2.0
        )
        
        scraper = UniversalScraper(page, config)
        
        # ç›´æ¥æŠ“å–å½“å‰é¡µé¢ï¼Œæ— éœ€å¯¼èˆª
        data = await scraper.scrape_current_page()
        
        print(f"âœ… æˆåŠŸæŠ“å– {len(data)} æ¡æ•°æ®")
```

---

### åœºæ™¯ 2: æ‰¹é‡æ“ä½œå¤šä¸ªæ ‡ç­¾é¡µ

**éœ€æ±‚**ï¼šç”¨æˆ·æ‰“å¼€äº†å¤šä¸ªç½‘é¡µï¼Œæƒ³ä¾æ¬¡åœ¨æ¯ä¸ªé¡µé¢ä¸Šæ‰§è¡Œæ“ä½œã€‚

**è§£å†³æ–¹æ¡ˆ**ï¼š

```python
async def process_all_tabs():
    async with BrowserManager(mode="connect") as bm:
        # è·å–æ‰€æœ‰æ‰“å¼€çš„é¡µé¢
        pages_info = await bm.list_all_pages()
        
        print(f"æ‰¾åˆ° {len(pages_info)} ä¸ªæ ‡ç­¾é¡µ")
        
        for info in pages_info:
            url = info['url']
            
            # è¿æ¥åˆ°è¿™ä¸ªé¡µé¢
            page = await bm.find_page_by_url(url, exact_match=True)
            
            if page:
                # åœ¨è¿™ä¸ªé¡µé¢ä¸Šæ‰§è¡Œæ“ä½œ
                await page.bring_to_front()  # åˆ‡æ¢åˆ°å‰å°
                title = await page.title()
                print(f"å¤„ç†: {title}")
                
                # æ‰§è¡Œä½ çš„æ“ä½œ...
                # await page.screenshot(path=f"{title}.png")
```

---

### åœºæ™¯ 3: æŸ¥æ‰¾ç‰¹å®šç½‘ç«™çš„å¤šä¸ªæ ‡ç­¾é¡µ

**éœ€æ±‚**ï¼šæŸ¥æ‰¾æ‰€æœ‰ GitHub ç›¸å…³çš„æ ‡ç­¾é¡µã€‚

**è§£å†³æ–¹æ¡ˆ**ï¼š

```python
async def find_github_tabs():
    async with BrowserManager(mode="connect") as bm:
        github_pages = []
        
        # éå†æ‰€æœ‰é¡µé¢
        for context in bm.browser.contexts:
            for page in context.pages:
                if "github.com" in page.url.lower():
                    github_pages.append(page)
        
        print(f"æ‰¾åˆ° {len(github_pages)} ä¸ª GitHub æ ‡ç­¾é¡µ:")
        for page in github_pages:
            print(f"  - {await page.title()}")
            print(f"    {page.url}")
```

---

### åœºæ™¯ 4: æ™ºèƒ½æŠ“å–å™¨ï¼ˆè‡ªåŠ¨æŸ¥æ‰¾æˆ–åˆ›å»ºé¡µé¢ï¼‰

**éœ€æ±‚**ï¼šå¦‚æœé¡µé¢å·²æ‰“å¼€å°±ç›´æ¥ç”¨ï¼Œæ²¡æ‰“å¼€å°±è‡ªåŠ¨æ‰“å¼€ã€‚

**è§£å†³æ–¹æ¡ˆ**ï¼š

```python
async def smart_scraper(target_url: str):
    async with BrowserManager(mode="connect") as bm:
        # å°è¯•æŸ¥æ‰¾å·²æ‰“å¼€çš„é¡µé¢
        page = await bm.get_or_create_page(target_url=target_url)
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦å¯¼èˆª
        if target_url not in page.url:
            print(f"é¡µé¢ä¸åŒ¹é…ï¼Œå¯¼èˆªåˆ°: {target_url}")
            await page.goto(target_url)
        else:
            print(f"ä½¿ç”¨å·²æ‰“å¼€çš„é¡µé¢: {page.url}")
        
        # ç»§ç»­æŠ“å–...
```

---

## ğŸ“ å®Œæ•´ç¤ºä¾‹

### ç¤ºä¾‹ 1: åŸºç¡€ç”¨æ³•

```python
import asyncio
from browser import BrowserManager

async def basic_usage():
    # è¿æ¥åˆ°å·²æ‰“å¼€çš„ Chrome
    async with BrowserManager(mode="connect") as bm:
        # åˆ—å‡ºæ‰€æœ‰æ ‡ç­¾é¡µ
        pages = await bm.list_all_pages()
        print(f"æ‰“å¼€äº† {len(pages)} ä¸ªæ ‡ç­¾é¡µ")
        
        # æŸ¥æ‰¾ SegmentFault é¡µé¢
        page = await bm.find_page_by_url("segmentfault.com")
        
        if page:
            print(f"æ‰¾åˆ°é¡µé¢: {await page.title()}")
            # åœ¨è¿™ä¸ªé¡µé¢ä¸Šæ“ä½œ...
        else:
            print("æœªæ‰¾åˆ°é¡µé¢")

asyncio.run(basic_usage())
```

---

### ç¤ºä¾‹ 2: ä¸æŠ“å–å™¨é›†æˆ

```python
import asyncio
from browser import BrowserManager
from puppeteer import UniversalScraper, create_scraper_config

async def scrape_with_existing_tab():
    async with BrowserManager(mode="connect") as bm:
        # æ–¹å¼ 1: ç›´æ¥åœ¨ get_or_create_page ä¸­æŒ‡å®š URL
        page = await bm.get_or_create_page(target_url="segmentfault.com")
        
        # æ–¹å¼ 2: å…ˆæŸ¥æ‰¾ï¼Œå†ä½¿ç”¨
        # page = await bm.find_page_by_url("segmentfault.com")
        # if not page:
        #     page = await bm.get_or_create_page()
        #     await page.goto("https://segmentfault.com/")
        
        # é…ç½®æŠ“å–å™¨
        config = create_scraper_config(
            url=page.url,  # ä½¿ç”¨å½“å‰ URL
            fields={
                "æ ‡é¢˜": "h3 a.text-body",
                "æŠ•ç¥¨æ•°": ".num-card .font-size-16"
            },
            container_selector=".list-group-item",
            delay=2.0
        )
        
        # åˆ›å»ºæŠ“å–å™¨
        scraper = UniversalScraper(page, config)
        
        # ç›´æ¥æŠ“å–å½“å‰é¡µé¢ï¼ˆä¸å¯¼èˆªï¼‰
        data = await scraper.scrape_current_page()
        
        # ä¿å­˜æ•°æ®
        scraper.save_to_json("output.json")
        print(f"âœ… æŠ“å–äº† {len(data)} æ¡æ•°æ®")

asyncio.run(scrape_with_existing_tab())
```

---

### ç¤ºä¾‹ 3: äº¤äº’å¼æ ‡ç­¾é¡µé€‰æ‹©

```python
import asyncio
from browser import BrowserManager

async def interactive_tab_selector():
    async with BrowserManager(mode="connect") as bm:
        # åˆ—å‡ºæ‰€æœ‰æ ‡ç­¾é¡µ
        pages_info = await bm.list_all_pages()
        
        print("\nå¯ç”¨çš„æ ‡ç­¾é¡µ:")
        for i, info in enumerate(pages_info, 1):
            print(f"{i}. {info['title']}")
            print(f"   {info['url']}\n")
        
        # ç”¨æˆ·é€‰æ‹©
        choice = int(input("é€‰æ‹©æ ‡ç­¾é¡µ (è¾“å…¥ç¼–å·): ")) - 1
        
        if 0 <= choice < len(pages_info):
            target_url = pages_info[choice]['url']
            
            # è¿æ¥åˆ°é€‰ä¸­çš„æ ‡ç­¾é¡µ
            page = await bm.find_page_by_url(target_url, exact_match=True)
            
            if page:
                print(f"\nâœ… å·²è¿æ¥åˆ°: {await page.title()}")
                await page.bring_to_front()  # åˆ‡æ¢åˆ°å‰å°
                
                # åœ¨è¿™ä¸ªé¡µé¢ä¸Šæ‰§è¡Œæ“ä½œ...
        else:
            print("æ— æ•ˆé€‰æ‹©")

asyncio.run(interactive_tab_selector())
```

---

## ğŸ§ª æµ‹è¯•

è¿è¡Œæµ‹è¯•è„šæœ¬ï¼š

```bash
# 1. å¯åŠ¨ Chromeï¼ˆå¼€å¯è¿œç¨‹è°ƒè¯•ï¼‰
chrome.exe --remote-debugging-port=9222

# 2. åœ¨ Chrome ä¸­æ‰“å¼€ä¸€äº›ç½‘é¡µï¼ˆå¦‚ SegmentFaultã€GitHub ç­‰ï¼‰

# 3. è¿è¡Œæµ‹è¯•
python test_browser_manager.py

# é€‰æ‹©æµ‹è¯•ï¼š
#   1. æŸ¥æ‰¾æŒ‡å®š URL çš„é¡µé¢
#   2. æµ‹è¯• get_or_create_page æ–°åŠŸèƒ½
#   3. åœ¨æŠ“å–å™¨ä¸­ä½¿ç”¨
#   4. äº¤äº’å¼é¡µé¢æŸ¥æ‰¾å™¨
```

---

## âš ï¸ æ³¨æ„äº‹é¡¹

### 1. ä»…åœ¨ connect æ¨¡å¼ä¸‹æœ‰æ•ˆ

```python
# âœ… æ­£ç¡®
async with BrowserManager(mode="connect") as bm:
    page = await bm.get_or_create_page(target_url="...")

# âŒ é”™è¯¯ï¼ˆlaunch æ¨¡å¼ä¸‹æ²¡æœ‰å·²æ‰“å¼€çš„æ ‡ç­¾é¡µï¼‰
async with BrowserManager(mode="launch") as bm:
    page = await bm.get_or_create_page(target_url="...")
```

### 2. URL åŒ¹é…è§„åˆ™

```python
# éƒ¨åˆ†åŒ¹é…ï¼ˆé»˜è®¤ï¼‰
await bm.find_page_by_url("github.com")
# âœ… åŒ¹é…: https://github.com/trending
# âœ… åŒ¹é…: https://github.com/topics/python
# âœ… åŒ¹é…: https://www.github.com/

# ç²¾ç¡®åŒ¹é…
await bm.find_page_by_url("https://github.com/trending", exact_match=True)
# âœ… åŒ¹é…: https://github.com/trending
# âŒ ä¸åŒ¹é…: https://github.com/trending?since=weekly
```

### 3. é¡µé¢å¯èƒ½å·²å…³é—­

```python
page = await bm.find_page_by_url("example.com")

if page:
    if not page.is_closed():
        # å®‰å…¨æ“ä½œ
        await page.reload()
    else:
        print("é¡µé¢å·²å…³é—­")
```

---

## ğŸ’¡ æœ€ä½³å®è·µ

### 1. ä¼˜é›…çš„å›é€€æœºåˆ¶

```python
async def get_target_page(bm, target_url):
    """è·å–ç›®æ ‡é¡µé¢ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™åˆ›å»º"""
    page = await bm.get_or_create_page(target_url=target_url)
    
    # æ£€æŸ¥ URL æ˜¯å¦åŒ¹é…
    if target_url not in page.url:
        # ä¸åŒ¹é…ï¼Œéœ€è¦å¯¼èˆª
        await page.goto(target_url)
    
    return page
```

### 2. æ‰¹é‡å¤„ç†

```python
async def process_multiple_sites(site_urls: list):
    async with BrowserManager(mode="connect") as bm:
        for url in site_urls:
            page = await bm.get_or_create_page(target_url=url)
            # å¤„ç†æ¯ä¸ªé¡µé¢...
```

### 3. é”™è¯¯å¤„ç†

```python
try:
    page = await bm.find_page_by_url("example.com")
    if page and not page.is_closed():
        await page.reload()
except Exception as e:
    print(f"æ“ä½œå¤±è´¥: {e}")
```

---

## ğŸ“š ç›¸å…³æ–‡æ¡£

- æµè§ˆå™¨ç®¡ç†å™¨åŸºç¡€: `/Users/admins/work/openai/README.md`
- é€šç”¨æŠ“å–å™¨: `/Users/admins/work/openai/docs/universal_scraper_guide.md`
- ç¤ºä¾‹ä»£ç : `/Users/admins/work/openai/examples/`

---

## ğŸ¯ æ€»ç»“

æ›´æ–°åçš„ `BrowserManager` è®©ä½ å¯ä»¥ï¼š

1. âœ… **ç›´æ¥ä½¿ç”¨å·²æ‰“å¼€çš„æ ‡ç­¾é¡µ** - æ— éœ€é‡æ–°å¯¼èˆª
2. âœ… **æŸ¥æ‰¾ç‰¹å®š URL çš„é¡µé¢** - æ”¯æŒéƒ¨åˆ†/ç²¾ç¡®åŒ¹é…
3. âœ… **åˆ—å‡ºæ‰€æœ‰æ ‡ç­¾é¡µ** - äº†è§£æµè§ˆå™¨çŠ¶æ€
4. âœ… **æ›´é«˜æ•ˆçš„æ•°æ®é‡‡é›†** - å‡å°‘é¡µé¢åŠ è½½æ—¶é—´

è¿™äº›åŠŸèƒ½ç‰¹åˆ«é€‚åˆï¼š
- ğŸ”„ å¤šæ ‡ç­¾é¡µæ‰¹é‡å¤„ç†
- ğŸ“Š å®æ—¶æ•°æ®ç›‘æ§
- ğŸ¯ ç²¾ç¡®é¡µé¢æ“ä½œ
- âš¡ å¿«é€ŸåŸå‹å¼€å‘
