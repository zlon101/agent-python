# é€šç”¨ç½‘é¡µæ•°æ®æŠ“å–å™¨ä½¿ç”¨æ–‡æ¡£

## ğŸ“– æ¦‚è¿°

é€šç”¨ç½‘é¡µæ•°æ®æŠ“å–å™¨æ˜¯ä¸€ä¸ªçµæ´»ã€å¼ºå¤§çš„æ•°æ®é‡‡é›†å·¥å…·ï¼Œæ”¯æŒï¼š
- âœ… è‡ªå®šä¹‰å­—æ®µå’ŒCSSé€‰æ‹©å™¨
- âœ… åˆ†é¡µæ•°æ®æŠ“å–ï¼ˆæŒ‰é’®/URLå‚æ•°ï¼‰
- âœ… çµæ´»çš„å»¶è¿Ÿé…ç½®
- âœ… é¡µç èŒƒå›´æ§åˆ¶
- âœ… JSONæ ¼å¼è¾“å‡º
- âœ… ä¸LangChain Agenté›†æˆ

---

## ğŸš€ å¿«é€Ÿå¼€å§‹

### æ–¹æ³•1ï¼šç›´æ¥ä½¿ç”¨æŠ“å–å™¨

```python
import asyncio
from browser import BrowserManager
from puppeteer import UniversalScraper, create_scraper_config

async def main():
    async with BrowserManager(mode="launch") as bm:
        page = await bm.get_or_create_page()
        
        # åˆ›å»ºé…ç½®
        config = create_scraper_config(
            url="https://segmentfault.com/",
            fields={
                "æ ‡é¢˜": "h3 a.text-body",
                "æŠ•ç¥¨æ•°": ".num-card .font-size-16"
            },
            container_selector=".list-group-item",
            next_button_selector="a.page-link[rel='next']",
            delay=5.0,
            max_pages=2
        )
        
        # æ‰§è¡ŒæŠ“å–
        scraper = UniversalScraper(page, config)
        data = await scraper.scrape()
        
        # ä¿å­˜æ•°æ®
        scraper.save_to_json("output.json")

asyncio.run(main())
```

### æ–¹æ³•2ï¼šé€šè¿‡Agentä½¿ç”¨

```python
from custom_agent import create_custom_agent
from puppeteer import get_browser_tools, get_universal_scraping_tools

# è·å–å·¥å…·
browser_tools = get_browser_tools(browser)
scraping_tools = get_universal_scraping_tools(browser)
all_tools = browser_tools + scraping_tools

# åˆ›å»ºAgent
agent = create_custom_agent(tools=all_tools)

# æ‰§è¡Œä»»åŠ¡
task = """
æŠ“å– SegmentFault é¦–é¡µæ–‡ç« ï¼š
- å­—æ®µ: {"æ ‡é¢˜": "h3 a", "æŠ•ç¥¨æ•°": ".vote-count"}
- å®¹å™¨: .list-group-item
- æŠ“å–2é¡µï¼Œåœç•™5ç§’
"""
result = await agent.ainvoke({"messages": [HumanMessage(task)]})
```

---

## ğŸ”§ é…ç½®å‚æ•°è¯¦è§£

### ScraperConfig å‚æ•°

| å‚æ•° | ç±»å‹ | å¿…å¡« | è¯´æ˜ |
|------|------|------|------|
| `url` | str | âœ… | ç›®æ ‡ç½‘å€ |
| `fields` | List[FieldConfig] | âœ… | å­—æ®µé…ç½®åˆ—è¡¨ |
| `container_selector` | str | âœ… | æ•°æ®é¡¹å®¹å™¨çš„CSSé€‰æ‹©å™¨ |
| `next_button_selector` | str | âŒ | ä¸‹ä¸€é¡µæŒ‰é’®é€‰æ‹©å™¨ï¼ˆåˆ†é¡µç”¨ï¼‰ |
| `page_range` | tuple | âŒ | é¡µç èŒƒå›´ (start, end) |
| `delay` | float | âŒ | é¡µé¢ç­‰å¾…æ—¶é—´ï¼ˆç§’ï¼‰ï¼Œé»˜è®¤3.0 |
| `max_pages` | int | âŒ | æœ€å¤§æŠ“å–é¡µæ•°ï¼Œ0è¡¨ç¤ºæ— é™åˆ¶ |

### FieldConfig å‚æ•°

| å‚æ•° | ç±»å‹ | å¿…å¡« | è¯´æ˜ |
|------|------|------|------|
| `name` | str | âœ… | å­—æ®µå |
| `selector` | str | âœ… | CSSé€‰æ‹©å™¨ |
| `attribute` | str | âŒ | æå–å±æ€§ï¼ˆå¦‚hrefã€srcï¼‰ï¼Œé»˜è®¤æå–æ–‡æœ¬ |
| `multiple` | bool | âŒ | æ˜¯å¦æå–å¤šä¸ªå€¼ï¼Œé»˜è®¤False |

---

## ğŸ“ ä½¿ç”¨ç¤ºä¾‹

### ç¤ºä¾‹1ï¼šåŸºç¡€æŠ“å–ï¼ˆå•é¡µï¼‰

```python
config = create_scraper_config(
    url="https://example.com",
    fields={
        "æ ‡é¢˜": "h2.title",
        "æè¿°": "p.description"
    },
    container_selector=".item",
    delay=3.0
)
```

### ç¤ºä¾‹2ï¼šåˆ†é¡µæŠ“å–ï¼ˆæŒ‰é’®ï¼‰

```python
config = create_scraper_config(
    url="https://example.com",
    fields={
        "æ ‡é¢˜": "h2.title"
    },
    container_selector=".item",
    next_button_selector="button.next",  # ä¸‹ä¸€é¡µæŒ‰é’®
    delay=5.0,
    max_pages=5  # æœ€å¤šæŠ“å–5é¡µ
)
```

### ç¤ºä¾‹3ï¼šé¡µç èŒƒå›´æ§åˆ¶

```python
config = create_scraper_config(
    url="https://example.com",
    fields={"æ ‡é¢˜": "h2"},
    container_selector=".item",
    next_button_selector="a.next",
    page_range=(2, 5),  # åªæŠ“å–ç¬¬2-5é¡µ
    delay=4.0
)
```

### ç¤ºä¾‹4ï¼šæå–å±æ€§å€¼

```python
from puppeteer.universal_scraper import FieldConfig, ScraperConfig

config = ScraperConfig(
    url="https://example.com",
    fields=[
        FieldConfig(name="æ ‡é¢˜", selector="h2 a"),
        FieldConfig(name="é“¾æ¥", selector="h2 a", attribute="href"),  # æå–hrefå±æ€§
        FieldConfig(name="å›¾ç‰‡", selector="img", attribute="src")     # æå–srcå±æ€§
    ],
    container_selector=".item",
    delay=3.0
)
```

### ç¤ºä¾‹5ï¼šæå–å¤šä¸ªå€¼

```python
config = ScraperConfig(
    url="https://example.com",
    fields=[
        FieldConfig(name="æ ‡é¢˜", selector="h2"),
        FieldConfig(
            name="æ ‡ç­¾", 
            selector=".tag", 
            multiple=True  # æå–æ‰€æœ‰æ ‡ç­¾
        )
    ],
    container_selector=".item",
    delay=3.0
)
```

---

## ğŸ¯ å®Œæ•´ç¤ºä¾‹ï¼šSegmentFault

æ ¹æ®ä½ çš„éœ€æ±‚å®ç°ï¼š

```python
import asyncio
from browser import BrowserManager
from puppeteer import UniversalScraper, create_scraper_config

async def scrape_segmentfault():
    """
    æŠ“å– SegmentFault é¦–é¡µæ–‡ç« åˆ—è¡¨
    
    éœ€æ±‚ï¼š
    - URL: https://segmentfault.com/
    - å®¹å™¨: .list-group.list-group-flush
    - å­—æ®µ:
      * æ ‡é¢˜: h3 a.text-body
      * æŠ•ç¥¨æ•°é‡: .num-card .font-size-16
      * é˜…è¯»æ•°é‡: .num-card.text-secondary .font-size-16
    - ä¸‹ä¸€é¡µ: a.page-link[rel='next']
    - å»¶è¿Ÿ: 5ç§’
    """
    
    async with BrowserManager(mode="launch", headless=False) as bm:
        page = await bm.get_or_create_page()
        
        # é…ç½®
        config = create_scraper_config(
            url="https://segmentfault.com/",
            fields={
                "æ ‡é¢˜": "h3 a.text-body",
                "æŠ•ç¥¨æ•°é‡": ".num-card .font-size-16",
                "é˜…è¯»æ•°é‡": ".num-card.text-secondary .font-size-16"
            },
            container_selector=".list-group.list-group-flush > .list-group-item",
            next_button_selector="a.page-link[rel='next']",
            delay=5.0,
            max_pages=2
        )
        
        # æŠ“å–
        scraper = UniversalScraper(page, config)
        data = await scraper.scrape()
        
        # ä¿å­˜ï¼ˆç®€åŒ–æ ¼å¼ï¼Œç›´æ¥ä¿å­˜æ•°æ®æ•°ç»„ï¼‰
        import json
        with open("output.json", 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… æŠ“å–å®Œæˆï¼š{len(data)} æ¡æ•°æ®")
        return data

# è¿è¡Œ
asyncio.run(scrape_segmentfault())
```

**è¾“å‡ºæ ¼å¼**ï¼ˆoutput.jsonï¼‰ï¼š

```json
[
  {
    "æ ‡é¢˜": "å¦‚ä½•ä¼˜åŒ–å‰ç«¯æ€§èƒ½",
    "æŠ•ç¥¨æ•°é‡": "5",
    "é˜…è¯»æ•°é‡": "120"
  },
  {
    "æ ‡é¢˜": "Pythonå¼‚æ­¥ç¼–ç¨‹å®æˆ˜",
    "æŠ•ç¥¨æ•°é‡": "8",
    "é˜…è¯»æ•°é‡": "256"
  }
]
```

---

## ğŸ› ï¸ Agentå·¥å…·ä½¿ç”¨

### å¯ç”¨å·¥å…·

| å·¥å…·å | åŠŸèƒ½ | ä½¿ç”¨åœºæ™¯ |
|--------|------|----------|
| `scrape_web_data` | é€šç”¨æŠ“å– | å¤§éƒ¨åˆ†åœºæ™¯ |
| `scrape_web_data_advanced` | é«˜çº§æŠ“å– | éœ€è¦é¡µç èŒƒå›´æ§åˆ¶ |
| `preview_scrape` | é¢„è§ˆç»“æœ | æµ‹è¯•é€‰æ‹©å™¨ |

### å·¥å…·å‚æ•°

#### scrape_web_data

```python
{
    "url": "https://example.com",
    "fields": '{"æ ‡é¢˜": "h2", "æè¿°": "p"}',  # JSONå­—ç¬¦ä¸²
    "container_selector": ".item",
    "next_button_selector": "button.next",  # å¯é€‰
    "delay": 5.0,
    "max_pages": 3,
    "filename": "output.json"
}
```

#### scrape_web_data_advanced

```python
{
    "url": "https://example.com",
    "fields_json": '{"æ ‡é¢˜": "h2"}',
    "container_selector": ".item",
    "next_button_selector": "a.next",
    "page_range_start": 2,  # èµ·å§‹é¡µ
    "page_range_end": 5,    # ç»“æŸé¡µ
    "delay": 4.0,
    "filename": "output.json"
}
```

#### preview_scrape

```python
{
    "url": "https://example.com",
    "fields": '{"æ ‡é¢˜": "h2"}',
    "container_selector": ".item",
    "limit": 3  # é¢„è§ˆ3æ¡
}
```

---

## ğŸ§ª æµ‹è¯•éªŒè¯

### å¿«é€Ÿæµ‹è¯•

```bash
# æµ‹è¯•åŸºç¡€åŠŸèƒ½
python test_universal_scraper.py
# é€‰æ‹© 1 (åŸºç¡€æŠ“å–)

# æµ‹è¯•åˆ†é¡µåŠŸèƒ½
python test_universal_scraper.py
# é€‰æ‹© 2 (åˆ†é¡µæŠ“å–)
```

### ä½¿ç”¨ç¤ºä¾‹è„šæœ¬

```bash
# ç›´æ¥ä½¿ç”¨æŠ“å–å™¨
python lib/puppeteer/universal_scraper/example.py
# é€‰æ‹© 6 (å®Œæ•´ç”¨æˆ·åœºæ™¯)

# Agenté›†æˆ
python examples/universal_scraper_agent.py
# é€‰æ‹© 1 (SegmentFault)
```

---

## â“ å¸¸è§é—®é¢˜

### Q1: å¦‚ä½•æ‰¾åˆ°æ­£ç¡®çš„CSSé€‰æ‹©å™¨ï¼Ÿ

**æ–¹æ³•1ï¼šæµè§ˆå™¨å¼€å‘è€…å·¥å…·**
1. å³é”®ç‚¹å‡»ç›®æ ‡å…ƒç´  â†’ æ£€æŸ¥
2. åœ¨Elementsé¢æ¿ä¸­æ‰¾åˆ°å…ƒç´ 
3. å³é”® â†’ Copy â†’ Copy selector

**æ–¹æ³•2ï¼šä½¿ç”¨preview_scrapeæµ‹è¯•**
```python
# å…ˆç”¨é¢„è§ˆå·¥å…·æµ‹è¯•
result = await preview_scrape(
    url="https://example.com",
    fields='{"æ ‡é¢˜": "h2.title"}',
    container_selector=".item",
    limit=3
)
print(result)
```

### Q2: åˆ†é¡µæŒ‰é’®ä¸å·¥ä½œæ€ä¹ˆåŠï¼Ÿ

**æ£€æŸ¥é¡¹ï¼š**
1. é€‰æ‹©å™¨æ˜¯å¦å‡†ç¡®ï¼Ÿ
2. æŒ‰é’®æ˜¯å¦åœ¨é¡µé¢åŠ è½½åæ‰å‡ºç°ï¼Ÿ
3. å»¶è¿Ÿæ—¶é—´æ˜¯å¦è¶³å¤Ÿï¼Ÿ

**è°ƒè¯•æ–¹æ³•ï¼š**
```python
# å¢åŠ å»¶è¿Ÿæ—¶é—´
config.delay = 8.0

# ä½¿ç”¨æ›´ç²¾ç¡®çš„é€‰æ‹©å™¨
next_button_selector = "a.page-link[rel='next']"  # å¸¦å±æ€§
```

### Q3: æå–çš„æ•°æ®ä¸ºç©ºï¼Ÿ

**å¯èƒ½åŸå› ï¼š**
1. å®¹å™¨é€‰æ‹©å™¨ä¸æ­£ç¡®
2. å­—æ®µé€‰æ‹©å™¨ä¸æ­£ç¡®
3. é¡µé¢éœ€è¦ç™»å½•
4. æ•°æ®åŠ¨æ€åŠ è½½

**è§£å†³æ–¹æ³•ï¼š**
```python
# 1. å¢åŠ ç­‰å¾…æ—¶é—´
config.delay = 5.0

# 2. ä½¿ç”¨preview_scrapeæµ‹è¯•
# 3. æ£€æŸ¥é¡µé¢HTMLç»“æ„
```

### Q4: å¦‚ä½•æŠ“å–éœ€è¦ç™»å½•çš„é¡µé¢ï¼Ÿ

```python
# å…ˆæ‰‹åŠ¨ç™»å½•ï¼Œç„¶åä½¿ç”¨connectæ¨¡å¼
async with BrowserManager(mode="connect") as bm:
    # ä½¿ç”¨å·²ç™»å½•çš„æµè§ˆå™¨
    page = await bm.get_or_create_page()
    # ... ç»§ç»­æŠ“å–
```

---

## ğŸ’¡ æœ€ä½³å®è·µ

### 1. åˆç†è®¾ç½®å»¶è¿Ÿæ—¶é—´

```python
# å¿«é€Ÿç½‘ç«™
config.delay = 2.0

# æ™®é€šç½‘ç«™
config.delay = 3.0 - 5.0

# æ…¢é€Ÿç½‘ç«™æˆ–éœ€è¦é‡åº¦æ¸²æŸ“
config.delay = 5.0 - 10.0
```

### 2. ä½¿ç”¨ç²¾ç¡®çš„é€‰æ‹©å™¨

```python
# âŒ ä¸å¥½ï¼šå¤ªå®½æ³›
".title"

# âœ… å¥½ï¼šæ›´ç²¾ç¡®
"h3.article-title > a"

# âœ… æœ€å¥½ï¼šåŒ…å«å±æ€§
"a.page-link[rel='next']"
```

### 3. å…ˆé¢„è§ˆå†å…¨é‡æŠ“å–

```python
# ç¬¬ä¸€æ­¥ï¼šé¢„è§ˆæµ‹è¯•
preview_result = await preview_scrape(...)

# ç¬¬äºŒæ­¥ï¼šç¡®è®¤æ— è¯¯åå…¨é‡æŠ“å–
full_data = await scrape_web_data(...)
```

### 4. åˆ†æ‰¹æŠ“å–å¤§é‡æ•°æ®

```python
# ä¸è¦ä¸€æ¬¡æ€§æŠ“å–å¤ªå¤šé¡µ
# âŒ ä¸å¥½
max_pages = 100

# âœ… å¥½ï¼šåˆ†æ‰¹æŠ“å–
for batch in range(0, 100, 10):
    config.page_range = (batch+1, batch+10)
    data = await scraper.scrape()
    # ä¿å­˜æ¯æ‰¹æ•°æ®
```

---

## ğŸ“š APIå‚è€ƒ

å®Œæ•´APIæ–‡æ¡£è¯·å‚è€ƒä»£ç æ³¨é‡Šï¼š
- `lib/puppeteer/universal_scraper/scraper.py`
- `lib/puppeteer/universal_scraper/tools.py`

---

## ğŸ”— ç›¸å…³é“¾æ¥

- é¡¹ç›®README: `/README.md`
- æµè§ˆå™¨ç®¡ç†: `/lib/browser/`
- Puppeteerå·¥å…·: `/lib/puppeteer/`
- ç¤ºä¾‹ä»£ç : `/examples/`

---

## ğŸ“ æ”¯æŒ

é‡åˆ°é—®é¢˜ï¼Ÿ
1. æŸ¥çœ‹ç¤ºä¾‹ä»£ç ï¼š`lib/puppeteer/universal_scraper/example.py`
2. è¿è¡Œæµ‹è¯•è„šæœ¬ï¼š`test_universal_scraper.py`
3. æ£€æŸ¥é…ç½®å‚æ•°æ˜¯å¦æ­£ç¡®
