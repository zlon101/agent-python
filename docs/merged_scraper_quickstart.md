# åˆ—è¡¨+è¯¦æƒ…é¡µåˆå¹¶æŠ“å– - å¿«é€Ÿä¸Šæ‰‹

## ğŸ¯ æ ¸å¿ƒåŠŸèƒ½

ä»åˆ—è¡¨é¡µæŠ“å–æ¦‚è§ˆä¿¡æ¯ï¼Œç„¶åè‡ªåŠ¨è®¿é—®æ¯ä¸ªè¯¦æƒ…é¡µè·å–å®Œæ•´æ•°æ®ï¼Œæœ€ååˆå¹¶ä¿å­˜ã€‚

**å…³é”®ç‰¹æ€§**ï¼šåˆ—è¡¨æ•°æ®å’Œè¯¦æƒ…æ•°æ®ä¸¥æ ¼å¯¹åº”ï¼Œç»ä¸é”™é…ã€‚

## ğŸš€ 30ç§’ä¸Šæ‰‹

```python
from browser import BrowserManager
from puppeteer import create_scraper_config, create_merged_scraper_config, MergedScraper

async with BrowserManager(mode="launch") as bm:
    page = await bm.get_or_create_page()
    
    # æ­¥éª¤1ï¼šé…ç½®åˆ—è¡¨é¡µ
    list_config = create_scraper_config(
        url="https://segmentfault.com/",
        fields={
            "æ ‡é¢˜": "h3 a.text-body",
            "è¯¦æƒ…é“¾æ¥": "h3 a.text-body"  # â† è¿™ä¸ªå­—æ®µç”¨äºè·³è½¬è¯¦æƒ…é¡µ
        },
        container_selector=".list-group-item",
        max_pages=2
    )
    
    # æ­¥éª¤2ï¼šé…ç½®è¯¦æƒ…é¡µ
    merged_config = create_merged_scraper_config(
        list_config=list_config,
        detail_fields={
            "å®Œæ•´å†…å®¹": ".article-content",
            "ä½œè€…": ".user-info .name"
        },
        detail_container_selector=".article-content",
        detail_url_field="è¯¦æƒ…é“¾æ¥"  # â† å¯¹åº”ä¸Šé¢çš„å­—æ®µå
    )
    
    # æ­¥éª¤3ï¼šæ‰§è¡ŒæŠ“å–
    scraper = MergedScraper(page, merged_config)
    data = await scraper.scrape()
    scraper.save_to_json("result.json")
```

## ğŸ“¤ è¾“å‡ºæ ¼å¼

```json
{
  "data": [
    {
      "list_data": {
        "æ ‡é¢˜": "æ–‡ç« æ ‡é¢˜",
        "è¯¦æƒ…é“¾æ¥": "https://..."
      },
      "detail_data": {
        "å®Œæ•´å†…å®¹": "æ–‡ç« å†…å®¹...",
        "ä½œè€…": "å¼ ä¸‰"
      },
      "metadata": {
        "list_page": 1,
        "item_index": 0,
        "scrape_status": "success"
      }
    }
  ]
}
```

## ğŸ’¡ ä¸‰ç§ä½¿ç”¨åœºæ™¯

### åœºæ™¯1ï¼šä»URLå¼€å§‹
```python
scraper = MergedScraper(page, merged_config)
data = await scraper.scrape()  # è‡ªåŠ¨è®¿é—®list_config.url
```

### åœºæ™¯2ï¼šå·²æ‰“å¼€çš„é¡µé¢
```python
scraper = MergedScraper(page, merged_config)
data = await scraper.scrape_from_current_page()  # ä»å½“å‰é¡µå¼€å§‹
```

### åœºæ™¯3ï¼šè¿æ¥å·²æ‰“å¼€çš„Chrome
```python
async with BrowserManager(mode="connect") as bm:
    page = await bm.get_or_create_page(target_url="example.com")
    scraper = MergedScraper(page, merged_config)
    data = await scraper.scrape_from_current_page()
```

## âš™ï¸ å¸¸ç”¨é…ç½®

```python
merged_config = create_merged_scraper_config(
    list_config=list_config,
    detail_fields={"å†…å®¹": ".article"},
    detail_container_selector=".article",
    detail_url_field="è¯¦æƒ…é“¾æ¥",
    
    # å¯é€‰é…ç½®
    detail_url_attribute="href",        # URLå±æ€§ï¼Œé»˜è®¤href
    back_wait_time=2.0,                 # è¿”å›åˆ—è¡¨é¡µç­‰å¾…æ—¶é—´
    detail_page_wait_time=2.0,          # è¯¦æƒ…é¡µåŠ è½½ç­‰å¾…æ—¶é—´
    max_detail_retries=2,               # è¯¦æƒ…é¡µå¤±è´¥é‡è¯•æ¬¡æ•°
    continue_on_error=True,             # å•ä¸ªå¤±è´¥æ˜¯å¦ç»§ç»­
    skip_invalid_urls=True              # è·³è¿‡æ— æ•ˆURL
)
```

## ğŸ§ª è¿è¡Œæµ‹è¯•

```bash
# å®Œæ•´ç¤ºä¾‹ï¼ˆ4ä¸ªåœºæ™¯ï¼‰
python lib/puppeteer/merged_scraper/example.py

# å¿«é€Ÿæµ‹è¯•
python examples/02.scrape_list_with_detail.py
```

## â“ å¸¸è§é—®é¢˜

### Q1: å¦‚ä½•ç¡®ä¿æ•°æ®ä¸é”™é…ï¼Ÿ

âœ… **è‡ªåŠ¨ä¿è¯**ï¼Œæ ¸å¿ƒæœºåˆ¶ï¼š
- ä¸¥æ ¼é¡ºåºæ‰§è¡Œï¼ˆAåˆ—è¡¨â†’Aè¯¦æƒ…â†’Båˆ—è¡¨â†’Bè¯¦æƒ…ï¼‰
- åŸå­åŒ–æ“ä½œï¼ˆåˆ—è¡¨+è¯¦æƒ…åœ¨åŒä¸€å‡½æ•°ä¸­åˆå¹¶ï¼‰
- å”¯ä¸€æ ‡è¯†è¿½è¸ªï¼ˆæ¯æ¡è®°å½•æœ‰å‡†ç¡®å®šä½ï¼‰

### Q2: è¯¦æƒ…é¡µæŠ“å–å¤±è´¥æ€ä¹ˆåŠï¼Ÿ

âœ… **è‡ªåŠ¨å¤„ç†**ï¼š
- å¤±è´¥ä¼šè‡ªåŠ¨é‡è¯•ï¼ˆ`max_detail_retries=2`ï¼‰
- è®°å½•å¤±è´¥çŠ¶æ€åˆ°`metadata.scrape_status`
- ä¸å½±å“å…¶ä»–é¡¹ç»§ç»­æŠ“å–ï¼ˆ`continue_on_error=True`ï¼‰

### Q3: ç›¸å¯¹URLæ€ä¹ˆå¤„ç†ï¼Ÿ

âœ… **è‡ªåŠ¨è¡¥å…¨**ï¼Œä¾‹å¦‚ï¼š
- `/article/123` â†’ `https://example.com/article/123`

## ğŸ“š è¯¦ç»†æ–‡æ¡£

- ğŸ“– å®Œæ•´æ–‡æ¡£ï¼š`lib/puppeteer/merged_scraper/README.md`
- ğŸ’¡ æŠ€æœ¯æ–¹æ¡ˆï¼š`docs/merged_scraper_solution.md`
- âœ… å®ç°æ€»ç»“ï¼š`docs/merged_scraper_implementation_summary.md`

## ğŸ“ æ ¸å¿ƒAPI

```python
# é…ç½®å‡½æ•°
create_merged_scraper_config(
    list_config,          # åˆ—è¡¨é¡µé…ç½®
    detail_fields,        # è¯¦æƒ…é¡µå­—æ®µ {"å­—æ®µå": "é€‰æ‹©å™¨"}
    detail_container_selector,  # è¯¦æƒ…é¡µå®¹å™¨
    detail_url_field      # URLå­—æ®µå
)

# æŠ“å–å™¨ç±»
class MergedScraper:
    def __init__(self, page, config)
    
    async def scrape()                     # ä»URLå¼€å§‹
    async def scrape_from_current_page()   # ä»å½“å‰é¡µå¼€å§‹
    
    def save_to_json(filename)             # ä¿å­˜JSON
    def get_data()                         # è·å–æ•°æ®
    def get_stats()                        # è·å–ç»Ÿè®¡
```

## ğŸ‰ å¼€å§‹ä½¿ç”¨

å¤åˆ¶ä¸Šé¢çš„30ç§’ç¤ºä¾‹ä»£ç ï¼Œä¿®æ”¹URLå’Œå­—æ®µé€‰æ‹©å™¨ï¼Œç«‹å³å¼€å§‹æŠ“å–ï¼
